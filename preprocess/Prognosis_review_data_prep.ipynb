{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "bc9f2889",
   "metadata": {},
   "outputs": [],
   "source": [
    "############# DATA PREPARATION PROGNOSIS REVIEWS #############\n",
    "\n",
    "# Import the packages\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "import rispy\n",
    "from pathlib import Path\n",
    "\n",
    "# Set a path to the directory with the raw datasets\n",
    "path_data = '/Users/ispiero2/Documents/Research/Datasets/Systematic_Reviews/Datasets_final/Raw_datasets/'\n",
    "# Set a path to the directory to store the clean output datasets\n",
    "path_results = '/Users/ispiero2/Documents/Research/Datasets/Systematic_Reviews/Datasets_final/Clean_datasets/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "08cae196",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ce031123",
   "metadata": {},
   "outputs": [],
   "source": [
    "#### Prognosis review 1: Cardiac risk (Damen et al. (2016)) ####\n",
    "# Preparation of the data for ASReview simulation\n",
    "\n",
    "#      The raw EndNote file 'Prognosis_1.enl' was opened in a new EndNote library\n",
    "#      The EndNote library was exported as \"Text Only\" and \"Tab Delimited\", for the inclusions and exclusions separately\n",
    "#      The contents of the exported 'Prognosis_1_incl.txt' and 'Prognosis_1_excl.txt' files were each copied and pasted in Excel\n",
    "#             For the exclusions, the some rows were manually fixed (merged): rows 240-241, rows 420-421, and rows 426-427\n",
    "#             For the inclusions, the some rows were manually fixed (merged): rows 70-71 and rows 72-73\n",
    "#      And saved as 'Prognosis_1_excl.xlsx' and 'Prognosis_1_incls.xlsx' files\n",
    "\n",
    "os.chdir(path_data)\n",
    "\n",
    "# All records (n = 777)\n",
    "# TA exclusions (n = 686)\n",
    "data_prog1_excl_orig = pd.read_excel('Prognosis_1_cardio/Prognosis_1_excl.xlsx', header = None)\n",
    "# TA inclusions (n = 91)\n",
    "data_prog1_incl_orig = pd.read_excel('Prognosis_1_cardio/Prognosis_1_incl.xlsx', header = None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "03cfa05e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/y5/345smlqx1z7b6xdlzc6qgqd80000gq/T/ipykernel_4715/1100389514.py:10: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  data_prog1_excl['label_included'] = list(np.repeat(0, len(data_prog1_excl)))\n",
      "/var/folders/y5/345smlqx1z7b6xdlzc6qgqd80000gq/T/ipykernel_4715/1100389514.py:11: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  data_prog1_incl['label_included'] = list(np.repeat(1, len(data_prog1_incl)))\n"
     ]
    }
   ],
   "source": [
    "# Choose the columns to be saved in the output\n",
    "data_prog1_excl = data_prog1_excl_orig.iloc[:,[0,1,2,3,5,34,37,38,51]]\n",
    "data_prog1_incl = data_prog1_incl_orig.iloc[:,[0,1,2,3,5,34,37,38,51]]\n",
    "\n",
    "# Name the respective columns\n",
    "data_prog1_excl.columns = ['type','authors','year','title','journal','pmid','keywords','abstract','language']\n",
    "data_prog1_incl.columns = ['type','authors','year','title','journal','pmid','keywords','abstract','language']\n",
    "\n",
    "# Add the labels column\n",
    "data_prog1_excl['label_included'] = list(np.repeat(0, len(data_prog1_excl)))\n",
    "data_prog1_incl['label_included'] = list(np.repeat(1, len(data_prog1_incl)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "c1c47b4d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Combine the exclusions and inclusions to one final dataframe and save the output\n",
    "data_prog1_final = pd.concat([data_prog1_excl, data_prog1_incl], ignore_index=True)\n",
    "data_prog1_final = data_prog1_final.sort_values(by=['authors']).reset_index(drop=True)\n",
    "\n",
    "# Save the output\n",
    "os.chdir(path_results)\n",
    "data_prog1_final.to_excel('Prog_cardio_labeled.xlsx')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f4e8ad7",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "03797086",
   "metadata": {},
   "outputs": [],
   "source": [
    "#### Prognosis review 2: rcri (Vernooij et al. (2023)) ####\n",
    "# Preparation of the data for ASReview simulation\n",
    "\n",
    "#      The raw EndNote 'Search_addedvalueRCRI.enlx' file was opened and exported as \"Text Only\" and \"Tab Delimited\"\n",
    "#      The content of the exported 'Prognosis_2_all.txt' file was copied and pasted in Excel (in the 'Prognosis_2_all_raw.xlsx' file)\n",
    "#      Each of the records that were spread over multiple rows were manually merged to correct these records:\n",
    "#             These could be recognized by rows that do not have a year in the year column\n",
    "#             After merging, this lead to the correct amount of rows of 3999\n",
    "#      -> The file was saved as 'Prognosis_2_all_final.xlsx' file\n",
    "\n",
    "#      The raw 'Full-text-screening_part1.xlsx' file was opened in excel\n",
    "#      The entries of Nummer 556, 644, 647, 654, and 887 were corrected manually (authors, title, journal)\n",
    "#      -> The file was saved as 'Full-text-screening_part1_adapted.xlsx'\n",
    "\n",
    "#      The raw 'Full-text-screening_part1_updatesearch2020.xlsx' file was opened in excel\n",
    "#      A column \"Titel\" was added with the titels corresponding\n",
    "\n",
    "os.chdir(path_data)\n",
    "\n",
    "# All records (n = 3999)\n",
    "data_prog2_all_orig = pd.read_excel('Prognosis_2_rcri/Prognosis_2_all_final.xlsx', header = None)\n",
    "\n",
    "# Inclusions based on original search (n = 942)\n",
    "data_prog2_incl1_orig = pd.read_excel('Prognosis_2_rcri/Full-text-screening_part1_adapted.xlsx')\n",
    "# Inclusions based on search update (n = 123)\n",
    "data_prog2_incl2_orig = pd.read_excel('Prognosis_2_rcri/Full_text_screening_part1_updatesearch2020_adapted.xlsx')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "f89227ee",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/y5/345smlqx1z7b6xdlzc6qgqd80000gq/T/ipykernel_4715/1456477940.py:7: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  data_prog2_all['year'] = data_prog2_all['year'].astype(str)\n",
      "/var/folders/y5/345smlqx1z7b6xdlzc6qgqd80000gq/T/ipykernel_4715/1456477940.py:34: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  data_prog2_all['label_included'] = labels_comb\n"
     ]
    }
   ],
   "source": [
    "# Choose the columns to be saved in the output\n",
    "data_prog2_all = data_prog2_all_orig.iloc[:,[0,1,2,3,5,34,37,38,51]]\n",
    "\n",
    "# Name the respective columns\n",
    "data_prog2_all.columns = ['type','authors','year','title','journal','pmid','keywords','abstract','language']\n",
    "\n",
    "data_prog2_all['year'] = data_prog2_all['year'].astype(str)\n",
    "data_prog2_incl1_orig['Jaar'] = data_prog2_incl2_orig['Jaar'].astype(str)\n",
    "data_prog2_incl2_orig['Jaar'] = data_prog2_incl2_orig['Jaar'].astype(str)\n",
    "\n",
    "labels = []\n",
    "for i in range(0,len(data_prog2_all)):\n",
    "    title = data_prog2_all['title'][i]\n",
    "    if len(data_prog2_incl1_orig[data_prog2_incl1_orig['Titel'].str.contains(title, regex = False)]) == 1:\n",
    "        labels.append(1)\n",
    "    else:\n",
    "        labels.append(0)\n",
    "        \n",
    "labels2 = []\n",
    "for i in range(0,len(data_prog2_all)):\n",
    "    title = data_prog2_all['title'][i]\n",
    "    if len(data_prog2_incl2_orig[data_prog2_incl2_orig['Titel'].str.contains(title, regex = False)]) == 1:\n",
    "        labels2.append(1)\n",
    "    else:\n",
    "        labels2.append(0)\n",
    "\n",
    "labels_comb = []\n",
    "for i in range(0,len(data_prog2_all)):\n",
    "    if labels[i] == 0 and labels2[i] == 0:\n",
    "        labels_comb.append(0)\n",
    "    else:\n",
    "        labels_comb.append(1)\n",
    "        \n",
    "data_prog2_all['label_included'] = labels_comb        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "b06aa96e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1057"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sum(data_prog2_all['label_included']) # This should be 942+133=1075"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "7a5ccbbc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1064"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Manually compared and corrected in excel:\n",
    "#      data_prog2_all['labels_included1'] = labels\n",
    "#      data_prog2_all['labels_included2'] = labels2\n",
    "#      os.chdir(path_data)\n",
    "#      data_prog2_all.to_excel('Prog2_test.xlsx')\n",
    "#      Then manually corrected\n",
    "#      And saved as 'Prognosis_2_all_manually_corrected_labels_final.xlsx'\n",
    "\n",
    "# Load the correct labels:\n",
    "os.chdir(path_data)\n",
    "data_prog2_labels = pd.read_excel('Prognosis_2_rcri/Prognosis_2_all_manually_corrected_labels_final.xlsx').sort_values(by=['authors']).reset_index()\n",
    "\n",
    "# Sort the data_prog2_all by title to match with the data_prog2_labels set:\n",
    "data_prog2_final = data_prog2_all.sort_values(by=['authors']).reset_index()\n",
    "\n",
    "data_prog2_final['label_included'] = data_prog2_labels['label_included']\n",
    "sum(data_prog2_final['label_included']) # 1064 was actually correct, since some were labeled as inclusion in both the original and updated search"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "a58e3ff1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the output\n",
    "os.chdir(path_results)\n",
    "data_prog2_final.to_excel('Prog_rcri_labeled.xlsx')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a591621",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "79031530",
   "metadata": {},
   "outputs": [],
   "source": [
    "#### Prognosis review 3: ecmo (Pladet et al. (2023)) ####\n",
    "# Preparation of the data for ASReview simulation\n",
    "\n",
    "#      The raw EndNote file 'articles total.enw' was opened in a new EndNote library\n",
    "#      The EndNote library was exported as \"Text Only\" and \"Tab Delimited\"\n",
    "#      The exported 'Prognosis_3_all.txt' file content was copied and pasted in Excel\n",
    "#      And saved as 'Prognosis_3_all.xlsx' file\n",
    "\n",
    "#      The raw EndNote file 'articles included.enw' was opened in a new EndNote library\n",
    "#      The EndNote library was exported as \"Text Only\" and \"Tab Delimited\"\n",
    "#      The exported 'Prognosis_3_incl.txt' file content copied and pasted in Excel\n",
    "#      And saved as 'Prognosis_3_all.xlsx' file\n",
    "\n",
    "\n",
    "os.chdir(path_data)\n",
    "\n",
    "# All records (n = 4274)\n",
    "data_prog3_all_orig = pd.read_excel('Prognosis_3_ecmo/Prognosis_3_all.xlsx', header = None)\n",
    "# TA inclusions (n = 377)\n",
    "data_prog3_incl_orig = pd.read_excel('Prognosis_3_ecmo/Prognosis_3_incl.xlsx', header = None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "a90137f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove row 287 due to faulty entry:\n",
    "data_prog3_all = data_prog3_all_orig.drop(index=287)\n",
    "data_prog3_all = data_prog3_all.reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "538097f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Choose the columns to be saved in the output\n",
    "data_prog3_all = data_prog3_all.iloc[:,[0,1,2,3,5,34,37,38,51]]\n",
    "data_prog3_incl = data_prog3_incl_orig.iloc[:,[0,1,2,3,5,34,37,38,51]]\n",
    "\n",
    "# Name the respective columns\n",
    "data_prog3_all.columns = ['type','authors','year','title','journal','pmid','keywords','abstract','language']\n",
    "data_prog3_incl.columns = ['type','authors','year','title','journal','pmid','keywords','abstract','language']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "0a1d7643",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the labels based on title comparison\n",
    "labels = []\n",
    "for i in range(0,len(data_prog3_all)):\n",
    "    title = data_prog3_all['title'][i]\n",
    "    if len(data_prog3_incl[data_prog3_incl['title'].str.contains(title, regex = False)]) == 1:\n",
    "        labels.append(1)\n",
    "    else:\n",
    "        labels.append(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "eb5b2d65",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "378"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Check labels\n",
    "sum(labels) # This should be 377. By manually checking in excel, the following should not have been labeled:\n",
    "# Falk et al. (2019) Extracorporeal Membrane Oxygenation for Septic Shock "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "7e3a0884",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "type                                                Journal Article\n",
       "authors                           L. Falk; J. Hultman; L. M. Broman\n",
       "year                                                         2019.0\n",
       "title             Extracorporeal Membrane Oxygenation for Septic...\n",
       "journal                                      Critical Care Medicine\n",
       "pmid                                               rayyan-164934298\n",
       "keywords          adult_x000D_\\narticle_x000D_\\ncardiomyopathy_x...\n",
       "abstract          Objectives: Septic shock carries a high mortal...\n",
       "language                                                    English\n",
       "label_included                                                    1\n",
       "Name: 1054, dtype: object"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Add the labels to the dataset\n",
    "data_prog3_all['label_included'] = labels\n",
    "\n",
    "# Check the label that was wrong\n",
    "data_prog3_all.loc[1054] # The location of Falk et al. (2019)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "e4a65296",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Unlabel that record\n",
    "data_prog3_all.at[1054, 'label_included'] = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "4ff85338",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "377"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sum(data_prog3_all['label_included'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "5c1bfdb4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the output\n",
    "os.chdir(path_results)\n",
    "data_prog3_all.to_excel('Prog_ecmo_labeled.xlsx')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c0a86cf",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "85ce95d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "#### Prognosis review 4: model reporting (Andaur Navarrow et al. (2022)) ####\n",
    "# Preparation of the data for ASReview simulation\n",
    "\n",
    "# The dataset consisted of 10 samples of the original dataset of 24814 records\n",
    "# To get the total set of records that was TA-screened, the 10 original files of records were loaded and merged\n",
    "# To get the included set of records, the 10 original files of inclusions were loaded and merged\n",
    "\n",
    "os.chdir(path_data)\n",
    "\n",
    "group1 = pd.DataFrame(rispy.load(Path('Prognosis_4_reporting/Review_Constanza/1. [T&A] Random samples/[T&A] ML_References_group1.txt'), encoding='utf-8'))\n",
    "group2 = pd.DataFrame(rispy.load(Path('Prognosis_4_reporting/Review_Constanza/1. [T&A] Random samples/[T&A] ML_References_group2.txt'), encoding='utf-8'))\n",
    "group3 = pd.DataFrame(rispy.load(Path('Prognosis_4_reporting/Review_Constanza/1. [T&A] Random samples/[T&A] ML_References_group3.txt'), encoding='utf-8'))\n",
    "group4 = pd.DataFrame(rispy.load(Path('Prognosis_4_reporting/Review_Constanza/1. [T&A] Random samples/[T&A] ML_References_group4.txt'), encoding='utf-8'))\n",
    "group5 = pd.DataFrame(rispy.load(Path('Prognosis_4_reporting/Review_Constanza/1. [T&A] Random samples/[T&A] ML_References_group5.txt'), encoding='utf-8'))\n",
    "group6 = pd.DataFrame(rispy.load(Path('Prognosis_4_reporting/Review_Constanza/1. [T&A] Random samples/[T&A] ML_References_group6.txt'), encoding='utf-8'))\n",
    "group7 = pd.DataFrame(rispy.load(Path('Prognosis_4_reporting/Review_Constanza/1. [T&A] Random samples/[T&A] ML_References_group7.txt'), encoding='utf-8'))\n",
    "group8 = pd.DataFrame(rispy.load(Path('Prognosis_4_reporting/Review_Constanza/1. [T&A] Random samples/[T&A] ML_References_group8.txt'), encoding='utf-8'))\n",
    "group9 = pd.DataFrame(rispy.load(Path('Prognosis_4_reporting/Review_Constanza/1. [T&A] Random samples/[T&A] ML_References_group9.txt'), encoding='utf-8'))\n",
    "group10 = pd.DataFrame(rispy.load(Path('Prognosis_4_reporting/Review_Constanza/1. [T&A] Random samples/[T&A] ML_References_group10.txt'), encoding='utf-8'))\n",
    "\n",
    "# Merge all the samples (n=2482)\n",
    "groups = [group1, group2, group3, group4, group5, group6, group7, group8, group9, group10]\n",
    "data_prog4_all_orig = pd.concat(groups).reset_index(drop=True)\n",
    "\n",
    "os.chdir(path_data)\n",
    "\n",
    "group1_incl = pd.DataFrame(rispy.load(Path('Prognosis_4_reporting/Review_Constanza/2. [T&A] Included References/[T&A] Ref_included_group1.ris'), encoding='utf-8'))\n",
    "group2_incl = pd.DataFrame(rispy.load(Path('Prognosis_4_reporting/Review_Constanza/2. [T&A] Included References/[T&A] Ref_included_group2.ris'), encoding='utf-8'))\n",
    "group3_incl = pd.DataFrame(rispy.load(Path('Prognosis_4_reporting/Review_Constanza/2. [T&A] Included References/[T&A] Ref_included_group3.ris'), encoding='utf-8'))\n",
    "group4_incl = pd.DataFrame(rispy.load(Path('Prognosis_4_reporting/Review_Constanza/2. [T&A] Included References/[T&A] Ref_included_group4.ris'), encoding='utf-8'))\n",
    "group5_incl = pd.DataFrame(rispy.load(Path('Prognosis_4_reporting/Review_Constanza/2. [T&A] Included References/[T&A] Ref_included_group5.ris'), encoding='utf-8'))\n",
    "group6_incl = pd.DataFrame(rispy.load(Path('Prognosis_4_reporting/Review_Constanza/2. [T&A] Included References/[T&A] Ref_included_group6.ris'), encoding='utf-8'))\n",
    "group7_incl = pd.DataFrame(rispy.load(Path('Prognosis_4_reporting/Review_Constanza/2. [T&A] Included References/[T&A] Ref_included_group7.ris'), encoding='utf-8'))\n",
    "group8_incl = pd.DataFrame(rispy.load(Path('Prognosis_4_reporting/Review_Constanza/2. [T&A] Included References/[T&A] Ref_included_group8.ris'), encoding='utf-8'))\n",
    "group9_incl = pd.DataFrame(rispy.load(Path('Prognosis_4_reporting/Review_Constanza/2. [T&A] Included References/[T&A] Ref_included_group9.ris'), encoding='utf-8'))\n",
    "group10_incl = pd.DataFrame(rispy.load(Path('Prognosis_4_reporting/Review_Constanza/2. [T&A] Included References/[T&A] Ref_included_group10.ris'), encoding='utf-8'))\n",
    "\n",
    "# Merge all the inclusions (n=312)\n",
    "groups_incl = [group1_incl, group2_incl, group3_incl, group4_incl, group5_incl, group6_incl, group7_incl, group8_incl, group9_incl, group10_incl]\n",
    "data_prog4_incl_orig = pd.concat(groups_incl).reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "5201532e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Choose the columns to be saved in the output\n",
    "data_prog4_all = data_prog4_all_orig.iloc[:,[0,3,5,1,4,9]]\n",
    "data_prog4_incl = data_prog4_incl_orig.iloc[:,[0,9,3,2,11,12]]\n",
    "\n",
    "# Name the respective columns\n",
    "data_prog4_all.columns = ['type','authors','year','title','keywords','abstract']\n",
    "data_prog4_incl.columns = ['type','authors','year','title','keywords','abstract']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "062d7550",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/y5/345smlqx1z7b6xdlzc6qgqd80000gq/T/ipykernel_4715/519758382.py:13: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  data_prog4_all['label_included'] = labels\n"
     ]
    }
   ],
   "source": [
    "# Get the labels based on title comparison\n",
    "labels = []\n",
    "for i in range(0,len(data_prog4_all)):\n",
    "    title = data_prog4_all['title'][i]\n",
    "    if len(data_prog4_incl[data_prog4_incl['title'].str.contains(title, regex = False)]) == 1:\n",
    "        labels.append(1)\n",
    "    else:\n",
    "        labels.append(0)\n",
    "        \n",
    "sum(labels) # = 312\n",
    "\n",
    "# Add the labels to the dataset\n",
    "data_prog4_all['label_included'] = labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "91aee5b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Insert columns to match with the other reviews datasets\n",
    "data_prog4_all.insert(4, 'journal', np.repeat(np.NaN, len(data_prog4_all)))\n",
    "data_prog4_all.insert(5, 'pmid', np.repeat(np.NaN, len(data_prog4_all)))\n",
    "data_prog4_all.insert(9, 'language', np.repeat(np.NaN, len(data_prog4_all)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "6d9ff923",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the output\n",
    "os.chdir(path_results)\n",
    "data_prog4_all.to_excel('Prog_reporting_labeled.xlsx')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a16a2237",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "fef6e8b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prognosis review 5: TRIPOD \n",
    "# Preparation of the data for ASReview simulation\n",
    "\n",
    "#      The raw EndNote file 'TRIPOD search 20140704-Converted.enlx' was opened in a new EndNote library\n",
    "#      The EndNote library was exported as \"Text Only\" and \"Tab Delimited\"\n",
    "#      The content of the exported 'Prognosis_5_all.txt' file was copied and pasted in Excel\n",
    "#      Each of the records that were spread over multiple rows were manually merged to correct these records:\n",
    "#             These could be recognized by rows that do not have a year in the year column\n",
    "#             After merging, this lead to the correct amount of rows of 4871\n",
    "#      And saved as 'Prognosis_5_all.xlsx' file \n",
    "\n",
    "#      The raw EndNote file 'TRIPOD full text selection_PH-Converted.enlx' was opened in a new EndNote library\n",
    "#      The EndNote library was exported as \"Text Only\" and \"Tab Delimited\"\n",
    "#      The content of the exported 'Prognosis_5_incl.txt' file was copied and pasted in Excel\n",
    "#      Each of the records that were spread over multiple rows were manually merged to correct these records:\n",
    "#             These could be recognized by rows that do not have a year in the year column\n",
    "#             After merging, this lead to the correct amount of rows of 347\n",
    "#      And saved as 'Prognosis_5_incl.xlsx' file \n",
    "\n",
    "os.chdir(path_data)\n",
    "\n",
    "# All records (n=4871)\n",
    "data_prog5_all_orig = pd.read_excel('Prognosis_5_tripod/Prognosis_5_all.xlsx', header = None)\n",
    "# TA inclusions (n=347)\n",
    "data_prog5_incl_orig = pd.read_excel('Prognosis_5_tripod/Prognosis_5_incl.xlsx', header = None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "e95d9471",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Choose the columns to be saved in the output\n",
    "data_prog5_all = data_prog5_all_orig.iloc[:,[0,1,2,3,5,34,37,38,51]]\n",
    "data_prog5_incl = data_prog5_incl_orig.iloc[:,[0,1,2,3,5,34,37,38,51]]\n",
    "\n",
    "# Name the respective columns\n",
    "data_prog5_all.columns = ['type','authors','year','title','journal','pmid','keywords','abstract','language']\n",
    "data_prog5_incl.columns = ['type','authors','year','title','journal','pmid','keywords','abstract','language']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "67a9aeda",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the labels based on title comparison\n",
    "labels = []\n",
    "for i in range(0,len(data_prog5_all)):\n",
    "    title = data_prog5_all['title'][i]\n",
    "    if len(data_prog5_incl[data_prog5_incl['title'].str.contains(title, regex = False)]) == 1:\n",
    "        labels.append(1)\n",
    "    else:\n",
    "        labels.append(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "ba53b71a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "345"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sum(labels) # Two labels were missed; manually compared in excel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "314eb8fe",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/y5/345smlqx1z7b6xdlzc6qgqd80000gq/T/ipykernel_4715/3209880023.py:2: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  data_prog5_all['label_included'] = labels\n"
     ]
    }
   ],
   "source": [
    "# Add the labels to the dataset\n",
    "data_prog5_all['label_included'] = labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "44073d07",
   "metadata": {},
   "outputs": [],
   "source": [
    "# One missed label is this:\n",
    "data_prog5_all.loc[data_prog5_all['title'] == '6-Minute walk distance is an independent predictor of mortality in patients with idiopathic pulmonary fibrosis', ['label_included']] = 1\n",
    "\n",
    "# The other missed label is this:\n",
    "data_prog5_all.loc[data_prog5_all['title'] == '6-minute walk distance as a predictor of outcome in idiopathic pulmonary fibrosis', ['label_included']] = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "8d894aa1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "347"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sum(data_prog5_all['label_included'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "9361d49c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the output\n",
    "os.chdir(path_results)\n",
    "data_prog5_all.to_excel('Prog_tripod_labeled.xlsx')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "70ad356d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "5bb7d467",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prognosis review 6: ntcp (Takada et al. (2023)?)\n",
    "# Preparation of the data for ASReview simulation\n",
    "\n",
    "# To get the excluded set of records, the 4 original files of records were loaded and merged\n",
    "# To get the included set of records, the original file was loaded\n",
    "\n",
    "os.chdir(path_data)\n",
    "\n",
    "excl1 = pd.DataFrame(rispy.load(Path('Prognosis_6_ntcp/Archief/ExportedRis_excluded_relevant_review_tiab.txt'), encoding='utf-8'))\n",
    "excl2 = pd.DataFrame(rispy.load(Path('Prognosis_6_ntcp/Archief/ExportedRis_excluded_tiab1.txt'), encoding='utf-8'))\n",
    "excl3 = pd.DataFrame(rispy.load(Path('Prognosis_6_ntcp/Archief/ExportedRis_excluded_tiab2.txt'), encoding='utf-8'))\n",
    "excl4 = pd.DataFrame(rispy.load(Path('Prognosis_6_ntcp/Archief/ExportedRis_excluded_tiab3.txt'), encoding='utf-8'))\n",
    "\n",
    "\n",
    "# All records (n=10664)\n",
    "\n",
    "# TA exclusions (n=9711)\n",
    "excls = [excl1, excl2, excl3, excl4]\n",
    "data_prog6_excl_orig = pd.concat(excls).reset_index(drop=True)\n",
    "\n",
    "# TA inclusions (n=953)\n",
    "data_prog6_incl_orig = pd.DataFrame(rispy.load(Path('Prognosis_6_ntcp/Archief/ExportedRis_included_tiab.txt'), encoding='utf-8'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "d67891c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Choose the columns to be saved in the output\n",
    "data_prog6_excl = data_prog6_excl_orig.iloc[:,[0,14,4,1,3,7]]#[0,1,2,3,5,34,37,38,51]]\n",
    "data_prog6_incl = data_prog6_incl_orig.iloc[:,[0,3,5,1,4,8]]\n",
    "\n",
    "# Name the respective columns\n",
    "data_prog6_excl.columns = ['type','authors','year','title','keywords','abstract']\n",
    "data_prog6_incl.columns = ['type','authors','year','title','keywords','abstract']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "1a5f2a7e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/y5/345smlqx1z7b6xdlzc6qgqd80000gq/T/ipykernel_4715/3222213281.py:2: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  data_prog6_excl['label_included'] = np.repeat(0,len(data_prog6_excl))\n",
      "/var/folders/y5/345smlqx1z7b6xdlzc6qgqd80000gq/T/ipykernel_4715/3222213281.py:3: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  data_prog6_incl['label_included'] = np.repeat(1,len(data_prog6_incl))\n"
     ]
    }
   ],
   "source": [
    "# Add the labels to the data\n",
    "data_prog6_excl['label_included'] = np.repeat(0,len(data_prog6_excl))\n",
    "data_prog6_incl['label_included'] = np.repeat(1,len(data_prog6_incl))\n",
    "\n",
    "# Merge both datasets and sort according to authors\n",
    "data_prog6_all = pd.concat([data_prog6_excl, data_prog6_incl])\n",
    "data_prog6_all = data_prog6_all.sort_values(by=['authors']).reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "ecfb4f06",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Insert columns to match with the other reviews datasets\n",
    "data_prog6_all.insert(4, 'journal', np.repeat(np.NaN, len(data_prog6_all)))\n",
    "data_prog6_all.insert(5, 'pmid', np.repeat(np.NaN, len(data_prog6_all)))\n",
    "data_prog6_all.insert(9, 'language', np.repeat(np.NaN, len(data_prog6_all)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "e4eab367",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the output\n",
    "os.chdir(path_results)\n",
    "data_prog6_all.to_excel('Prog_ntcp_labeled.xlsx')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe1e9c51",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "d396575f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prognosis review 7: CLEF Diabetes\n",
    "# Preparation of the data for ASReview simulation\n",
    "\n",
    "# The data are retrieved from the CLEF 2019 challenge:\n",
    "# From: https://github.com/CLEF-TAR/tar/tree/master/2019-TAR/Task2/Testing/Prognosis/topics/CD012661\n",
    "#      All PubMed IDs were copied and pasted PubMed \n",
    "#      The selection (\"All results\") was saved in format (\"PubMed\") and then by clicking \"Create file\"\n",
    "#      The resulting raw EndNote file 'Prognosis_review_7.enl' was opened in a new EndNote library\n",
    "#      The EndNote library was exported as \"Text Only\" and \"Tab Delimited\"\n",
    "#      The content of the exported 'Prognosis_7_all.txt' file was copied and pasted in Excel\n",
    "#      Each of the records that were spread over multiple rows were manually merged to correct these records:\n",
    "#             These could be recognized by rows that do not have a year in the year column\n",
    "#             After merging, this lead to the correct amount of rows of 3367\n",
    "#      And saved as 'Prognosis_7_all.xlsx' file \n",
    "\n",
    "# From: https://github.com/CLEF-TAR/tar/blob/master/2019-TAR/Task2/Testing/Prognosis/qrels/full.test.prognosis.abs.2019.qrels\n",
    "#      These data were copied and pasted in Excel and manually converted into a .csv file\n",
    "#      And saved as 'Clef_prognosis_inclusions.csv\n",
    "\n",
    "os.chdir(path_data)\n",
    "\n",
    "# All records (n=3366)\n",
    "data_prog7_all_orig = pd.read_excel('Prognosis_7_diabetes/Prognosis_7_all.xlsx')\n",
    "\n",
    "# TA inclusions (n=192)\n",
    "data_prog7_incl_orig = pd.read_csv('Prognosis_7_diabetes/Clef_prognosis_inclusions.csv', header = None)\n",
    "\n",
    "# FT inclusions \n",
    "data_prog7_ft_incl_orig = pd.read_csv('Prognosis_7_diabetes/Prognosis_fulltext_inclusions.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "0039566b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Choose the columns to be saved in the output\n",
    "data_prog7_all = data_prog7_all_orig.iloc[:,[0,1,2,3,5,34,37,38,51]]\n",
    "\n",
    "# Name the respective columns\n",
    "data_prog7_all.columns = ['type','authors','year','title','journal','pmid','keywords','abstract','language']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "aa7a1f0a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Retrieve the included records\n",
    "data_prog7_incl = data_prog7_incl_orig\n",
    "data_prog7_incl.columns = ['review_id', 'unknown1', 'pmid', 'label_included', 'unknown2']\n",
    "data_prog7_incl = data_prog7_incl.loc[data_prog7_incl['label_included'] == 1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "9ee968ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the labels based on pmid's\n",
    "labels = []\n",
    "for i in range(0,len(data_prog7_all)):\n",
    "    p = data_prog7_all['pmid'][i]\n",
    "    if any(data_prog7_incl['pmid'] == p):\n",
    "        labels.append(1)\n",
    "    else:\n",
    "        labels.append(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "c3ca7965",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "192"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Check the labels\n",
    "sum(labels) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "a141d0cf",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/y5/345smlqx1z7b6xdlzc6qgqd80000gq/T/ipykernel_42281/1228395798.py:2: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  data_prog7_all['label_included'] = labels\n"
     ]
    }
   ],
   "source": [
    "# Add the labels to the dataset\n",
    "data_prog7_all['label_included'] = labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "221cc053",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the output\n",
    "os.chdir(path_results)\n",
    "data_prog7_all.to_excel('Prog_diabetes_labeled.xlsx')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "df3c3ecb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "94"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Get the full-text labels based on pmid's\n",
    "data_prog7_ft_incl = data_prog7_ft_incl_orig.loc[data_prog7_ft_incl_orig['Included'] == 1]\n",
    "\n",
    "ft_labels = []\n",
    "for i in range(0,len(data_prog7_all)):\n",
    "    p = data_prog7_all['pmid'][i]\n",
    "    if any(data_prog7_ft_incl['Document'] == p):\n",
    "        ft_labels.append(1)\n",
    "    else:\n",
    "        ft_labels.append(0)\n",
    "sum(ft_labels)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
