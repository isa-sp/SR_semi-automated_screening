{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "415154ba",
   "metadata": {},
   "source": [
    "# Generation of results for the simulations of semi-automated title-abstract screening\n",
    "\n",
    "This notebook contains the code for generation of the results. <br>\n",
    "\n",
    "#### Part I: Set-up\n",
    "- **1. Import the packages and functions**\n",
    "- **2. Import the intervention and prognsos review datasets** <br>\n",
    "\n",
    "#### Part II: Simulations with original review datasets \n",
    "- **3. Retrieve and merge the output from all simulations** \n",
    "<br>The simulations based on the imported datasets were previously run with a seperate code on a High Performance Computer (HPC). The simulations were run for variations of: the 6 prognosis and 6 intervention review datasets (= 12 datasets), 2 feature extraction models + 3 classification models ( = 5 model combinations), and 200 randomly sampled initial training data. This resulted in 12\\*5*200 = 12000 simulations stored in the same number of pickle files (.p). Each of these files consists of a ranking from simulating the semi-automated screening with the respective dataset and modeling methods. All these files are loaded and the output stored for futher processing.\n",
    "- **4. Compute performance metrics from the retrieved simulation output**\n",
    "<br>The retrieved output is then used to calculate the performance metrics (recall, precision at 95% recall, WSS at 95% recall, and nWSS at 95% recall) for each of the simulations.\n",
    "- **5. Create raw tables with all performance metrics seperately** \n",
    "- **6. Process raw tables into pooled tabels (for results)**\n",
    "- **7. Create histograms for WSS and precision (for results)** \n",
    "- **8. Create boxplots/lineplots of increasing recall during screening (for results)** <br>\n",
    "\n",
    "#### Part III: Simulations with adapted review datasets \n",
    "- **9. Variations in number of (relevant) records**\n",
    "- **10. Retrieve and merge the output from all simulations**\n",
    "The simulations using the manually adapted datasets were previously run with a seperate code on a High Performance Computer (HPC). The simulations were run for variations of: the 4 prognosis and 4 intervention datasets manually adapted to contain 2000, 1000, and 500 records of which 50 inclusions ( = 24 manually adapted datasets), 5 samples (seeds) for each manually adapted dataset, 1 feature extraction model + 1 classification model (default models), and 200 randomly sampled initial training data. This resulted in 24\\*5*200 = 24000 simulations stored in the same number of pickle files (.p). Each of these files consists of a ranking from simulating the semi-automated screening with the respective dataset and sampling seed. All these files are loaded and the output stored for futher processing.\n",
    "- **11. Compute performance metrics from the retrieved simulation output**\n",
    "<br>The retrieved output is then used to calculate the performance metrics (recall, precision at 95% recall, WSS at 95% recall, and nWSS at 95% recall) for each of the simulations.\n",
    "- **12. Create raw tables with all performance metrics seperately**\n",
    "- **13. Process raw tables into pooled tables (for results)**\n",
    "- **14. Create histograms for WSS and precision (for results)**\n",
    "- **15. Create lineplots of increasing recall during screening (for results)**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f5b93dc9",
   "metadata": {},
   "source": [
    "## Part I: Set-up"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1514387d",
   "metadata": {},
   "source": [
    "### 1. Import the packages and functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "51d2e8a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append('../../')\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import pickle\n",
    "import os\n",
    "import shutil\n",
    "import math\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "import argparse\n",
    "import collections"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "f148a165",
   "metadata": {},
   "outputs": [],
   "source": [
    "from functions import compute_metrics, compute_nwss\n",
    "from functions import generate_recall_table_prop, generate_recall_table_ss, max_recall_prop, max_recall_ss\n",
    "from functions import generate_wss_table, generate_results_table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "7eac1d50",
   "metadata": {},
   "outputs": [],
   "source": [
    "from asreview.models.classifiers import LogisticClassifier, LSTMBaseClassifier, LSTMPoolClassifier, NaiveBayesClassifier, NN2LayerClassifier, RandomForestClassifier, SVMClassifier\n",
    "from asreview.models.query import ClusterQuery, MaxQuery, MaxRandomQuery, MaxUncertaintyQuery, RandomQuery, UncertaintyQuery\n",
    "from asreview.models.balance import DoubleBalance, SimpleBalance, UndersampleBalance\n",
    "from asreview.models.feature_extraction import Doc2Vec, EmbeddingIdf, EmbeddingLSTM, SBERT, Tfidf\n",
    "from asreview import open_state\n",
    "\n",
    "from pathlib import Path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "af186360",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.chdir(\"..\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "1d0e78ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO put results in correct folders\n",
    "# TODO change data files to .csv\n",
    "\n",
    "path_data = 'data/' \n",
    "path_results_HPC = '/Users/ispiero2/Documents/Research/Results_HPC/'\n",
    "path_results = 'results/' "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "740ed8ad",
   "metadata": {},
   "source": [
    "### 2. Import the intervention and prognosis review datasets"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a0232786",
   "metadata": {},
   "source": [
    "The intervention review datasets that were used for simulation are imported (numbering of the datasets is ordered by authors), and the prognosis review datasets that were used for simulation are imported (numbering of the datasets is ordered by author, so numbers do not correspond to numbers in data prep):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "abc71797",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load all the review datasets from the dataset-containing folder into a dictionary\n",
    "review_dic = {}\n",
    "\n",
    "for file_name in os.listdir(path_data):\n",
    "    if file_name.endswith('.xlsx'):\n",
    "        file_path = os.path.join(path_data, file_name)\n",
    "        df = pd.read_excel(file_path)\n",
    "        key = os.path.splitext(file_name)[0].split(\"_\")[0]\n",
    "        review_dic[key] = df\n",
    "\n",
    "review_dic = dict(sorted(review_dic.items()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "11875fe3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create separate dictionaries for intervention and prognosis review datasets\n",
    "dfs_int = {key: value for key, value in review_dic.items() if key.startswith('Int')}\n",
    "dfs_prog = {key: value for key, value in review_dic.items() if key.startswith('Prog')}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6a9a8681",
   "metadata": {},
   "source": [
    "## Part II: Simulations with original review datasets"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8a26eea5",
   "metadata": {},
   "source": [
    "### 3. Retrieve and merge the output from all simulations\n",
    "\n",
    "To assess the performance of the semi-automated screening tool, not only the reviews, but also the classification models, feature extraction models, and/or query models were varied in the simulations:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "4f8ae8c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Specify the the classification, feature extraction, and query model(s) that were tested\n",
    "train_models = [LogisticClassifier(), NaiveBayesClassifier(), SVMClassifier()] \n",
    "feature_models = [Tfidf(), SBERT()] \n",
    "query_models = [MaxQuery()]\n",
    "\n",
    "# Specify the number of simulations per review-model combination  \n",
    "n_simulations = 200 "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d456f158",
   "metadata": {},
   "source": [
    "All the output from the simulations of these variations (conducted on the HPC) can then be retrieved and merged as follows:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "bac00ae7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a list of the review-model combination names\n",
    "sim_list_names = []\n",
    "for review in review_dic:\n",
    "    for train_model in train_models:\n",
    "        for feature_model in feature_models:\n",
    "            for query_model in query_models:\n",
    "                review_id = str(review + \"_\" + train_model.name + \"_\" + feature_model.name + \"_\" + query_model.name)\n",
    "                sim_list_names.append(review_id)\n",
    "                \n",
    "# Retrieve the output from the HPC generated pickle files with each having the rankings of a single simulation\n",
    "multiple_sims = []\n",
    "for i in range(0, len(sim_list_names)):\n",
    "    raw_output = {}\n",
    "    for j in range(1,n_simulations+1):\n",
    "        if Path(path_results_HPC +'sim_{review_id}_{sim}.p'.format(review_id=sim_list_names[i], sim=j)).is_file():\n",
    "            with open(path_results_HPC + 'sim_{review_id}_{sim}.p'.format(review_id=sim_list_names[i], sim=j),'rb') as f:\n",
    "                raw_output.update(pickle.load(f))\n",
    "    if len(raw_output) > 0:\n",
    "        multiple_sims.append((sim_list_names[i], len(review_dic[sim_list_names[i].split('_')[0]]), n_simulations, raw_output))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "c67891f6",
   "metadata": {},
   "outputs": [
    {
     "ename": "OSError",
     "evalue": "[Errno 28] No space left on device",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mOSError\u001b[0m                                   Traceback (most recent call last)",
      "\u001b[0;32m/var/folders/y5/345smlqx1z7b6xdlzc6qgqd80000gq/T/ipykernel_12892/1888728713.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# Save (back-up) the file with the simulation results\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;32mwith\u001b[0m \u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpath_results\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m'multiple_sims_saved_all_final_2okt.p'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m'wb'\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m     \u001b[0mpickle\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdump\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmultiple_sims\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mOSError\u001b[0m: [Errno 28] No space left on device"
     ]
    }
   ],
   "source": [
    "# Save (back-up) the file with the simulation results \n",
    "with open(path_results + 'multiple_sims_saved_all_final_2okt.p','wb') as f:\n",
    "    pickle.dump(multiple_sims, f)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ba4d6741",
   "metadata": {},
   "source": [
    "or the output can be directly opened from the already saved file:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a63cd131",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Open the file with the simulation results\n",
    "with open(path_results + 'multiple_sims_saved_all_final_2okt.p','rb') as f:\n",
    "    multiple_sims = pickle.load(f)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "31b5777f",
   "metadata": {},
   "source": [
    "The output can be separated in dictionaries for the prognosis reviews and intervention reviews each:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d3cac621",
   "metadata": {},
   "outputs": [],
   "source": [
    "#TODO remove this part?\n",
    "# # Distinguish between the intervention and prognosis reviews by creating a separate dictionary for each\n",
    "# multiple_sims_prog = multiple_sims[0:35]\n",
    "# multiple_sims_int = multiple_sims[35:75]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fd5988cb",
   "metadata": {},
   "source": [
    "### 4. Compute performance metrics from the retrieved simulation output"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "91733d01",
   "metadata": {},
   "source": [
    "The proportions (i.e. proportion of records screened) and sample sizes (i.e. the number of records screened) of interest can be defined. These are then used for evaluation of the ranking of the records and to calculate the performance metrics at each of these proportions/sample sizes screened. These calculations may take a while to run, therefore they were previously saved and can also directly be opened."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab19fcdb",
   "metadata": {},
   "outputs": [],
   "source": [
    "proportions = [0, 0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9, 1]\n",
    "sample_sizes = list(map(int,list(np.linspace(0, 99, 100,retstep = True)[0]))) + list(map(int,list(np.linspace(100, 12400, 124,retstep = True)[0])))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7a4937ee",
   "metadata": {},
   "source": [
    "Using these proportions and sizes, the following function can be used to derive the performance metrics of the simulation(s):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f7017cbe",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: remove hashtags from compute_metrics function, remove nwss 2\n",
    "\n",
    "# Use the compute_metrics function to compute the metrics from the retrieved simulation output\n",
    "#raw_output = compute_metrics.compute_metrics(multiple_sims, proportions, sample_sizes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb94b7a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save (back-up) a file with the computed output\n",
    "# with open(path_results + 'sims_output_saved_all_final_2okt.p','wb') as f:\n",
    "#     pickle.dump(raw_output, f)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ab35aabe",
   "metadata": {},
   "source": [
    "and the normalized work-saved-over sampling metric: (TODO merge this with the compute_metrics function):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a7adbfe3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO remove compute_nwss function if above works\n",
    "\n",
    "# Use the compute_nwss function to derive the normalized WSS of the retrieved simulation output\n",
    "#raw_output_nwss = compute_nwss.compute_nwss(multiple_sims, proportions, sample_sizes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4326019c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save (back-up) a file with the computed output\n",
    "# with open(path_results + 'sims_output_saved_all_nwss_final.p','wb') as f:\n",
    "#     pickle.dump(raw_output_nwss, f)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6c728ac0",
   "metadata": {},
   "source": [
    "Or directly open the file containing the output (as these especially take a while to run):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "091b1123",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Open the file with the computed output\n",
    "with open(path_results + 'sims_output_saved_all_final_2okt.p','rb') as f: #'sims_output_saved_all_final_extra.p','rb') as f:\n",
    "    raw_output = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "75448bc7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Open the file with the computed output\n",
    "# with open(path_results + 'sims_output_saved_all_nwss_final.p','rb') as f:\n",
    "#     raw_output_nwss = pickle.load(f)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5befaa63",
   "metadata": {},
   "source": [
    "### 5. Create raw tables with all performance metrics seperately"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fcf256f7",
   "metadata": {},
   "source": [
    "Filter the (for now) relevant parts of the output for the results:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c46199c",
   "metadata": {},
   "outputs": [],
   "source": [
    "evaluation = {}\n",
    "for i in range(0, len(raw_output)):\n",
    "    evaluation[raw_output[i][0]] = []\n",
    "    evaluation[raw_output[i][0]].append(raw_output[i][3:9])\n",
    "    \n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "485d4c42",
   "metadata": {},
   "outputs": [],
   "source": [
    "# evaluation_nwss = {}\n",
    "# for i in range(0, len(raw_output_nwss)):\n",
    "#     evaluation_nwss[raw_output_nwss[i][0]] = []\n",
    "#     evaluation_nwss[raw_output_nwss[i][0]].append(raw_output_nwss[i][3:])\n",
    "    \n",
    "# import warnings\n",
    "# warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "76b9f3c0",
   "metadata": {},
   "source": [
    "Create a raw table with the performance metrics for proportions:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4810fdf1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use the generate_recall_table_prop function to generate a table with all recall values for all proportions\n",
    "df_prop = generate_recall_table_prop.generate_recall_table_prop(evaluation, proportions, n_simulations)\n",
    "df_prop"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "50899057",
   "metadata": {},
   "source": [
    "Calculate the maximum recall values that could be obtained at each of the proportions screened:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "054470e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sort the dictionary\n",
    "review_dic_ord = collections.OrderedDict(sorted(review_dic.items()))\n",
    "# Use the max_recall_prop function to calculate the maximum achievable recall for each review\n",
    "df_max_recalls = max_recall_prop.max_recall_prop(review_dic_ord, proportions)\n",
    "df_max_recalls"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a2d0d38b",
   "metadata": {},
   "source": [
    "Create a raw table with the performance metrics for sample sizes:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "152e6a01",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO remove this part:\n",
    "\n",
    "# Use the generate_recall_table_ss function to generate a table with all recall values for all sample sizes\n",
    "# df_ss = generate_recall_table_ss.generate_recall_table_ss(evaluation, sample_sizes, n_simulations)\n",
    "# df_ss.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b2ff4f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save (back-up) a file with the computed tables (these take almost a day to compute)\n",
    "# # Computed table with sample size\n",
    "# with open(path_results + 'sims_output_saved_all_ss_final.p','wb') as f:\n",
    "#     pickle.dump(df_ss, f)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "83a586d9",
   "metadata": {},
   "source": [
    "Or directly open the file with the sample sizes table:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0eb4c9f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Open the computed table with sample size\n",
    "# with open(path_results + 'sims_output_saved_all_ss_final.p','rb') as f:\n",
    "#     df_ss = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f3ae2802",
   "metadata": {},
   "outputs": [],
   "source": [
    "# df_ss"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ad411820",
   "metadata": {},
   "source": [
    "Calculate the maximum recall values that could be obtained at each of the sample sizes screened"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8abd04d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Define the sample sizes (number of records screened) at which to calculate the maximul achievable recalls\n",
    "# sample_sizes_mr = list(map(int,list(np.linspace(0, 12400, 12400,retstep = True)[0])))\n",
    "# # Use the max_recall_ss function to calculate the maximum achievable recall for each review\n",
    "# df_max_recalls_ss = max_recall_ss.max_recall_ss(review_dic_ord, sample_sizes_mr)\n",
    "# df_max_recalls_ss"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5be693a6",
   "metadata": {},
   "source": [
    "Create a raw table with the work-saved-over sampling, normalized work-saved-over sampling, workload reduction in number of records, and workload reduction in hours:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c7d92b99",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use the generate_wss_table function to create a table with the workload metrics\n",
    "df_wss = generate_wss_table.generate_wss_table(evaluation, n_simulations)\n",
    "df_wss\n",
    "#df_wss.to_excel('Table_wss.xlsx')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eed65001",
   "metadata": {},
   "source": [
    "Create a raw table with the precision metric:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "01499b82",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_prec = pd.DataFrame()\n",
    "length = n_simulations\n",
    "for key, value in evaluation.items():\n",
    "    names = key.split('_')\n",
    "    review = [names[0]] * length\n",
    "    train_model = [names[1]] * length\n",
    "    feature_model = [names[2]] * length\n",
    "    query_model = [names[3]] * length\n",
    "    simulations = range(1, n_simulations+1)\n",
    "    precision = value[0][4]['Precision'] ###\n",
    "    df_sim = pd.DataFrame(list(zip(review,train_model,feature_model,query_model,simulations, precision)),\n",
    "                           columns = ['Review', 'Train model', 'Feature model', 'Query model', 'Simulation', 'Precision@95%'])\n",
    "    df_prec = pd.concat([df_prec, df_sim])\n",
    "\n",
    "    df_prec = df_prec.reset_index(drop = True)\n",
    "\n",
    "df_prec"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b33ef690",
   "metadata": {},
   "source": [
    "### 6. Process raw tables into pooled tables (for results)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a104773f",
   "metadata": {},
   "source": [
    "The generate_results_table function creates the table containing the WSS and precision values as presented in te results (df_wss_prec) and the table used for create figures (df_wss_prec_all_values):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "86be4f4d",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_wss_prec_all_values, df_wss_prec = generate_results_table.generate_results_table(df_wss, df_prec)\n",
    "#df_wss_prec.to_excel('Table_WSS_precision_final_extra.xlsx')\n",
    "df_wss_prec_all_values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "349223ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_wss_prec"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0b7464bd",
   "metadata": {},
   "source": [
    "### 7. Create histograms for WSS and precision (for results)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0a659fb2",
   "metadata": {},
   "source": [
    "Remove the output of the reviews that are not included in the results of our study/replace the numbering:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "850ee85d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO do this beforehand and adapt in simulation HPC\n",
    "\n",
    "df_wss_prec_all_values_ed = df_wss_prec_all_values.copy()\n",
    "df_prop_ed = df_prop.copy()\n",
    "df_max_recalls_ed = df_max_recalls.copy()\n",
    "\n",
    "# Remove prognosis review 5 (wrong study design), and change numbering\n",
    "df_wss_prec_all_values_ed = df_wss_prec_all_values_ed.drop(df_wss_prec_all_values_ed[df_wss_prec_all_values_ed['Review'] == 'Prog5'].index)\n",
    "df_wss_prec_all_values_ed['Review'] = df_wss_prec_all_values_ed['Review'].replace({'Prog6': 'Prog5',\n",
    "                                                                                   'Prog7': 'Prog6'})\n",
    "df_prop_ed = df_prop_ed.drop(df_prop_ed[df_prop_ed['Review'] == 'Prog5'].index)\n",
    "df_prop_ed['Review'] = df_prop_ed['Review'].replace({'Prog6': 'Prog5',\n",
    "                                                     'Prog7': 'Prog6'})\n",
    "df_prop_ed['Review_full'] = df_prop_ed['Review_full'].apply(lambda x: x.replace('Prognosis 6', 'Prognosis 5') if 'Prognosis 6' in x else x)\n",
    "df_prop_ed['Review_full'] = df_prop_ed['Review_full'].apply(lambda x: x.replace('Prognosis 7', 'Prognosis 6') if 'Prognosis 7' in x else x)\n",
    "df_prop_ed['Simulation'] = df_prop_ed['Simulation'].apply(lambda x: x.replace('Prognosis 6', 'Prognosis 5') if 'Prognosis 6' in x else x)\n",
    "df_prop_ed['Simulation'] = df_prop_ed['Simulation'].apply(lambda x: x.replace('Prognosis 7', 'Prognosis 6') if 'Prognosis 7' in x else x)\n",
    "df_max_recalls_ed = df_max_recalls_ed.drop(df_max_recalls_ed[df_max_recalls_ed['Review'] == 'Prog5'].index)\n",
    "df_max_recalls_ed['Review'] = df_max_recalls_ed['Review'].replace({'Prog6': 'Prog5',\n",
    "                                                                   'Prog7': 'Prog6'})\n",
    "\n",
    "# Remove intervention review 7 and 8 (decided too few relevant records for proper simulation)\n",
    "df_wss_prec_all_values_ed = df_wss_prec_all_values_ed.drop(df_wss_prec_all_values_ed[df_wss_prec_all_values_ed['Review'] == 'Int7'].index)\n",
    "df_wss_prec_all_values_ed = df_wss_prec_all_values_ed.drop(df_wss_prec_all_values_ed[df_wss_prec_all_values_ed['Review'] == 'Int8'].index)\n",
    "df_prop_ed = df_prop_ed.drop(df_prop_ed[df_prop_ed['Review'] == 'Int7'].index)\n",
    "df_prop_ed = df_prop_ed.drop(df_prop_ed[df_prop_ed['Review'] == 'Int8'].index)\n",
    "df_max_recalls_ed = df_max_recalls_ed.drop(df_max_recalls_ed[df_max_recalls_ed['Review'] == 'Int7'].index)\n",
    "df_max_recalls_ed = df_max_recalls_ed.drop(df_max_recalls_ed[df_max_recalls_ed['Review'] == 'Int8'].index)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "29bec866",
   "metadata": {},
   "source": [
    "Create histograms for (n-)WSS and precision for intervention and prognosis reviews seperately:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aaa15956",
   "metadata": {},
   "outputs": [],
   "source": [
    "#TODO create functions \n",
    "\n",
    "# Choose variables to plot\n",
    "variables_to_plot = ['Mean_WSS95', 'Mean_nWSS95', 'Mean_prec95']\n",
    "variable_names = ['n-WSS@95', 'WSS@95', 'Precision@95%']\n",
    "y_lims = [1,1,0.5]\n",
    "review_types = ['Int', 'Prog']\n",
    "\n",
    "# def generate_histograms(df_wss_prec_all_values, variables_to_plot, variable_names, review_types, sort_by = 'Models'):\n",
    "    \n",
    "#     # Change dtype to numeric\n",
    "#     for variable in variables_to_plot:\n",
    "#         df_wss_prec_all_values[variable] = pd.to_numeric(df_wss_prec_all_values[variable])\n",
    "        \n",
    "#     # Create a histogram for each variable and review type\n",
    "#     for i in range(0, len(variables_to_plot)):\n",
    "#         for j in review_types:\n",
    "            \n",
    "#             sns.barplot(x='Review', y=variables_to_plot[i], hue=sort_by, \n",
    "#                         data=df_wss_prec_all_values[df_wss_prec_all_values['Review'].str.startswith(j)]).set(ylabel=variable_names[i],ylim=(0,y_lims[i]))\n",
    "#             plt.legend(fontsize='10', title_fontsize='14', bbox_to_anchor=(1.05, 1), loc=2, borderaxespad=0.)\n",
    "#             plt.show()\n",
    "\n",
    "def generate_histograms(dataset, x, variables, y_labels, review_types, hue = 'Models'):\n",
    "    \n",
    "    # Change dtype to numeric\n",
    "    for variable in variables:\n",
    "        dataset[variable] = pd.to_numeric(dataset[variable])\n",
    "        \n",
    "    # Create a histogram for each variable and review type\n",
    "    for i in range(0, len(variables)):\n",
    "        for j in review_types:\n",
    "            \n",
    "            sns.barplot(x=x, y=variables[i], hue=hue, \n",
    "                        data=dataset[dataset['Review'].str.startswith(j)]).set(ylabel=y_labels[i],ylim=(0,y_lims[i]))\n",
    "            plt.legend(fontsize='10', title_fontsize='14', bbox_to_anchor=(1.05, 1), loc=2, borderaxespad=0.)\n",
    "            plt.show()\n",
    "            \n",
    "generate_histograms(dataset=df_wss_prec_all_values_ed, \n",
    "                    x='Review',\n",
    "                    variables=variables_to_plot, \n",
    "                    y_labels=variable_names, \n",
    "                    review_types=review_types)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ee434fda",
   "metadata": {},
   "source": [
    "### 8. Create boxplots/lineplots of increasing recall during screening (for results)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7a95e5f1",
   "metadata": {},
   "source": [
    "Create boxplots for the simulations of the default models (or any other subset of data/modeling methods):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "02b6bcb1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Optionally choose subset(s) to plot\n",
    "subset_models = ['nb - tfidf']\n",
    "subset_reviews = ['Prog', 'Int']\n",
    "\n",
    "def generate_boxplots(df_prop_ed, subset_models = None, subset_reviews = None):\n",
    "    \n",
    "    # Select the subset from the dataframe containing all recall values for each proportion screened\n",
    "    df_boxplots = df_prop_ed.copy()\n",
    "    \n",
    "    if subset_models != None:\n",
    "        models = '|'.join(subset_models)\n",
    "        df_boxplots = df_boxplots[df_boxplots['Simulation'].str.contains(models, regex=True)]\n",
    "    if subset_reviews != None:\n",
    "        reviews = '|'.join(subset_reviews)\n",
    "        df_boxplots = df_boxplots[df_boxplots['Review'].str.contains(reviews, regex=True)]\n",
    "\n",
    "    # Create a figure with boxplots for each review-model combination seperately\n",
    "    sns.set(style = 'ticks', font_scale = 1.5)\n",
    "    p1 = sns.catplot(data = df_boxplots, x = 'percentage of records screened', y = 'recall',\n",
    "                     col = 'Simulation', kind = 'box', col_wrap = 2, color = 'white', aspect = 1.3)\n",
    "\n",
    "    axes = p1.fig.axes\n",
    "    x_axis = df_boxplots['percentage of records screened'][0:11]\n",
    "\n",
    "    for i in range(0, len(df_boxplots['Simulation'].unique())):\n",
    "        review = df_boxplots.loc[df_boxplots['Simulation'] == df_boxplots['Simulation'].unique()[i], 'Review'].values[0]\n",
    "        max_recalls_per_prop = df_max_recalls_ed.loc[df_max_recalls_ed['Review'] == review]['Maximum recall']    \n",
    "        axes[i].plot(x_axis, max_recalls_per_prop+0.005, 'k-', linewidth = 1, color = 'black', linestyle = '--', label = \"maximal recall\")   \n",
    "        axes[i].legend(loc=\"lower right\")\n",
    "    p1.set_titles(col_template = \"{col_name}\", size = 16, weight = 'bold')\n",
    "\n",
    "    plt.show()\n",
    "    \n",
    "generate_boxplots(df_prop_ed, subset_models, subset_reviews)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "84377abd",
   "metadata": {},
   "source": [
    "Create lineplots for all simulations (or any other subset of data/modeling methods):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "69d62cd6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Optionally choose a subset to plot\n",
    "subset_reviews = ['Prog']\n",
    "\n",
    "def generate_lineplots(df_prop_ed, df_max_recalls_ed, subset_reviews = None):\n",
    "    \n",
    "    # Select the subset from the dataframe containing all recall values for each proportion screened\n",
    "    df_lineplots = df_prop_ed.copy()\n",
    "    \n",
    "    df_lineplots['Models'] = df_lineplots['Models'].astype(pd.CategoricalDtype(categories=['tfidf - nb',\n",
    "                                                                  'tfidf - svm',\n",
    "                                                                  'tfidf - logistic',\n",
    "                                                                  'sbert - svm',\n",
    "                                                                  'sbert - logistic']))\n",
    "    \n",
    "    if subset_reviews != None:\n",
    "        reviews = '|'.join(subset_reviews)\n",
    "        df_lineplots = df_lineplots[df_lineplots['Review'].str.contains(reviews, regex=True)]\n",
    "\n",
    "    # Create a figure with lineplots for each review-model combination seperately\n",
    "    p2 = sns.catplot(data = df_lineplots, kind = 'point', \n",
    "                     x = 'percentage of records screened', y = 'recall', \n",
    "                     col = 'Review_full', \n",
    "                     hue = 'Models', \n",
    "                     errorbar = 'ci',\n",
    "                     col_wrap = 2, aspect = 1.4, legend = False\n",
    "                     )\n",
    "\n",
    "    axes = p2.fig.axes\n",
    "\n",
    "    for i in range(0, len(df_lineplots['Review'].unique())):\n",
    "        max_recalls_per_prop = df_max_recalls_ed.loc[df_max_recalls_ed['Review'] == df_lineplots['Review'].unique()[i]]['Maximum recall']    \n",
    "        manual_screening = [0, 0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9, 1]\n",
    "        x_axis = df_lineplots['percentage of records screened'][0:len(max_recalls_per_prop)]\n",
    "        axes[i].plot(x_axis, max_recalls_per_prop + 0.005, 'k-', linewidth = 1, color = 'grey', linestyle = '--', label = \"maximum recall\") \n",
    "        axes[i].plot(x_axis, manual_screening, 'k-', linewidth = 1, color = 'grey', linestyle = '-', label = \"manual screening\") \n",
    "        axes[i].legend(loc=\"lower right\", fontsize = 12.5)\n",
    "    \n",
    "    if subset_reviews[0] == 'Int8':\n",
    "        p2.set_titles('Intervention A1', size = 16, weight = 'bold')\n",
    "    else:\n",
    "        p2.set_titles(col_template = \"{col_name}\", size = 16, weight = 'bold')\n",
    "    p2.set_xlabels('Percentage of records screened', size = 16)\n",
    "    p2.set_ylabels('Recall', size = 16)\n",
    "    p2.set(ylim = (0, 1.01))\n",
    "    plt.show()\n",
    "\n",
    "generate_lineplots(df_prop_ed, df_max_recalls_ed, subset_reviews)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "680936a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "subset_reviews = ['Int']\n",
    "generate_lineplots(df_prop_ed, df_max_recalls_ed, subset_reviews)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d03e40b",
   "metadata": {},
   "outputs": [],
   "source": [
    "subset_reviews = ['Int8']\n",
    "generate_lineplots(df_prop, df_max_recalls, subset_reviews)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1ee3d8fa",
   "metadata": {},
   "source": [
    "## Part III: Simulations with adapted review datasets"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b812df3c",
   "metadata": {},
   "source": [
    "### 9. Variations in number of (relevant) records "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "db099cfa",
   "metadata": {},
   "source": [
    "Create dataframes of manually adapted numbers of (relevant) records that were also used to simulate the semi-automated screening process on the High Performance Computer (HPC)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "feb61852",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a function to create a dictionary with subsets of a dataset consisting of varying numbers of records and inclusions:\n",
    "\n",
    "def df_var_dict(review_name, df, sizes, incl_prop): \n",
    "    \n",
    "    '''\n",
    "    df (pandas.DataFrame):    a dataframe of a review with at least containing the columns 'abstract', 'title', and 'label_included'\n",
    "    sizes (list):             a list of integers of dataframe sizes to vary  \n",
    "    incl_prop (list):         a list of integers of inclusion proportions to vary \n",
    "    '''\n",
    "    \n",
    "    # First a list of possible size/inclusion combinations is created:\n",
    "    unique_combinations = []\n",
    "    for i in range(0, len(sizes)):\n",
    "        unique_combinations.append([sizes[i],incl_prop[i]])\n",
    "\n",
    "    # Then a dictionary is created with for each combination a sample of the dataset. If the sample cannot be retrieved,\n",
    "    # i.e. the dataset size is too small or the inclusion proportion is not available, the respective item in the dictionary remains empty.\n",
    "    df_dict = {}\n",
    "    \n",
    "    # For each combination of size/inclusion proportion:\n",
    "    for i in range(len(unique_combinations)):\n",
    "        # Check if the dataframe includes enough records to sample the respective size\n",
    "        if len(df) >= unique_combinations[i][0]:\n",
    "            \n",
    "            \n",
    "            # Check if the inclusion proportion is possible for the respective size\n",
    "            if df.loc[df['label_included'] == 1].shape[0] >= int(unique_combinations[i][0] * unique_combinations[i][1]) and df.loc[df['label_included'] == 0].shape[0] >= int(unique_combinations[i][0] - (unique_combinations[i][0] * unique_combinations[i][1])):\n",
    "                # If so:\n",
    "                # Sample random inclusions\n",
    "                incl = df.loc[df['label_included'] == 1].sample(n = int(unique_combinations[i][0] * unique_combinations[i][1]), replace = False, random_state = 1)\n",
    "                # Sample random exclusions\n",
    "                excl = df.loc[df['label_included'] == 0].sample(n = int(unique_combinations[i][0] - (unique_combinations[i][0] * unique_combinations[i][1])), replace = False, random_state = 1)\n",
    "                # Create a dataframe of the inclusions and exclusions\n",
    "                df_new = pd.concat([incl, excl]).sort_values('authors').reset_index()\n",
    "                name = review_name + \"_\" + str(len(df_new)) + \"_\" + str(unique_combinations[i][1])\n",
    "            # If the size/inclusion proportion is not possible, leave the dataframe empty\n",
    "            else:\n",
    "                name = review_name + \"_\" + str(len(df_new)) + \"_\" + str(unique_combinations[i][1])\n",
    "                df_new = [] \n",
    "        else:\n",
    "            df_new = []\n",
    "        \n",
    "        if len(df_new) > 0:\n",
    "            if df_new['label_included'].sum() <= 10:\n",
    "                df_new = []\n",
    "        \n",
    "        # Store the dataframe in the dictionary of all retrieved dataframes\n",
    "        if len(df_new) > 0:\n",
    "            df_dict[name] = df_new\n",
    "  \n",
    "    return(df_dict)\n",
    "\n",
    "# Apply the function:\n",
    "sizes = [500, 1000, 2000]\n",
    "incl_prop = [0.1, 0.05, 0.025]\n",
    "ss_dict = df_var_dict(review_name = 'Int1', df = dfs_int['Int1'], sizes = sizes, incl_prop = incl_prop)                 \n",
    "ss_dict.update(df_var_dict(review_name = 'Int2', df = dfs_int['Int2'], sizes = sizes, incl_prop = incl_prop))\n",
    "ss_dict.update(df_var_dict(review_name = 'Int4', df = dfs_int['Int4'], sizes = sizes, incl_prop = incl_prop))\n",
    "ss_dict.update(df_var_dict(review_name = 'Int6', df = dfs_int['Int6'], sizes = sizes, incl_prop = incl_prop))\n",
    "ss_dict.update(df_var_dict(review_name = 'Prog3', df = dfs_prog['Prog3'], sizes = sizes, incl_prop = incl_prop))\n",
    "ss_dict.update(df_var_dict(review_name = 'Prog4', df = dfs_prog['Prog4'], sizes = sizes, incl_prop = incl_prop))\n",
    "ss_dict.update(df_var_dict(review_name = 'Prog6', df = dfs_prog['Prog6'], sizes = sizes, incl_prop = incl_prop))\n",
    "ss_dict.update(df_var_dict(review_name = 'Prog7', df = dfs_prog['Prog7'], sizes = sizes, incl_prop = incl_prop))\n",
    "\n",
    "# Save the keys\n",
    "ss_sims = list(ss_dict.keys())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "edaa399e",
   "metadata": {},
   "source": [
    "### 10. Retrieve and merge the output from all simulations"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "72a3c556",
   "metadata": {},
   "source": [
    "To assess the performance of the semi-automated screening tool, the classification models, feature extraction models, and/or query models were kept constant while the reviews were manually adapted to consist of equal number of (relevant) records."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a2be9bdf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO put results in correct folder\n",
    "path_results_HPC = '/Users/ispiero2/Documents/Research/Results_HPC/new_seed/' #new_results/''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7be833d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Specify the the classification, feature extraction, and query model(s) that were tested\n",
    "train_models = [NaiveBayesClassifier()]\n",
    "feature_models = [Tfidf()] \n",
    "query_models = [MaxQuery()]\n",
    "\n",
    "# Specify the number of simulations per review-model combination  \n",
    "n_simulations = 200 \n",
    "\n",
    "# Specify the number of sampling seeds of manually adapted datasets\n",
    "seeds = 3"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bf2f1f8c",
   "metadata": {},
   "source": [
    "All the output from the simulations of these variations (conducted on the HPC) can then be retrieved and merged as follows:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "02684671",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a list of the review-model combination names\n",
    "sim_list_names_ss = []\n",
    "for review in ss_sims:\n",
    "    for train_model in train_models:\n",
    "        for feature_model in feature_models:\n",
    "            for query_model in query_models:\n",
    "                review_id = str(review + \"_\" + train_model.name + \"_\" + feature_model.name + \"_\" + query_model.name)\n",
    "                sim_list_names_ss.append(review_id)\n",
    "\n",
    "# Derive the results from the HPC retrieved pickle files with each having the rankings of a single simulation\n",
    "multiple_sims_sizes = []\n",
    "n_simulations = 200 # number of simulations per review-model combination\n",
    "for i in range(0, len(sim_list_names_ss)):\n",
    "    for k in range(1, seeds+1):\n",
    "        raw_output_ss = {}\n",
    "        for j in range(1,n_simulations+1):\n",
    "            if Path(path_results_HPC +'sim_{review_id}_{sim}_{seed}.p'.format(review_id=sim_list_names_ss[i], sim=j, seed = k)).is_file():\n",
    "                with open(path_results_HPC + 'sim_{review_id}_{sim}_{seed}.p'.format(review_id=sim_list_names_ss[i], sim=j, seed = k),'rb') as f:\n",
    "                    raw_output_ss.update(pickle.load(f))\n",
    "        if len(raw_output_ss) > 0:\n",
    "            review_id = str(sim_list_names_ss[i] + '_' + str(k))\n",
    "            multiple_sims_sizes.append((review_id, len(ss_dict['_'.join(sim_list_names_ss[0].split('_')[0:3])]), n_simulations, raw_output_ss))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "84cda0a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Create a list of the review-model combination names\n",
    "# sim_list_names_ss = []\n",
    "# for review in ss_sims:\n",
    "#     for train_model in train_models:\n",
    "#         for feature_model in feature_models:\n",
    "#             for query_model in query_models:\n",
    "#                 review_id = str(review + \"_\" + train_model.name + \"_\" + feature_model.name + \"_\" + query_model.name)\n",
    "#                 sim_list_names_ss.append(review_id)\n",
    "\n",
    "# # Derive the results from the HPC retrieved pickle files with each having the rankings of a single simulation\n",
    "# multiple_sims_sizes = []\n",
    "# n_simulations = 200 # number of simulations per review-model combination\n",
    "# for i in range(0, len(sim_list_names_ss)):\n",
    "#     raw_output_ss = {}\n",
    "#     for j in range(1,n_simulations+1):\n",
    "#         if Path(path_results_HPC +'sim_{review_id}_{sim}.p'.format(review_id=sim_list_names_ss[i], sim=j)).is_file():\n",
    "#             with open(path_results_HPC + 'sim_{review_id}_{sim}.p'.format(review_id=sim_list_names_ss[i], sim=j),'rb') as f:\n",
    "#                 raw_output_ss.update(pickle.load(f))\n",
    "#     if len(raw_output_ss) > 0:\n",
    "#         multiple_sims_sizes.append((sim_list_names_ss[i], len(ss_dict['_'.join(sim_list_names_ss[0].split('_')[0:3])]), n_simulations, raw_output_ss))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "54fd2dbc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save (back-up) the file with the simulation results\n",
    "# with open(path_results + 'multiple_sims_sizes_seeds.p','wb') as f:\n",
    "#     pickle.dump(multiple_sims_sizes, f)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9d3c4e05",
   "metadata": {},
   "source": [
    "or the output can be directly opened from the already saved file:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f06d0a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Open the file with the simulation results\n",
    "with open(path_results + 'multiple_sims_sizes_seeds.p','rb') as f:\n",
    "    multiple_sims_sizes = pickle.load(f)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ea84a9ea",
   "metadata": {},
   "source": [
    "### 11. Compute performance metrics from the retrieved simulation output"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0af20870",
   "metadata": {},
   "source": [
    "The proportions (i.e. proportion of records screened) and sample sizes (i.e. the number of records screened) of interest can be defined. These are then used for evaluation of the ranking of the records and to calculate the performance metrics at each of these proportions/sample sizes screened."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c66a9409",
   "metadata": {},
   "outputs": [],
   "source": [
    "proportions = [0, 0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9, 1]\n",
    "sample_sizes = list(map(int,list(np.linspace(0, 99, 100,retstep = True)[0]))) + list(map(int,list(np.linspace(100, 12400, 124,retstep = True)[0])))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e94a8a77",
   "metadata": {},
   "source": [
    "Using these proportions and sizes, the following function can be used to derive the performance metrics of the simulation(s):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "47ce7d1b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use the compute_metrics function to compute the metrics from the retrieved simulation output\n",
    "#raw_output_sizes = compute_metrics.compute_metrics(multiple_sims_sizes, proportions, sample_sizes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec35a4d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save (back-up) a file with the computed output\n",
    "# with open(path_results + 'raw_output_sizes_seeds.p','wb') as f:\n",
    "#     pickle.dump(raw_output_sizes, f)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "21746659",
   "metadata": {},
   "source": [
    "and the normalized work-saved-over sampling metric: (TODO merge this with the compute_metrics function):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "109f10bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use the compute_nwss function to derive the normalized WSS of the retrieved simulation output\n",
    "#raw_output_nwss_sizes = compute_nwss.compute_nwss(multiple_sims_sizes, proportions, sample_sizes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "973321c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save (back-up) a file with the computed output\n",
    "# with open(path_results + 'raw_output_nwss_sizes_seeds.p','wb') as f:\n",
    "#     pickle.dump(raw_output_nwss_sizes, f)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6e4d135a",
   "metadata": {},
   "source": [
    "Or directly open the file containing the output (as these especially take a while to run):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "92a6f676",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Open the files with the computed output\n",
    "with open(path_results + 'raw_output_sizes_seeds.p','rb') as f:\n",
    "    raw_output_sizes = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d213f656",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Open the files with the computed output\n",
    "with open(path_results + 'raw_output_nwss_sizes_seeds.p','rb') as f:\n",
    "    raw_output_nwss_sizes = pickle.load(f)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6e846f5c",
   "metadata": {},
   "source": [
    "### 12. Create raw tables with all performance metrics seperately"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ed822052",
   "metadata": {},
   "source": [
    "Filter the (for now) relevant parts of the output for the results:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "56f9fc71",
   "metadata": {},
   "outputs": [],
   "source": [
    "evaluation_sizes = {}\n",
    "for i in range(0, len(raw_output_sizes)):\n",
    "    evaluation_sizes[raw_output_sizes[i][0]] = []\n",
    "    evaluation_sizes[raw_output_sizes[i][0]].append(raw_output_sizes[i][3:8])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e72c1ee6",
   "metadata": {},
   "outputs": [],
   "source": [
    "evaluation_sizes_nwss = {}\n",
    "for i in range(0, len(raw_output_nwss_sizes)):\n",
    "    evaluation_sizes_nwss[raw_output_nwss_sizes[i][0]] = []\n",
    "    evaluation_sizes_nwss[raw_output_nwss_sizes[i][0]].append(raw_output_nwss_sizes[i][3:8])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0a201be3",
   "metadata": {},
   "source": [
    "Create a raw table with the performance metrics for proportions:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "702fe2a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use the generate_recall_table_prop function to generate a table with all recall values for all proportions\n",
    "df_prop_sizes = generate_recall_table_prop.generate_recall_table_prop(evaluation_sizes, proportions, n_simulations, data_type = 'adapted')\n",
    "\n",
    "# Adapt the review names\n",
    "df_prop_sizes['Review'] = df_prop_sizes.apply(lambda row: row['Review'] + '_' + str(row['Total records']) + '_' + str(row['Relevant records']), axis=1)\n",
    "\n",
    "df_prop_sizes.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c5a019d1",
   "metadata": {},
   "source": [
    "Calculate the maximum recall values that could be obtained at each of the proportions screened"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe08fb7c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sort the dictionary\n",
    "ss_dict_ord = collections.OrderedDict(sorted(ss_dict.items()))\n",
    "# Use the max_recall_prop function to calculate the maximum achievable recall for each review\n",
    "df_max_recalls_sizes = max_recall_prop.max_recall_prop(ss_dict_ord, proportions)\n",
    "df_max_recalls_sizes.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6cafd870",
   "metadata": {},
   "source": [
    "Create a raw table with the work-saved-over sampling, normalized work-saved-over sampling, workload reduction in number of records, and workload reduction in hours:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8889c673",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use the generate_wss_table function to create a table with the workload metrics\n",
    "df_wss_sizes = generate_wss_table.generate_wss_table(evaluation_sizes, evaluation_sizes_nwss, n_simulations, data_type = 'adapted')\n",
    "\n",
    "# Adapt the review names\n",
    "df_wss_sizes['Review'] = df_wss_sizes.apply(lambda row: row['Review'] + '_' + str(row['Total records']) + '_' + str(row['Relevant records']), axis=1)\n",
    "\n",
    "df_wss_sizes.head()\n",
    "#df_wss_sizes.to_excel('Table_wss_sizes.xlsx')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e4751ba4",
   "metadata": {},
   "source": [
    "Create a raw table with the precision metric:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19ab8c22",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Output table for precision\n",
    "\n",
    "df_prec_sizes = pd.DataFrame()\n",
    "length = n_simulations\n",
    "for key, value in evaluation_sizes.items():\n",
    "    names = key.split('_')\n",
    "    review = [names[0]] * length\n",
    "    n_records = [names[1]] * length\n",
    "    rel_records = [names[2]] * length\n",
    "    train_model = [names[3]] * length\n",
    "    feature_model = [names[4]] * length\n",
    "    query_model = [names[5]] * length\n",
    "    seed = [names[6]] * length\n",
    "    simulations = range(1, n_simulations+1)\n",
    "    precision = value[0][4]['Precision'] ###\n",
    "    df_sim = pd.DataFrame(list(zip(review, seed, n_records, rel_records, train_model, feature_model, query_model, simulations, precision)),\n",
    "                           columns = ['Review', 'Seed', 'Total records', 'Relevant records', 'Train model', 'Feature model', 'Query model', 'Simulation', 'Precision@95%'])\n",
    "    df_prec_sizes = pd.concat([df_prec_sizes, df_sim])\n",
    "    \n",
    "    df_prec_sizes = df_prec_sizes.reset_index(drop = True)\n",
    "    \n",
    "df_prec_sizes['Review'] = df_prec_sizes.apply(lambda row: row['Review'] + '_' + str(row['Total records']) + '_' + str(row['Relevant records']), axis=1)\n",
    "\n",
    "df_prec_sizes"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5cb03ebb",
   "metadata": {},
   "source": [
    "### 13. Process raw tables into pooled tables (for results)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6ab7a8f2",
   "metadata": {},
   "source": [
    "Create a table with the results of wss/workload reduction and precision combined:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be93a8f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO reset old generate results table function / add seeds\n",
    "df_wss_prec_all_values_sizes, df_wss_prec_sizes = generate_results_table.generate_results_table(df_wss_sizes, df_prec_sizes, data_type = 'adapted')\n",
    "#df_wss_prec_all_values_sizes[['Review', 'Total records', 'Relevant records']] = df_wss_prec_all_values_sizes['Review'].str.split('_', expand=True)\n",
    "\n",
    "#df_wss_prec_sizes.to_excel('Table_WSS_precision_sizes.xlsx')\n",
    "df_wss_prec_all_values_sizes.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "942a958c",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_wss_prec_sizes[['Review', 'Total records', 'Relevant records']] = df_wss_prec_sizes['Review'].str.split('_', expand=True)\n",
    "df_wss_prec_sizes"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b2265d90",
   "metadata": {},
   "source": [
    "### 14. Create histograms for WSS and precision (for results)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "61e0e8c0",
   "metadata": {},
   "source": [
    "Replace the numbering in the output of the reviews since Prognosis review 5 is not included in our study"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1099b189",
   "metadata": {},
   "outputs": [],
   "source": [
    "#df_wss_prec_all_values_ed = df_wss_prec_all_values.copy()\n",
    "df_prop_sizes_ed = df_prop_sizes.copy()\n",
    "df_max_recalls_sizes_ed = df_max_recalls_sizes.copy()\n",
    "df_wss_prec_all_values_sizes_ed = df_wss_prec_all_values_sizes.copy()\n",
    "\n",
    "# Remove prognosis review 5 (wrong study design), and change numbering\n",
    "# df_wss_prec_all_values_sizes_ed['Review'] = df_wss_prec_all_values_sizes_ed['Review'].replace({'Prog6': 'Prog5',\n",
    "#                                                                                                'Prog7': 'Prog6'})\n",
    "\n",
    "df_wss_prec_all_values_sizes_ed['Review'] = df_wss_prec_all_values_sizes_ed['Review'].apply(lambda x: x.replace('Prog6', 'Prog5') if 'Prog6' in x else x)\n",
    "df_wss_prec_all_values_sizes_ed['Review'] = df_wss_prec_all_values_sizes_ed['Review'].apply(lambda x: x.replace('Prog7', 'Prog6') if 'Prog7' in x else x)\n",
    "\n",
    "df_prop_sizes_ed['Review'] = df_prop_sizes_ed['Review'].apply(lambda x: x.replace('Prog6', 'Prog5') if 'Prog6' in x else x)\n",
    "df_prop_sizes_ed['Review'] = df_prop_sizes_ed['Review'].apply(lambda x: x.replace('Prog7', 'Prog6') if 'Prog7' in x else x)\n",
    "\n",
    "df_prop_sizes_ed['Review_full'] = df_prop_sizes_ed['Review_full'].apply(lambda x: x.replace('Prognosis 6', 'Prognosis 5') if 'Prognosis 6' in x else x)\n",
    "df_prop_sizes_ed['Review_full'] = df_prop_sizes_ed['Review_full'].apply(lambda x: x.replace('Prognosis 7', 'Prognosis 6') if 'Prognosis 7' in x else x)\n",
    "df_prop_sizes_ed['Simulation'] = df_prop_sizes_ed['Simulation'].apply(lambda x: x.replace('Prognosis 6', 'Prognosis 5') if 'Prognosis 6' in x else x)\n",
    "df_prop_sizes_ed['Simulation'] = df_prop_sizes_ed['Simulation'].apply(lambda x: x.replace('Prognosis 7', 'Prognosis 6') if 'Prognosis 7' in x else x)\n",
    "df_max_recalls_sizes_ed = df_max_recalls_sizes_ed.drop(df_max_recalls_sizes_ed[df_max_recalls_sizes_ed['Review'] == 'Prog5'].index)\n",
    "df_max_recalls_sizes_ed['Review'] = df_max_recalls_sizes_ed['Review'].replace({'Prog6': 'Prog5',\n",
    "                                                                               'Prog7': 'Prog6'})"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c25e3c02",
   "metadata": {},
   "source": [
    "Create histograms for (n-)WSS and precision for intervention and prognosis reviews seperately"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6097d81f",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_wss_prec_all_values_sizes_ed = df_wss_prec_all_values_sizes_ed.apply(pd.to_numeric, errors='ignore')\n",
    "\n",
    "# Choose variables to plot\n",
    "variables_to_plot = ['Mean_WSS95', 'Mean_nWSS95', 'Mean_prec95']\n",
    "variable_names = ['n-WSS@95', 'WSS@95', 'Precision@95%']\n",
    "y_lims = [1,1,0.5]\n",
    "review_types = ['Int', 'Prog']\n",
    "            \n",
    "generate_histograms(dataset=df_wss_prec_all_values_sizes_ed, \n",
    "                    x='Total records', \n",
    "                    variables=variables_to_plot, \n",
    "                    y_labels=variable_names, \n",
    "                    review_types=review_types, \n",
    "                    hue='Review')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "20cb0bf7",
   "metadata": {},
   "source": [
    "### 15. Create lineplots of increasing recall during screening (for results)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "59f8d3ba",
   "metadata": {},
   "source": [
    "Create lineplots for all simulations (or any other subset of data/modeling methods):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "56a10b0a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_lineplots(df_prop_ed, df_max_recalls_ed, subset_reviews = None):\n",
    "    \n",
    "    # Select the subset from the dataframe containing all recall values for each proportion screened\n",
    "    df_lineplots = df_prop_ed.copy()\n",
    "    \n",
    "    df_lineplots['Models'] = df_lineplots['Models'].astype(pd.CategoricalDtype(categories=['tfidf - nb',\n",
    "                                                                  'tfidf - svm',\n",
    "                                                                  'tfidf - logistic',\n",
    "                                                                  'sbert - svm',\n",
    "                                                                  'sbert - logistic']))\n",
    "    \n",
    "    if subset_reviews != None:\n",
    "        reviews = '|'.join(subset_reviews)\n",
    "        df_lineplots = df_lineplots[df_lineplots['Review'].str.contains(reviews, regex=True)]\n",
    "\n",
    "    # Create a figure with lineplots for each review-model combination seperately\n",
    "    p2 = sns.catplot(data = df_lineplots, kind = 'point', \n",
    "                     x = 'percentage of records screened', y = 'recall', \n",
    "                     col = 'Review_full', \n",
    "                     hue = 'Seed', \n",
    "                     errorbar = 'ci',\n",
    "                     col_wrap = 2, aspect = 1.4, legend = False\n",
    "                     )\n",
    "\n",
    "    axes = p2.fig.axes\n",
    "\n",
    "    for i in range(0, len(df_lineplots['Review'].unique())):\n",
    "        max_recalls_per_prop = df_max_recalls_ed.loc[df_max_recalls_ed['Review'] == df_lineplots['Review'].unique()[i]]['Maximum recall']    \n",
    "        manual_screening = [0, 0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9, 1]\n",
    "        x_axis = df_lineplots['percentage of records screened'][0:len(max_recalls_per_prop)]\n",
    "        axes[i].plot(x_axis, max_recalls_per_prop + 0.005, 'k-', linewidth = 1, color = 'grey', linestyle = '--', label = \"maximum recall\") \n",
    "        axes[i].plot(x_axis, manual_screening, 'k-', linewidth = 1, color = 'grey', linestyle = '-', label = \"manual screening\") \n",
    "        axes[i].legend(loc=\"lower right\", fontsize = 12.5)\n",
    "    \n",
    "    if subset_reviews[0] == 'Int8':\n",
    "        p2.set_titles('Intervention A1', size = 16, weight = 'bold')\n",
    "    else:\n",
    "        p2.set_titles(col_template = \"{col_name}\", size = 16, weight = 'bold')\n",
    "    p2.set_xlabels('Percentage of records screened', size = 16)\n",
    "    p2.set_ylabels('Recall', size = 16)\n",
    "    p2.set(ylim = (0, 1.01))\n",
    "    plt.show()\n",
    "    \n",
    "subset_reviews = ['2000']\n",
    "generate_lineplots(df_prop_sizes, df_max_recalls_sizes, subset_reviews)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21be0554",
   "metadata": {},
   "outputs": [],
   "source": [
    "subset_reviews = ['2000']\n",
    "generate_lineplots(df_prop_sizes, df_max_recalls_sizes, subset_reviews)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb8c3c1e",
   "metadata": {},
   "outputs": [],
   "source": [
    "subset_reviews = ['1000']\n",
    "generate_lineplots(df_prop_sizes, df_max_recalls_sizes, subset_reviews)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b7282fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "subset_reviews = ['2000']\n",
    "generate_lineplots(df_prop_sizes, df_max_recalls_sizes, subset_reviews)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1565045d",
   "metadata": {},
   "source": [
    "End"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
