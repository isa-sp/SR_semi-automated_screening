{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "415154ba",
   "metadata": {},
   "source": [
    "# Generation of results for the simulations of semi-automated title-abstract screening\n",
    "\n",
    "This notebook contains the code for the generation of the results. \n",
    "- **A. Import the packages and functions**\n",
    "- **B. Import the intervention and prognsos review datasets**\n",
    "- **C. Retrieve and merge the output from all simulations** \n",
    "The simulations using the imported datasets were previously run with a seperate code on a High Performance Computer (HPC). The simulations were run for variations of: the 12 datasets, 2 feature extraction models + 3 classification models ( = 5 model combinations), and 200 randomly sampled initial training data. This resulted in 12*5*200 ( = 12000) simulations stored in the same number of pickle files (.p). Each of these files consists of a ranking from simulating the semi-automated screening with the respective dataset and modeling methods. All these files are loaded and the output stored for futher processing.\n",
    "- **D. Compute performance metrics from the retrieved simulation output**\n",
    "The retrieved output is then used to calculate the performance metrics (recall, precision, WSS etc.) for each of the simulations.\n",
    "- **E. Create raw tables with all performance metrics seperately** \n",
    "- **F. Process raw tables into pooled tabels (for results)**\n",
    "- **G. Create histograms for WSS and precision (for results)**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1514387d",
   "metadata": {},
   "source": [
    "### A. Import the packages and functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "51d2e8a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append('../')\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import pickle\n",
    "import os\n",
    "import shutil\n",
    "import math\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "import argparse\n",
    "import collections"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "f148a165",
   "metadata": {},
   "outputs": [],
   "source": [
    "from functions import compute_metrics, compute_nwss\n",
    "from functions import generate_recall_table_prop, generate_recall_table_ss, max_recall_prop, max_recall_ss\n",
    "from functions import generate_wss_table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "7eac1d50",
   "metadata": {},
   "outputs": [],
   "source": [
    "from asreview.models.classifiers import LogisticClassifier, LSTMBaseClassifier, LSTMPoolClassifier, NaiveBayesClassifier, NN2LayerClassifier, RandomForestClassifier, SVMClassifier\n",
    "from asreview.models.query import ClusterQuery, MaxQuery, MaxRandomQuery, MaxUncertaintyQuery, RandomQuery, UncertaintyQuery\n",
    "from asreview.models.balance import DoubleBalance, SimpleBalance, UndersampleBalance\n",
    "from asreview.models.feature_extraction import Doc2Vec, EmbeddingIdf, EmbeddingLSTM, SBERT, Tfidf\n",
    "from asreview import open_state\n",
    "\n",
    "from pathlib import Path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "1d0e78ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "path_data = '/Users/ispiero2/Documents/Research/Datasets/Systematic_reviews/Datasets_final/Clean_datasets/data_github/' \n",
    "path_results_HPC = '/Users/ispiero2/Documents/Research/Results_HPC/'\n",
    "path_results = '/Users/ispiero2/Documents/Research/Results_tmp/' "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "740ed8ad",
   "metadata": {},
   "source": [
    "### B. Import the intervention and prognosis review datasets"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a0232786",
   "metadata": {},
   "source": [
    "The intervention review datasets that were used for simulation are imported (numbering of the datasets is ordered by authors), and the prognosis review datasets that were used for simulation are imported (numbering of the datasets is ordered by author, so numbers do not correspond to numbers in data prep):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "ea7b0f26",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load all the review datasets in the dataset containing folder into a dictionary\n",
    "review_dic = {}\n",
    "\n",
    "for file_name in os.listdir(path_data):\n",
    "    if file_name.endswith('.xlsx'):\n",
    "        file_path = os.path.join(path_data, file_name)\n",
    "        df = pd.read_excel(file_path)\n",
    "        key = os.path.splitext(file_name)[0].split(\"_\")[0]\n",
    "        review_dic[key] = df\n",
    "\n",
    "review_dic = dict(sorted(review_dic.items()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "905e3e7d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert titles and abstracts to strings and replace NaN with empty strings\n",
    "for review in review_dic:\n",
    "    review_dic[review]['title'] = review_dic[review]['title'].replace(np.nan, '')\n",
    "    review_dic[review]['abstract'] = review_dic[review]['abstract'].replace(np.nan, '')\n",
    "    review_dic[review]['title'] = review_dic[review]['title'].astype(str)\n",
    "    review_dic[review]['abstract'] = review_dic[review]['abstract'].astype(str)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "9f12d398",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create separate dictionaries for intervention and prognosis review datasets\n",
    "dfs_int = {key: value for key, value in review_dic.items() if key.startswith('Int')}\n",
    "dfs_prog = {key: value for key, value in review_dic.items() if key.startswith('Prog')}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8a26eea5",
   "metadata": {},
   "source": [
    "### C. Retrieve and merge the output from all simulations\n",
    "\n",
    "To assess the performance of the semi-automated screening tool, not only the reviews, but also the classification models, feature extraction models, and/or query models were varied in the simulations:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "4f8ae8c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Specify the the classification, feature extraction, and query model(s) that were tested\n",
    "train_models = [LogisticClassifier(), NaiveBayesClassifier(), SVMClassifier()] \n",
    "feature_models = [Tfidf(), SBERT()] \n",
    "query_models = [MaxQuery()]\n",
    "\n",
    "# Specify the number of simulations per review-model combination  \n",
    "n_simulations = 200 "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d456f158",
   "metadata": {},
   "source": [
    "All the output from the simulations of these variations (conducted on the HPC) can then be retrieved and merged as follows:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "bac00ae7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a list of the review-model combination names\n",
    "sim_list_names = []\n",
    "for review in review_dic:\n",
    "    for train_model in train_models:\n",
    "        for feature_model in feature_models:\n",
    "            for query_model in query_models:\n",
    "                review_id = str(review + \"_\" + train_model.name + \"_\" + feature_model.name + \"_\" + query_model.name)\n",
    "                sim_list_names.append(review_id)\n",
    "                \n",
    "# Retrieve the output from the HPC generated pickle files with each having the rankings of a single simulation\n",
    "multiple_sims = []\n",
    "for i in range(0, len(sim_list_names)):\n",
    "    raw_output = {}\n",
    "    for j in range(1,n_simulations+1):\n",
    "        if Path(path_results_HPC +'sim_{review_id}_{sim}.p'.format(review_id=sim_list_names[i], sim=j)).is_file():\n",
    "            with open(path_results_HPC + 'sim_{review_id}_{sim}.p'.format(review_id=sim_list_names[i], sim=j),'rb') as f:\n",
    "                raw_output.update(pickle.load(f))\n",
    "    if len(raw_output) > 0:\n",
    "        multiple_sims.append((sim_list_names[i], len(review_dic[sim_list_names[i].split('_')[0]]), n_simulations, raw_output))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "c67891f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save (back-up) the file with the simulation results \n",
    "# with open(path_results + 'multiple_sims_saved_all_final.p','wb') as f:\n",
    "#     pickle.dump(multiple_sims, f)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ba4d6741",
   "metadata": {},
   "source": [
    "or the output can be directly opened from the already saved file:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "a63cd131",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Open the file with the simulation results\n",
    "with open(path_results + 'multiple_sims_saved_all_final.p','rb') as f:\n",
    "    multiple_sims = pickle.load(f)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "31b5777f",
   "metadata": {},
   "source": [
    "The output can be separated in dictionaries for the prognosis reviews and intervention reviews each:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "d3cac621",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Distinguish between the intervention and prognosis reviews by creating a separate dictionary for each\n",
    "multiple_sims_prog = multiple_sims[0:35]\n",
    "multiple_sims_int = multiple_sims[35:75]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fd5988cb",
   "metadata": {},
   "source": [
    "### D. Compute performance metrics from the retrieved simulation output"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "91733d01",
   "metadata": {},
   "source": [
    "The proportions (i.e. proportion of records screened) and sample sizes (i.e. the number of records screened) of interest can be defined. These are then used for evaluation of the ranking of the records and to calculate the performance metrics at each of these proportions/sample sizes screened."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "ab19fcdb",
   "metadata": {},
   "outputs": [],
   "source": [
    "proportions = [0, 0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9, 1]\n",
    "sample_sizes = list(map(int,list(np.linspace(0, 99, 100,retstep = True)[0]))) + list(map(int,list(np.linspace(100, 12400, 124,retstep = True)[0])))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7a4937ee",
   "metadata": {},
   "source": [
    "Using these proportions and sizes, the following function can be used to derive the performance metrics of the simulation(s):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "f7017cbe",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use the compute_metrics function to compute the metrics from the retrieved simulation output\n",
    "#raw_output = compute_metrics.compute_metrics(multiple_sims, proportions, sample_sizes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "bb94b7a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save (back-up) a file with the computed output\n",
    "# with open(path_results + 'sims_output_saved_all_final_extra.p','wb') as f:\n",
    "#     pickle.dump(raw_output, f)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ab35aabe",
   "metadata": {},
   "source": [
    "and the normalized work-saved-over sampling metric: (TODO merge this with the compute_metrics function):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "a7adbfe3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use the compute_nwss function to derive the normalized WSS of the retrieved simulation output\n",
    "#raw_output_nwss = compute_nwss.compute_nwss(multiple_sims, proportions, sample_sizes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "4326019c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save (back-up) a file with the computed output\n",
    "# with open(path_results + 'sims_output_saved_all_nwss_final.p','wb') as f:\n",
    "#     pickle.dump(raw_output_nwss, f)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6c728ac0",
   "metadata": {},
   "source": [
    "Or directly open the file containing the output (as these especially take a while to run):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "091b1123",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Open the file with the computed output\n",
    "with open(path_results + 'sims_output_saved_all_final_extra.p','rb') as f:\n",
    "    raw_output = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "75448bc7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Open the file with the computed output\n",
    "with open(path_results + 'sims_output_saved_all_nwss_final.p','rb') as f:\n",
    "    raw_output_nwss = pickle.load(f)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5befaa63",
   "metadata": {},
   "source": [
    "### E. Create raw tables with all performance metrics seperately"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fcf256f7",
   "metadata": {},
   "source": [
    "Filter the (for now) relevant parts of the output for the results:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "3c46199c",
   "metadata": {},
   "outputs": [],
   "source": [
    "evaluation = {}\n",
    "for i in range(0, len(raw_output)):\n",
    "    evaluation[raw_output[i][0]] = []\n",
    "    evaluation[raw_output[i][0]].append(raw_output[i][3:7])\n",
    "    \n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "485d4c42",
   "metadata": {},
   "outputs": [],
   "source": [
    "evaluation_nwss = {}\n",
    "for i in range(0, len(raw_output_nwss)):\n",
    "    evaluation_nwss[raw_output_nwss[i][0]] = []\n",
    "    evaluation_nwss[raw_output_nwss[i][0]].append(raw_output_nwss[i][3:])\n",
    "    \n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "76b9f3c0",
   "metadata": {},
   "source": [
    "Create a raw table with the performance metrics for proportions:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "4810fdf1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Review</th>\n",
       "      <th>Simulation number</th>\n",
       "      <th>Train model</th>\n",
       "      <th>Feature model</th>\n",
       "      <th>Query model</th>\n",
       "      <th>Simulation</th>\n",
       "      <th>percentage of records screened</th>\n",
       "      <th>recall</th>\n",
       "      <th>Review_full</th>\n",
       "      <th>Models</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Prog1</td>\n",
       "      <td>1</td>\n",
       "      <td>logistic</td>\n",
       "      <td>tfidf</td>\n",
       "      <td>max</td>\n",
       "      <td>Prognosis 1 (logistic - tfidf)</td>\n",
       "      <td>0%</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>Prognosis 1</td>\n",
       "      <td>tfidf - logistic</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Prog1</td>\n",
       "      <td>1</td>\n",
       "      <td>logistic</td>\n",
       "      <td>tfidf</td>\n",
       "      <td>max</td>\n",
       "      <td>Prognosis 1 (logistic - tfidf)</td>\n",
       "      <td>10%</td>\n",
       "      <td>0.470199</td>\n",
       "      <td>Prognosis 1</td>\n",
       "      <td>tfidf - logistic</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Prog1</td>\n",
       "      <td>1</td>\n",
       "      <td>logistic</td>\n",
       "      <td>tfidf</td>\n",
       "      <td>max</td>\n",
       "      <td>Prognosis 1 (logistic - tfidf)</td>\n",
       "      <td>20%</td>\n",
       "      <td>0.754967</td>\n",
       "      <td>Prognosis 1</td>\n",
       "      <td>tfidf - logistic</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Prog1</td>\n",
       "      <td>1</td>\n",
       "      <td>logistic</td>\n",
       "      <td>tfidf</td>\n",
       "      <td>max</td>\n",
       "      <td>Prognosis 1 (logistic - tfidf)</td>\n",
       "      <td>30%</td>\n",
       "      <td>0.907285</td>\n",
       "      <td>Prognosis 1</td>\n",
       "      <td>tfidf - logistic</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Prog1</td>\n",
       "      <td>1</td>\n",
       "      <td>logistic</td>\n",
       "      <td>tfidf</td>\n",
       "      <td>max</td>\n",
       "      <td>Prognosis 1 (logistic - tfidf)</td>\n",
       "      <td>40%</td>\n",
       "      <td>0.953642</td>\n",
       "      <td>Prognosis 1</td>\n",
       "      <td>tfidf - logistic</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  Review  Simulation number Train model Feature model Query model  \\\n",
       "0  Prog1                  1    logistic         tfidf         max   \n",
       "1  Prog1                  1    logistic         tfidf         max   \n",
       "2  Prog1                  1    logistic         tfidf         max   \n",
       "3  Prog1                  1    logistic         tfidf         max   \n",
       "4  Prog1                  1    logistic         tfidf         max   \n",
       "\n",
       "                       Simulation percentage of records screened    recall  \\\n",
       "0  Prognosis 1 (logistic - tfidf)                             0%  0.000000   \n",
       "1  Prognosis 1 (logistic - tfidf)                            10%  0.470199   \n",
       "2  Prognosis 1 (logistic - tfidf)                            20%  0.754967   \n",
       "3  Prognosis 1 (logistic - tfidf)                            30%  0.907285   \n",
       "4  Prognosis 1 (logistic - tfidf)                            40%  0.953642   \n",
       "\n",
       "   Review_full            Models  \n",
       "0  Prognosis 1  tfidf - logistic  \n",
       "1  Prognosis 1  tfidf - logistic  \n",
       "2  Prognosis 1  tfidf - logistic  \n",
       "3  Prognosis 1  tfidf - logistic  \n",
       "4  Prognosis 1  tfidf - logistic  "
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Use the generate_recall_table_prop function to generate a table with all recall values for all proportions\n",
    "df_prop = generate_recall_table_prop.generate_recall_table_prop(evaluation, proportions, n_simulations)\n",
    "df_prop.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "50899057",
   "metadata": {},
   "source": [
    "Calculate the maximum recall values that could be obtained at each of the proportions screened"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "054470e5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Review</th>\n",
       "      <th>Maximum recall</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Int1</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Int1</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Int1</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Int1</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Int1</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  Review  Maximum recall\n",
       "0   Int1             0.0\n",
       "1   Int1             1.0\n",
       "2   Int1             1.0\n",
       "3   Int1             1.0\n",
       "4   Int1             1.0"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Sort the dictionary\n",
    "review_dic_ord = collections.OrderedDict(sorted(review_dic.items()))\n",
    "# Use the max_recall_prop function to calculate the maximum achievable recall for each review\n",
    "df_max_recalls = max_recall_prop.max_recall_prop(review_dic_ord, proportions)\n",
    "df_max_recalls.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a2d0d38b",
   "metadata": {},
   "source": [
    "Create a raw table with the performance metrics for sample sizes:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "152e6a01",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use the generate_recall_table_ss function to generate a table with all recall values for all sample sizes\n",
    "# df_ss = generate_recall_table_ss.generate_recall_table_ss(evaluation, sample_sizes, n_simulations)\n",
    "# df_ss.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "7b2ff4f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save (back-up) a file with the computed tables (these take almost a day to compute)\n",
    "# # Computed table with sample size\n",
    "# with open(path_results + 'sims_output_saved_all_ss_final.p','wb') as f:\n",
    "#     pickle.dump(df_ss, f)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "83a586d9",
   "metadata": {},
   "source": [
    "Or directly open the file with the sample sizes table:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "0eb4c9f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Open the computed table with sample size\n",
    "with open(path_results + 'sims_output_saved_all_ss_final.p','rb') as f:\n",
    "    df_ss = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "f3ae2802",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Review</th>\n",
       "      <th>Train model</th>\n",
       "      <th>Feature model</th>\n",
       "      <th>Query model</th>\n",
       "      <th>Simulation</th>\n",
       "      <th>Number of records screened</th>\n",
       "      <th>Recall</th>\n",
       "      <th>Simulation number</th>\n",
       "      <th>Review_full</th>\n",
       "      <th>Models</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Prog1</td>\n",
       "      <td>logistic</td>\n",
       "      <td>tfidf</td>\n",
       "      <td>max</td>\n",
       "      <td>Prognosis 1 (logistic - tfidf)</td>\n",
       "      <td>0</td>\n",
       "      <td>0.00</td>\n",
       "      <td>1</td>\n",
       "      <td>Prognosis 1</td>\n",
       "      <td>logistic - tfidf</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Prog1</td>\n",
       "      <td>logistic</td>\n",
       "      <td>tfidf</td>\n",
       "      <td>max</td>\n",
       "      <td>Prognosis 1 (logistic - tfidf)</td>\n",
       "      <td>1</td>\n",
       "      <td>0.00</td>\n",
       "      <td>1</td>\n",
       "      <td>Prognosis 1</td>\n",
       "      <td>logistic - tfidf</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Prog1</td>\n",
       "      <td>logistic</td>\n",
       "      <td>tfidf</td>\n",
       "      <td>max</td>\n",
       "      <td>Prognosis 1 (logistic - tfidf)</td>\n",
       "      <td>2</td>\n",
       "      <td>0.01</td>\n",
       "      <td>1</td>\n",
       "      <td>Prognosis 1</td>\n",
       "      <td>logistic - tfidf</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Prog1</td>\n",
       "      <td>logistic</td>\n",
       "      <td>tfidf</td>\n",
       "      <td>max</td>\n",
       "      <td>Prognosis 1 (logistic - tfidf)</td>\n",
       "      <td>3</td>\n",
       "      <td>0.01</td>\n",
       "      <td>1</td>\n",
       "      <td>Prognosis 1</td>\n",
       "      <td>logistic - tfidf</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Prog1</td>\n",
       "      <td>logistic</td>\n",
       "      <td>tfidf</td>\n",
       "      <td>max</td>\n",
       "      <td>Prognosis 1 (logistic - tfidf)</td>\n",
       "      <td>4</td>\n",
       "      <td>0.01</td>\n",
       "      <td>1</td>\n",
       "      <td>Prognosis 1</td>\n",
       "      <td>logistic - tfidf</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  Review Train model Feature model Query model  \\\n",
       "0  Prog1    logistic         tfidf         max   \n",
       "1  Prog1    logistic         tfidf         max   \n",
       "2  Prog1    logistic         tfidf         max   \n",
       "3  Prog1    logistic         tfidf         max   \n",
       "4  Prog1    logistic         tfidf         max   \n",
       "\n",
       "                       Simulation  Number of records screened  Recall  \\\n",
       "0  Prognosis 1 (logistic - tfidf)                           0    0.00   \n",
       "1  Prognosis 1 (logistic - tfidf)                           1    0.00   \n",
       "2  Prognosis 1 (logistic - tfidf)                           2    0.01   \n",
       "3  Prognosis 1 (logistic - tfidf)                           3    0.01   \n",
       "4  Prognosis 1 (logistic - tfidf)                           4    0.01   \n",
       "\n",
       "   Simulation number  Review_full            Models  \n",
       "0                  1  Prognosis 1  logistic - tfidf  \n",
       "1                  1  Prognosis 1  logistic - tfidf  \n",
       "2                  1  Prognosis 1  logistic - tfidf  \n",
       "3                  1  Prognosis 1  logistic - tfidf  \n",
       "4                  1  Prognosis 1  logistic - tfidf  "
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_ss.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ad411820",
   "metadata": {},
   "source": [
    "Calculate the maximum recall values that could be obtained at each of the sample sizes screened"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "8abd04d7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Review</th>\n",
       "      <th>Maximum recall</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Int1</td>\n",
       "      <td>0.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Int1</td>\n",
       "      <td>0.02</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Int1</td>\n",
       "      <td>0.04</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Int1</td>\n",
       "      <td>0.06</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Int1</td>\n",
       "      <td>0.07</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  Review  Maximum recall\n",
       "0   Int1            0.00\n",
       "1   Int1            0.02\n",
       "2   Int1            0.04\n",
       "3   Int1            0.06\n",
       "4   Int1            0.07"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Define the sample sizes (number of records screened) at which to calculate the maximul achievable recalls\n",
    "sample_sizes_mr = list(map(int,list(np.linspace(0, 12400, 12400,retstep = True)[0])))\n",
    "# Use the max_recall_ss function to calculate the maximum achievable recall for each review\n",
    "df_max_recalls_ss = max_recall_ss.max_recall_ss(review_dic_ord, sample_sizes_mr)\n",
    "df_max_recalls_ss.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5be693a6",
   "metadata": {},
   "source": [
    "Create a raw table with the work-saved-over sampling, normalized work-saved-over sampling, workload reduction in number of records, and workload reduction in hours:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "c7d92b99",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Review</th>\n",
       "      <th>Train model</th>\n",
       "      <th>Feature model</th>\n",
       "      <th>Query model</th>\n",
       "      <th>Simulation</th>\n",
       "      <th>WSS@95%</th>\n",
       "      <th>n-WSS@95%</th>\n",
       "      <th>Workload reduction (n)</th>\n",
       "      <th>Workload reduction (hours)</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Prog1</td>\n",
       "      <td>logistic</td>\n",
       "      <td>tfidf</td>\n",
       "      <td>max</td>\n",
       "      <td>1</td>\n",
       "      <td>0.564827</td>\n",
       "      <td>0.690278</td>\n",
       "      <td>1526</td>\n",
       "      <td>12.7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Prog1</td>\n",
       "      <td>logistic</td>\n",
       "      <td>tfidf</td>\n",
       "      <td>max</td>\n",
       "      <td>2</td>\n",
       "      <td>0.562006</td>\n",
       "      <td>0.687037</td>\n",
       "      <td>1519</td>\n",
       "      <td>12.7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Prog1</td>\n",
       "      <td>logistic</td>\n",
       "      <td>tfidf</td>\n",
       "      <td>max</td>\n",
       "      <td>3</td>\n",
       "      <td>0.541861</td>\n",
       "      <td>0.663889</td>\n",
       "      <td>1469</td>\n",
       "      <td>12.2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Prog1</td>\n",
       "      <td>logistic</td>\n",
       "      <td>tfidf</td>\n",
       "      <td>max</td>\n",
       "      <td>4</td>\n",
       "      <td>0.570064</td>\n",
       "      <td>0.696759</td>\n",
       "      <td>1539</td>\n",
       "      <td>12.8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Prog1</td>\n",
       "      <td>logistic</td>\n",
       "      <td>tfidf</td>\n",
       "      <td>max</td>\n",
       "      <td>5</td>\n",
       "      <td>0.574899</td>\n",
       "      <td>0.702315</td>\n",
       "      <td>1551</td>\n",
       "      <td>12.9</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  Review Train model Feature model Query model  Simulation   WSS@95%  \\\n",
       "0  Prog1    logistic         tfidf         max           1  0.564827   \n",
       "1  Prog1    logistic         tfidf         max           2  0.562006   \n",
       "2  Prog1    logistic         tfidf         max           3  0.541861   \n",
       "3  Prog1    logistic         tfidf         max           4  0.570064   \n",
       "4  Prog1    logistic         tfidf         max           5  0.574899   \n",
       "\n",
       "   n-WSS@95%  Workload reduction (n)  Workload reduction (hours)  \n",
       "0   0.690278                    1526                        12.7  \n",
       "1   0.687037                    1519                        12.7  \n",
       "2   0.663889                    1469                        12.2  \n",
       "3   0.696759                    1539                        12.8  \n",
       "4   0.702315                    1551                        12.9  "
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Use the generate_wss_table function to create a table with the workload metrics\n",
    "df_wss = generate_wss_table.generate_wss_table(evaluation, evaluation_nwss, n_simulations)\n",
    "df_wss.head()\n",
    "#df_wss.to_excel('Table_wss.xlsx')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eed65001",
   "metadata": {},
   "source": [
    "Create a raw table with the precision metric:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "01499b82",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Review</th>\n",
       "      <th>Train model</th>\n",
       "      <th>Feature model</th>\n",
       "      <th>Query model</th>\n",
       "      <th>Simulation</th>\n",
       "      <th>Precision@95%</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Prog1</td>\n",
       "      <td>logistic</td>\n",
       "      <td>tfidf</td>\n",
       "      <td>max</td>\n",
       "      <td>1</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Prog1</td>\n",
       "      <td>logistic</td>\n",
       "      <td>tfidf</td>\n",
       "      <td>max</td>\n",
       "      <td>2</td>\n",
       "      <td>0.572581</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Prog1</td>\n",
       "      <td>logistic</td>\n",
       "      <td>tfidf</td>\n",
       "      <td>max</td>\n",
       "      <td>3</td>\n",
       "      <td>0.459677</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Prog1</td>\n",
       "      <td>logistic</td>\n",
       "      <td>tfidf</td>\n",
       "      <td>max</td>\n",
       "      <td>4</td>\n",
       "      <td>0.367785</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Prog1</td>\n",
       "      <td>logistic</td>\n",
       "      <td>tfidf</td>\n",
       "      <td>max</td>\n",
       "      <td>5</td>\n",
       "      <td>0.290030</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  Review Train model Feature model Query model  Simulation  Precision@95%\n",
       "0  Prog1    logistic         tfidf         max           1       0.000000\n",
       "1  Prog1    logistic         tfidf         max           2       0.572581\n",
       "2  Prog1    logistic         tfidf         max           3       0.459677\n",
       "3  Prog1    logistic         tfidf         max           4       0.367785\n",
       "4  Prog1    logistic         tfidf         max           5       0.290030"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_prec = pd.DataFrame()\n",
    "length = n_simulations\n",
    "for key, value in evaluation.items():\n",
    "    names = key.split('_')\n",
    "    review = [names[0]] * length\n",
    "    train_model = [names[1]] * length\n",
    "    feature_model = [names[2]] * length\n",
    "    query_model = [names[3]] * length\n",
    "    simulations = range(1, n_simulations+1)\n",
    "    precision = value[0][0]['Precision'] ###\n",
    "    df_sim = pd.DataFrame(list(zip(review,train_model,feature_model,query_model,simulations, precision)),\n",
    "                           columns = ['Review', 'Train model', 'Feature model', 'Query model', 'Simulation', 'Precision@95%'])\n",
    "    df_prec = pd.concat([df_prec, df_sim])\n",
    "\n",
    "    df_prec = df_prec.reset_index(drop = True)\n",
    "\n",
    "df_prec.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b33ef690",
   "metadata": {},
   "source": [
    "### F. Process raw tables into pooled tables (for results)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a104773f",
   "metadata": {},
   "source": [
    "Create a table with th results of wss and precision combined:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "86be4f4d",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_wss_prec = pd.merge(df_wss, df_prec, on=['Review', 'Train model', 'Feature model', 'Query model', 'Simulation'])\n",
    "df_wss_prec.drop(['Query model'], axis=1, inplace=True)\n",
    "\n",
    "df1 = df_wss_prec.groupby(['Review', 'Train model', 'Feature model'])['WSS@95%'].agg(Mean1 = 'mean',\n",
    "                                                                                     Count1 = 'count',\n",
    "                                                                                     SD1 = 'std').reset_index()\n",
    "\n",
    "ci95_hi_1 = []\n",
    "ci95_lo_1 = []\n",
    "\n",
    "for i in df1.index:\n",
    "    m, c, s = df1.loc[i][3:6]\n",
    "    ci95_hi_1.append(\"{:.3f}\".format(m + 1.96*s/math.sqrt(c)))\n",
    "    ci95_lo_1.append(\"{:.3f}\".format(m - 1.96*s/math.sqrt(c)))\n",
    "df1['ci95_hi_1'] = ci95_hi_1\n",
    "df1['ci95_lo_1'] = ci95_lo_1\n",
    "df1['Mean1'] = df1['Mean1'].apply(lambda x: '{:,.3f}'.format(x))\n",
    "\n",
    "df2 = df_wss_prec.groupby(['Review', 'Train model', 'Feature model'])['n-WSS@95%'].agg(Mean2 = 'mean',\n",
    "                                                                                       Count2 = 'count',\n",
    "                                                                                       SD2 = 'std').reset_index()\n",
    "\n",
    "ci95_hi_2 = []\n",
    "ci95_lo_2 = []\n",
    "\n",
    "for i in df2.index:\n",
    "    m, c, s = df2.loc[i][3:6]\n",
    "    ci95_hi_2.append(\"{:.3f}\".format(m + 1.96*s/math.sqrt(c)))\n",
    "    ci95_lo_2.append(\"{:.3f}\".format(m - 1.96*s/math.sqrt(c)))\n",
    "df2['ci95_hi_2'] = ci95_hi_2\n",
    "df2['ci95_lo_2'] = ci95_lo_2\n",
    "df2['Mean2'] = df2['Mean2'].apply(lambda x: '{:,.3f}'.format(x))\n",
    "\n",
    "df3 = df_wss_prec.groupby(['Review', 'Train model', 'Feature model'])['Precision@95%'].agg(Mean3 = 'mean',\n",
    "                                                                                           Count3 = 'count',\n",
    "                                                                                           SD3 = 'std').reset_index()\n",
    "ci95_hi_3 = []\n",
    "ci95_lo_3 = []\n",
    "\n",
    "for i in df3.index:\n",
    "    m, c, s = df3.loc[i][3:6]\n",
    "    ci95_hi_3.append(\"{:.3f}\".format(m + 1.96*s/math.sqrt(c)))\n",
    "    ci95_lo_3.append(\"{:.3f}\".format(m - 1.96*s/math.sqrt(c)))\n",
    "df3['ci95_hi_3'] = ci95_hi_3\n",
    "df3['ci95_lo_3'] = ci95_lo_3\n",
    "df3['Mean3'] = df3['Mean3'].apply(lambda x: '{:,.3f}'.format(x))\n",
    "\n",
    "df4 = df_wss_prec.groupby(['Review', 'Train model', 'Feature model'])['Workload reduction (n)'].agg(Mean4 = 'mean',\n",
    "                                                                                     Count4 = 'count',\n",
    "                                                                                     SD4 = 'std').reset_index()\n",
    "\n",
    "ci95_hi_4 = []\n",
    "ci95_lo_4 = []\n",
    "\n",
    "for i in df4.index:\n",
    "    m, c, s = df4.loc[i][3:6]\n",
    "    ci95_hi_4.append(\"{:.0f}\".format(m + 1.96*s/math.sqrt(c)))\n",
    "    ci95_lo_4.append(\"{:.0f}\".format(m - 1.96*s/math.sqrt(c)))\n",
    "df4['ci95_hi_4'] = ci95_hi_4\n",
    "df4['ci95_lo_4'] = ci95_lo_4\n",
    "df4['Mean4'] = df4['Mean4'].apply(lambda x: '{:,.0f}'.format(x))\n",
    "\n",
    "df5 = df_wss_prec.groupby(['Review', 'Train model', 'Feature model'])['Workload reduction (hours)'].agg(Mean5 = 'mean',\n",
    "                                                                                     Count5 = 'count',\n",
    "                                                                                     SD5 = 'std').reset_index()\n",
    "\n",
    "ci95_hi_5 = []\n",
    "ci95_lo_5 = []\n",
    "\n",
    "for i in df5.index:\n",
    "    m, c, s = df5.loc[i][3:6]\n",
    "    ci95_hi_5.append(\"{:.1f}\".format(m + 1.96*s/math.sqrt(c)))\n",
    "    ci95_lo_5.append(\"{:.1f}\".format(m - 1.96*s/math.sqrt(c)))\n",
    "df5['ci95_hi_5'] = ci95_hi_5\n",
    "df5['ci95_lo_5'] = ci95_lo_5\n",
    "df5['Mean5'] = df5['Mean5'].apply(lambda x: '{:,.1f}'.format(x))\n",
    "\n",
    "\n",
    "df_wss_prec = pd.merge(df1[['Review', 'Train model', 'Feature model', 'Mean1', 'ci95_hi_1', 'ci95_lo_1']], \n",
    "                       df2[['Review', 'Train model', 'Feature model', 'Mean2', 'ci95_hi_2', 'ci95_lo_2']],\n",
    "                       how = 'left', on = ['Review', 'Train model', 'Feature model'])\n",
    "df_wss_prec = pd.merge(df_wss_prec, \n",
    "                       df3[['Review', 'Train model', 'Feature model', 'Mean3', 'ci95_hi_3', 'ci95_lo_3']], \n",
    "                       how = 'left', on = ['Review', 'Train model', 'Feature model'])\n",
    "df_wss_prec = pd.merge(df_wss_prec, \n",
    "                       df4[['Review', 'Train model', 'Feature model', 'Mean4', 'ci95_hi_4', 'ci95_lo_4']], \n",
    "                       how = 'left', on = ['Review', 'Train model', 'Feature model'])\n",
    "df_wss_prec = pd.merge(df_wss_prec, \n",
    "                       df5[['Review', 'Train model', 'Feature model', 'Mean5', 'ci95_hi_5', 'ci95_lo_5']], \n",
    "                       how = 'left', on = ['Review', 'Train model', 'Feature model'])\n",
    "\n",
    "df_wss_prec['WSS@95%recall (CI)'] = df_wss_prec[\"Mean1\"] + \" (\" + df_wss_prec[\"ci95_lo_1\"].astype(str) + \"-\" + df_wss_prec[\"ci95_hi_1\"].astype(str) + \")\"\n",
    "df_wss_prec['n-WSS@95%recall (CI)'] = df_wss_prec[\"Mean2\"] + \" (\" + df_wss_prec[\"ci95_lo_2\"].astype(str) + \"-\" + df_wss_prec[\"ci95_hi_2\"].astype(str) + \")\"\n",
    "df_wss_prec['Precision@95%recall (CI)'] = df_wss_prec[\"Mean3\"] + \" (\" + df_wss_prec[\"ci95_lo_3\"].astype(str) + \"-\" + df_wss_prec[\"ci95_hi_3\"].astype(str) + \")\"\n",
    "df_wss_prec['Workload reduction in record numbers (CI)'] = df_wss_prec[\"Mean4\"] + \" (\" + df_wss_prec[\"ci95_lo_4\"].astype(str) + \"-\" + df_wss_prec[\"ci95_hi_4\"].astype(str) + \")\"\n",
    "df_wss_prec['Workload reduction in hours (CI)'] = df_wss_prec[\"Mean5\"] + \" (\" + df_wss_prec[\"ci95_lo_5\"].astype(str) + \"-\" + df_wss_prec[\"ci95_hi_5\"].astype(str) + \")\"\n",
    "\n",
    "df_wss_prec.drop(['Mean1', 'ci95_hi_1', 'ci95_lo_1', 'Mean2', 'ci95_hi_2', 'ci95_lo_2', 'Mean3', 'ci95_hi_3', 'ci95_lo_3', 'Mean4', 'ci95_hi_4', 'ci95_lo_4',  'Mean5', 'ci95_hi_5', 'ci95_lo_5'], axis=1, inplace=True)\n",
    "\n",
    "df_wss_prec.to_excel('Table_WSS_precision_final_extra.xlsx')\n",
    "df_wss_prec"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0b7464bd",
   "metadata": {},
   "source": [
    "### G. Create histograms for WSS and precision (for results)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "29bec866",
   "metadata": {},
   "source": [
    "Create histograms for WSS and precision"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aaa15956",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import math\n",
    "\n",
    "df_wss_prec = pd.merge(df_wss, df_prec, on=['Review', 'Train model', 'Feature model', 'Query model', 'Simulation'])\n",
    "df_wss_prec.drop(['Query model'], axis=1, inplace=True)\n",
    "\n",
    "df1 = df_wss_prec.groupby(['Review', 'Train model', 'Feature model'])['WSS@95%'].agg(Mean1 = 'mean',\n",
    "                                                                                     Count1 = 'count',\n",
    "                                                                                     SD1 = 'std').reset_index()\n",
    "\n",
    "ci95_hi_1 = []\n",
    "ci95_lo_1 = []\n",
    "\n",
    "for i in df1.index:\n",
    "    m, c, s = df1.loc[i][3:6]\n",
    "    ci95_hi_1.append(\"{:.3f}\".format(m + 1.96*s/math.sqrt(c)))\n",
    "    ci95_lo_1.append(\"{:.3f}\".format(m - 1.96*s/math.sqrt(c)))\n",
    "df1['ci95_hi_1'] = ci95_hi_1\n",
    "df1['ci95_lo_1'] = ci95_lo_1\n",
    "#df1['Mean1'] = df1['Mean1'].apply(lambda x: '{:,.3f}'.format(x))\n",
    "\n",
    "df2 = df_wss_prec.groupby(['Review', 'Train model', 'Feature model'])['n-WSS@95%'].agg(Mean2 = 'mean',\n",
    "                                                                                       Count2 = 'count',\n",
    "                                                                                       SD2 = 'std').reset_index()\n",
    "\n",
    "ci95_hi_2 = []\n",
    "ci95_lo_2 = []\n",
    "\n",
    "for i in df2.index:\n",
    "    m, c, s = df2.loc[i][3:6]\n",
    "    ci95_hi_2.append(\"{:.3f}\".format(m + 1.96*s/math.sqrt(c)))\n",
    "    ci95_lo_2.append(\"{:.3f}\".format(m - 1.96*s/math.sqrt(c)))\n",
    "df2['ci95_hi_2'] = ci95_hi_2\n",
    "df2['ci95_lo_2'] = ci95_lo_2\n",
    "#df2['Mean2'] = df2['Mean2'].apply(lambda x: '{:,.3f}'.format(x))\n",
    "\n",
    "df3 = df_wss_prec.groupby(['Review', 'Train model', 'Feature model'])['Precision@95%'].agg(Mean3 = 'mean',\n",
    "                                                                                           Count3 = 'count',\n",
    "                                                                                           SD3 = 'std').reset_index()\n",
    "ci95_hi_3 = []\n",
    "ci95_lo_3 = []\n",
    "\n",
    "for i in df3.index:\n",
    "    m, c, s = df3.loc[i][3:6]\n",
    "    ci95_hi_3.append(\"{:.3f}\".format(m + 1.96*s/math.sqrt(c)))\n",
    "    ci95_lo_3.append(\"{:.3f}\".format(m - 1.96*s/math.sqrt(c)))\n",
    "df3['ci95_hi_3'] = ci95_hi_3\n",
    "df3['ci95_lo_3'] = ci95_lo_3\n",
    "#df3['Mean3'] = df3['Mean3'].apply(lambda x: '{:,.3f}'.format(x))\n",
    "\n",
    "df_wss_prec = pd.merge(df1[['Review', 'Train model', 'Feature model', 'Mean1', 'ci95_hi_1', 'ci95_lo_1']], \n",
    "                       df2[['Review', 'Train model', 'Feature model', 'Mean2', 'ci95_hi_2', 'ci95_lo_2']],\n",
    "                       how = 'left', on = ['Review', 'Train model', 'Feature model'])\n",
    "df_wss_prec = pd.merge(df_wss_prec, \n",
    "                       df3[['Review', 'Train model', 'Feature model', 'Mean3', 'ci95_hi_3', 'ci95_lo_3']], \n",
    "                       how = 'left', on = ['Review', 'Train model', 'Feature model'])\n",
    "\n",
    "df_wss_prec = df_wss_prec.drop(df_wss_prec[df_wss_prec['Review'] == 'Prog5'].index)\n",
    "df_wss_prec = df_wss_prec.drop(df_wss_prec[df_wss_prec['Review'] == 'Int7'].index)\n",
    "df_wss_prec = df_wss_prec.drop(df_wss_prec[df_wss_prec['Review'] == 'Int8'].index)\n",
    "df_wss_prec['Review'] = df_wss_prec['Review'].replace({'Prog6': 'Prog5',\n",
    "                                                                   'Prog7': 'Prog6'})\n",
    "\n",
    "#df_wss_prec['WSS@95%recall (CI)'] = df_wss_prec[\"Mean1\"] + \" (\" + df_wss_prec[\"ci95_lo_1\"].astype(str) + \"-\" + df_wss_prec[\"ci95_hi_1\"].astype(str) + \")\"\n",
    "#df_wss_prec['n-WSS@95%recall (CI)'] = df_wss_prec[\"Mean2\"] + \" (\" + df_wss_prec[\"ci95_lo_2\"].astype(str) + \"-\" + df_wss_prec[\"ci95_hi_2\"].astype(str) + \")\"\n",
    "#df_wss_prec['Precision@95%recall (CI)'] = df_wss_prec[\"Mean3\"] + \" (\" + df_wss_prec[\"ci95_lo_3\"].astype(str) + \"-\" + df_wss_prec[\"ci95_hi_3\"].astype(str) + \")\"\n",
    "\n",
    "df_wss_prec['Models'] = df_wss_prec['Feature model'] + \" - \" + df_wss_prec['Train model']\n",
    "sns.barplot(x='Review',\n",
    "            y='Mean3', \n",
    "            hue='Models', \n",
    "            data=df_wss_prec[0:30]).set(ylabel='Precision@95%',ylim=(0,0.5))\n",
    "plt.legend(fontsize='10', title_fontsize='14', bbox_to_anchor=(1.05, 1), loc=2, borderaxespad=0.)\n",
    "\n",
    "plt.show()\n",
    "\n",
    "df_wss_prec['Models'] = df_wss_prec['Feature model'] + \" - \" + df_wss_prec['Train model']\n",
    "sns.barplot(x='Review',\n",
    "            y='Mean3', \n",
    "            hue='Models', \n",
    "            data=df_wss_prec[30:60]).set(ylabel='Precision@95%')\n",
    "plt.legend(fontsize='10', title_fontsize='14', bbox_to_anchor=(1.05, 1), loc=2, borderaxespad=0.)\n",
    "\n",
    "plt.show()\n",
    "\n",
    "df_wss_prec['Models'] = df_wss_prec['Feature model'] + \" - \" + df_wss_prec['Train model']\n",
    "sns.barplot(x='Review',\n",
    "            y='Mean1', \n",
    "            hue='Models', \n",
    "            data=df_wss_prec[0:30]).set(ylabel='WSS@95%', ylim=(0,1))\n",
    "plt.legend(fontsize='10', title_fontsize='14', bbox_to_anchor=(1.05, 1), loc=2, borderaxespad=0.)\n",
    "\n",
    "plt.show()\n",
    "\n",
    "df_wss_prec['Models'] = df_wss_prec['Feature model'] + \" - \" + df_wss_prec['Train model']\n",
    "sns.barplot(x='Review',\n",
    "            y='Mean1', \n",
    "            hue='Models', \n",
    "            data=df_wss_prec[30:60]).set(ylabel='WSS@95%', ylim=(0,1))\n",
    "plt.legend(fontsize='10', title_fontsize='14', bbox_to_anchor=(1.05, 1), loc=2, borderaxespad=0.)\n",
    "\n",
    "plt.show()\n",
    "\n",
    "df_wss_prec['Models'] = df_wss_prec['Feature model'] + \" - \" + df_wss_prec['Train model']\n",
    "sns.barplot(x='Review',\n",
    "            y='Mean2', \n",
    "            hue='Models', \n",
    "            data=df_wss_prec[0:30]).set(ylabel='n-WSS@95%', ylim=(0,1))\n",
    "plt.legend(fontsize='10', title_fontsize='14', bbox_to_anchor=(1.05, 1), loc=2, borderaxespad=0.)\n",
    "\n",
    "plt.show()\n",
    "\n",
    "df_wss_prec['Models'] = df_wss_prec['Feature model'] + \" - \" + df_wss_prec['Train model']\n",
    "sns.barplot(x='Review',\n",
    "            y='Mean2', \n",
    "            hue='Models', \n",
    "            data=df_wss_prec[30:60]).set(ylabel='n-WSS@95%', ylim=(0,1))\n",
    "plt.legend(fontsize='10', title_fontsize='14', bbox_to_anchor=(1.05, 1), loc=2, borderaxespad=0.)\n",
    "\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ee434fda",
   "metadata": {},
   "source": [
    "### H. Create boxplots/lineplots of increasing recall during screening (for results)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7a95e5f1",
   "metadata": {},
   "source": [
    "Create boxplots for the simulations of the default models (or any other subset of data/modeling methods):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "02b6bcb1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Boxplots\n",
    "\n",
    "# Optionally choose a subset to plot\n",
    "subset = 'nb'\n",
    "df_prop_subset = df_prop[df_prop['Simulation'].str.contains(subset)]\n",
    "df_ss_subset = df_ss[df_ss['Simulation'].str.contains(subset)]\n",
    "subset = 'Prog'\n",
    "df_prop_subset = df_prop_subset[df_prop_subset['Simulation'].str.contains(subset)]\n",
    "df_ss_subset = df_ss_subset[df_ss_subset['Simulation'].str.contains(subset)]\n",
    "\n",
    "# With percentage on the x-axis\n",
    "sns.set(style = 'ticks', font_scale = 1.5) #, font = 'Times New Roman') \n",
    "p1 = sns.catplot(data = df_prop_subset, x = 'percentage of records screened', y = 'recall',\n",
    "            col = 'Simulation', kind = 'box', col_wrap = 2, color = 'white', aspect = 1.3)\n",
    "\n",
    "axes = p1.fig.axes\n",
    "x_axis = df_prop_subset['percentage of records screened'][0:11]\n",
    "\n",
    "for i in range(0, len(df_prop_subset['Review'].unique())):\n",
    "    max_recalls_per_prop = df_max_recalls.loc[df_max_recalls['Review'] == df_prop_subset['Review'].unique()[i]]['Maximum recall']    \n",
    "    axes[i].plot(x_axis, max_recalls_per_prop+0.005, 'k-', linewidth = 1, color = 'black', linestyle = '--', label = \"maximal recall\")   \n",
    "    axes[i].legend(loc=\"lower right\")\n",
    "    \n",
    "p1.set_titles(col_template = \"{col_name}\", size = 16, weight = 'bold')\n",
    "\n",
    "plt.show()\n",
    "\n",
    "# # With sample size on the x-axis\n",
    "# df_ss_not_na = df_ss[df_ss['Recall'].apply(lambda x: isinstance(x, float))]\n",
    "# sns.catplot(data = df_ss_not_na, x = 'Number of records screened', y = 'Recall',\n",
    "#             col = 'Simulation', kind = 'box', col_wrap = 2, color = 'white')\n",
    "# plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "84377abd",
   "metadata": {},
   "source": [
    "Create lineplots for all simulations (or any other subset of data/modeling methods):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "69d62cd6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Lineplots with percentage on the x-axis\n",
    "import math\n",
    "\n",
    "# Optionally choose a subset to plot\n",
    "subset = 'Int'\n",
    "df_prop_subset = df_prop[df_prop['Simulation'].str.contains(subset)]\n",
    "df_max_recalls_ed = df_max_recalls\n",
    "\n",
    "# For prognosis run:\n",
    "df_prop_subset = df_prop_subset.drop(df_prop_subset[df_prop_subset['Review'] == 'Prog5'].index)\n",
    "df_prop_subset['Review_full'] = df_prop_subset['Review_full'].replace({'Prognosis 6': 'Prognosis 5',\n",
    "                                                                       'Prognosis 7': 'Prognosis 6'})\n",
    "df_prop_subset['Review'] = df_prop_subset['Review'].replace({'Prog6': 'Prog5',\n",
    "                                                             'Prog7': 'Prog6'})\n",
    "df_max_recalls_ed = df_max_recalls\n",
    "df_max_recalls_ed = df_max_recalls_ed.drop(df_max_recalls_ed[df_max_recalls_ed['Review'] == 'Prog5'].index)\n",
    "df_max_recalls_ed['Review'] = df_max_recalls_ed['Review'].replace({'Prog6': 'Prog5',\n",
    "                                                                   'Prog7': 'Prog6'})\n",
    "\n",
    "# For intervention run:\n",
    "df_prop_subset = df_prop_subset.drop(df_prop_subset[df_prop_subset['Review'] == 'Int7'].index)\n",
    "df_prop_subset = df_prop_subset.drop(df_prop_subset[df_prop_subset['Review'] == 'Int8'].index)\n",
    "\n",
    "# Plot\n",
    "p3 = sns.catplot(data = df_prop_subset, kind = 'point', \n",
    "                 x = 'percentage of records screened', y = 'recall', \n",
    "                 col = 'Review_full', \n",
    "                 #hue = 'Models', \n",
    "                 hue = df_prop_subset['Models'].astype(pd.CategoricalDtype(categories=['tfidf - nb',\n",
    "                                                                                      'tfidf - svm',\n",
    "                                                                                      'tfidf - logistic',\n",
    "                                                                                      'sbert - svm',\n",
    "                                                                                      'sbert - logistic'])), \n",
    "                 errorbar = 'ci',\n",
    "                 col_wrap = 2, aspect = 1.4, legend = False\n",
    "                 )\n",
    "\n",
    "axes = p3.fig.axes\n",
    "\n",
    "for i in range(0, len(df_prop_subset['Review'].unique())): \n",
    "    max_recalls_per_prop = df_max_recalls_ed.loc[df_max_recalls_ed['Review'] == df_prop_subset['Review'].unique()[i]]['Maximum recall']    \n",
    "    manual_screening = [0, 0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9, 1]\n",
    "    x_axis = df_prop_subset['percentage of records screened'][0:len(max_recalls_per_prop)]\n",
    "    axes[i].plot(x_axis, max_recalls_per_prop + 0.005, 'k-', linewidth = 1, color = 'grey', linestyle = '--', label = \"maximum recall\") \n",
    "    axes[i].plot(x_axis, manual_screening, 'k-', linewidth = 1, color = 'grey', linestyle = '-', label = \"manual screening\") \n",
    "    axes[i].legend(loc=\"lower right\", fontsize = 12.5)\n",
    "\n",
    "p3.set_titles(col_template = \"{col_name}\", size = 16, weight = 'bold')\n",
    "p3.set_xlabels('Percentage of records screened', size = 16)\n",
    "p3.set_ylabels('Recall', size = 16)\n",
    "p3.set(ylim = (0, 1.01))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2dcebf3c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Lineplots with percentage on the x-axis (Appendix intervention review)\n",
    "import math\n",
    "\n",
    "# Optionally choose a subset to plot\n",
    "subset = 'Intervention 8'\n",
    "df_prop_subset = df_prop[df_prop['Simulation'].str.contains(subset)]\n",
    "df_prop_subset['Review_full'] = df_prop_subset['Review_full'].replace({'Intervention 8': 'Intervention A1'})\n",
    "df_prop_subset['Review'] = df_prop_subset['Review'].replace({'Int8': 'IntA1'})\n",
    "\n",
    "df_max_recalls_ed = df_max_recalls\n",
    "df_max_recalls_ed = df_max_recalls_ed.drop(df_max_recalls_ed[df_max_recalls_ed['Review'] == 'Prog5'].index)\n",
    "df_max_recalls_ed['Review'] = df_max_recalls_ed['Review'].replace({'Int8': 'IntA1'})\n",
    "\n",
    "# Plot\n",
    "p3 = sns.catplot(data = df_prop_subset, kind = 'point', \n",
    "                 x = 'percentage of records screened', y = 'recall', \n",
    "                 col = 'Review_full', \n",
    "                 #hue = 'Models', \n",
    "                 hue = df_prop_subset['Models'].astype(pd.CategoricalDtype(categories=['tfidf - nb',\n",
    "                                                                                      'tfidf - svm',\n",
    "                                                                                      'tfidf - logistic',\n",
    "                                                                                      'sbert - svm',\n",
    "                                                                                      'sbert - logistic'])), \n",
    "                 errorbar = 'ci',\n",
    "                 col_wrap = 2, aspect = 1.4, legend = False\n",
    "                 )\n",
    "\n",
    "axes = p3.fig.axes\n",
    "\n",
    "for i in range(0, len(df_prop_subset['Review'].unique())): \n",
    "    max_recalls_per_prop = df_max_recalls_ed.loc[df_max_recalls_ed['Review'] == df_prop_subset['Review'].unique()[i]]['Maximum recall']    \n",
    "    manual_screening = [0, 0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9, 1]\n",
    "    x_axis = df_prop_subset['percentage of records screened'][0:len(max_recalls_per_prop)]\n",
    "    axes[i].plot(x_axis, max_recalls_per_prop + 0.005, 'k-', linewidth = 1, color = 'grey', linestyle = '--', label = \"maximum recall\") \n",
    "    axes[i].plot(x_axis, manual_screening, 'k-', linewidth = 1, color = 'grey', linestyle = '-', label = \"manual screening\") \n",
    "    axes[i].legend(loc=\"lower right\", fontsize = 12.5)\n",
    "\n",
    "p3.set_titles(col_template = \"{col_name}\", size = 16, weight = 'bold')\n",
    "p3.set_xlabels('Percentage of records screened', size = 16)\n",
    "p3.set_ylabels('Recall', size = 16)\n",
    "p3.set(ylim = (0, 1.01))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c3eca0a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Lineplots with sample size (zoomed in) on the x-axis\n",
    "import math\n",
    "\n",
    "# Optionally choose a subset to plot\n",
    "subset = 'Int'\n",
    "df_prop_subset = df_ss[df_ss['Simulation'].str.contains(subset)]\n",
    "df_prop_subset = df_prop_subset.drop(df_prop_subset[df_prop_subset['Number of records screened'] >= 1001].index)\n",
    "\n",
    "review_len = []\n",
    "review_incl = []\n",
    "for i in range(0, len(df_prop_subset['Review'].unique())):  \n",
    "    review_len.append(len(review_dic[df_prop_subset['Review'].unique()[i]]))\n",
    "    review_incl.append(sum(review_dic[df_prop_subset['Review'].unique()[i]]['label_included']))\n",
    "\n",
    "# For prognosis run:\n",
    "# df_prop_subset = df_prop_subset.drop(df_prop_subset[df_prop_subset['Review'] == 'Prog5'].index)\n",
    "# df_prop_subset['Review_full'] = df_prop_subset['Review_full'].replace({'Prognosis 6': 'Prognosis 5',\n",
    "#                                                                        'Prognosis 7': 'Prognosis 6'})\n",
    "# df_prop_subset['Review'] = df_prop_subset['Review'].replace({'Prog6': 'Prog5',\n",
    "#                                                              'Prog7': 'Prog6'})\n",
    "# df_max_recalls_ed = df_max_recalls_ss\n",
    "# df_max_recalls_ed = df_max_recalls_ed.drop(df_max_recalls_ed[df_max_recalls_ed['Review'] == 'Prog5'].index)\n",
    "# df_max_recalls_ed['Review'] = df_max_recalls_ed['Review'].replace({'Prog6': 'Prog5',\n",
    "#                                                                    'Prog7': 'Prog6'})\n",
    "# review_incl = [review_incl[i] for i in (0,1,2,3,5,6)] \n",
    "# review_len = [review_len[i] for i in (0,1,2,3,5,6)] \n",
    "\n",
    "# For intervention run:\n",
    "df_max_recalls_ed = df_max_recalls_ss\n",
    "df_prop_subset = df_prop_subset.drop(df_prop_subset[df_prop_subset['Review'] == 'Int7'].index)\n",
    "df_prop_subset = df_prop_subset.drop(df_prop_subset[df_prop_subset['Review'] == 'Int8'].index)\n",
    "review_incl = [review_incl[i] for i in (0,1,2,3,4,5)] \n",
    "review_len = [review_len[i] for i in (0,1,2,3,4,5)] \n",
    "\n",
    "# Plot\n",
    "p3 = sns.relplot(data = df_prop_subset, kind='line', \n",
    "                 x = 'Number of records screened', y = 'Recall', \n",
    "                 col = 'Review_full', hue = 'Models', \n",
    "                 col_wrap = 2, aspect = 1.5, legend = False\n",
    "                 )\n",
    "\n",
    "plt.legend(loc='upper right')\n",
    "\n",
    "axes = p3.fig.axes\n",
    "for i in range(0, len(df_prop_subset['Review'].unique())):  \n",
    "    max_recalls_per_ss = df_max_recalls_ed.loc[df_max_recalls_ed['Review'] == df_prop_subset['Review'].unique()[i]]['Maximum recall']   \n",
    "    max_recalls_per_ss = max_recalls_per_ss.dropna()\n",
    "    max_recalls_per_ss = max_recalls_per_ss[0:1000] # added\n",
    "    \n",
    "    if len(max_recalls_per_ss) >= 1000:\n",
    "        x_length = [0,1000]\n",
    "        man_screen = [0, 1000/review_len[i]]\n",
    "    else:\n",
    "        x_length = [0, len(max_recalls_per_ss)]\n",
    "        man_screen = [0, 1]  \n",
    "\n",
    "    axes[i].plot(x_length, man_screen, 'k-', linewidth = 1, color = 'grey', linestyle = '-', label = \"manual screening\") \n",
    "  \n",
    "    if len(max_recalls_per_ss) < 1000:\n",
    "        x_axis = sample_sizes_mr[0:len(max_recalls_per_ss)]\n",
    "    else:\n",
    "        x_axis = sample_sizes_mr[0:1000] #sample_sizes_mr[0:len(max_recalls_per_ss)] #df_prop_subset['Number of records screened'][0:len(max_recalls_per_ss)]\n",
    "    axes[i].plot(x_axis, max_recalls_per_ss + 0.005, 'k-', linewidth = 1, color = 'black', \n",
    "                 linestyle = '--', label = \"maximum recall\") \n",
    "    #axes[i].legend(loc = \"lower right\")  \n",
    "    \n",
    "p3.set_titles(col_template = \"{col_name}\", size = 16, weight = 'bold')\n",
    "p3.set_xlabels('Number of records screened', size = 16)\n",
    "p3.set_ylabels('Cumulative recall', size = 16)\n",
    "p3.set(ylim = (0, 1.01))\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f8a4566",
   "metadata": {},
   "outputs": [],
   "source": [
    "#### PROGNOSIS INCLUSION PROPORTION VARIATIONS ####"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "feb61852",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a function to create a dictionary with subsets of a dataset consisting of varying numbers of records and inclusions:\n",
    "\n",
    "def df_var_dict(review_name, df, sizes, incl_prop): \n",
    "    \n",
    "    '''\n",
    "    df (pandas.DataFrame):    a dataframe of a review with at least containing the columns 'abstract', 'title', and 'label_included'\n",
    "    sizes (list):             a list of integers of dataframe sizes to vary  \n",
    "    incl_prop (list):         a list of integers of inclusion proportions to vary \n",
    "    '''\n",
    "    \n",
    "    # First a list of possible size/inclusion combinations is created:\n",
    "    unique_combinations = []\n",
    "    \n",
    "    # The maximum possible size for different inclusion\n",
    "    sizes = sizes #[round(df.loc[df['label_included'] == 0].shape[0]/(1-min(incl_prop)))]\n",
    "    \n",
    "    for n in sizes:\n",
    "        for m in incl_prop:\n",
    "            unique_combinations.append([n,m])\n",
    "\n",
    "    # Then a dictionary is created with for each combination a sample of the dataset. If the sample cannot be retrieved,\n",
    "    # i.e. the dataset size is too small or the inclusion proportion is not available, the respective item in the dictionary remains empty.\n",
    "    df_dict = {}\n",
    "    \n",
    "    # For each combination of size/inclusion proportion:\n",
    "    for i in range(len(unique_combinations)):\n",
    "        # Check if the dataframe includes enough records to sample the respective size\n",
    "        if len(df) >= unique_combinations[i][0]:\n",
    "            \n",
    "            \n",
    "            # Check if the inclusion proportion is possible for the respective size\n",
    "            if df.loc[df['label_included'] == 1].shape[0] >= int(unique_combinations[i][0] * unique_combinations[i][1]) and df.loc[df['label_included'] == 0].shape[0] >= int(unique_combinations[i][0] - (unique_combinations[i][0] * unique_combinations[i][1])):\n",
    "                # If so:\n",
    "                # Sample random inclusions\n",
    "                incl = df.loc[df['label_included'] == 1].sample(n = int(unique_combinations[i][0] * unique_combinations[i][1]), replace = False)\n",
    "                # Sample random exclusions\n",
    "                excl = df.loc[df['label_included'] == 0].sample(n = int(unique_combinations[i][0] - (unique_combinations[i][0] * unique_combinations[i][1])), replace = False)\n",
    "                # Create a dataframe of the inclusions and exclusions\n",
    "                df_new = pd.concat([incl, excl]).sort_values('authors').reset_index()\n",
    "                name = review_name + \"_\" + str(len(df_new)) + \"_\" + str(unique_combinations[i][1])\n",
    "            # If the size/inclusion proportion is not possible, leave the dataframe empty\n",
    "            else:\n",
    "                name = review_name + \"_\" + str(len(df_new)) + \"_\" + str(unique_combinations[i][1])\n",
    "                df_new = [] \n",
    "        else:\n",
    "            df_new = []\n",
    "        \n",
    "        if len(df_new) > 0:\n",
    "            if df_new['label_included'].sum() <= 10:\n",
    "                df_new = []\n",
    "        \n",
    "        # Store the dataframe in the dictionary of all retrieved dataframes\n",
    "        if len(df_new) > 0:\n",
    "            df_dict[name] = df_new\n",
    "  \n",
    "    return(df_dict)\n",
    "\n",
    "# Apply the function:\n",
    "# incl_prop = [0.01, 0.025, 0.05, 0.075]\n",
    "# ss_dict = df_var_dict(review_name = 'Prog1', df = dfs_prog['Prog1'], sizes = [len(dfs_prog['Prog1'])], incl_prop = incl_prop)\n",
    "# ss_dict.update(df_var_dict(review_name = 'Prog2', df = dfs_prog['Prog2'], sizes = [len(dfs_prog['Prog2'])], incl_prop = incl_prop))\n",
    "# ss_dict.update(df_var_dict(review_name = 'Prog3', df = dfs_prog['Prog3'], sizes = [len(dfs_prog['Prog3'])], incl_prop = incl_prop))\n",
    "# ss_dict.update(df_var_dict(review_name = 'Prog4', df = dfs_prog['Prog4'], sizes = [len(dfs_prog['Prog4'])], incl_prop = incl_prop))\n",
    "# ss_dict.update(df_var_dict(review_name = 'Prog6', df = dfs_prog['Prog6'], sizes = [len(dfs_prog['Prog6'])], incl_prop = incl_prop))\n",
    "# ss_dict.update(df_var_dict(review_name = 'Prog7', df = dfs_prog['Prog7'], sizes = [len(dfs_prog['Prog7'])], incl_prop = incl_prop))\n",
    "\n",
    "# ss_dict.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "575153a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Added for last analysis\n",
    "sizes = [2000]\n",
    "incl_prop = [0.025]#incl_prop = [0.01, 0.025, 0.05, 0.075]\n",
    "ss_dict = df_var_dict(review_name = 'Int1', df = dfs_int['Int1'], sizes = sizes, incl_prop = incl_prop)\n",
    "ss_dict.update(df_var_dict(review_name = 'Int2', df = dfs_int['Int2'], sizes = sizes, incl_prop = incl_prop))\n",
    "#ss_dict.update(df_var_dict(review_name = 'Int3', df = dfs_int['Int3'], sizes = sizes, incl_prop = incl_prop))\n",
    "ss_dict.update(df_var_dict(review_name = 'Int4', df = dfs_int['Int4'], sizes = sizes, incl_prop = incl_prop))\n",
    "#ss_dict.update(df_var_dict(review_name = 'Int5', df = dfs_int['Int5'], sizes = sizes, incl_prop = incl_prop))\n",
    "ss_dict.update(df_var_dict(review_name = 'Int6', df = dfs_int['Int6'], sizes = sizes, incl_prop = incl_prop))\n",
    "\n",
    "sizes = [1000]\n",
    "incl_prop = [0.05]\n",
    "ss_dict.update(df_var_dict(review_name = 'Int1', df = dfs_int['Int1'], sizes = sizes, incl_prop = incl_prop))\n",
    "ss_dict.update(df_var_dict(review_name = 'Int2', df = dfs_int['Int2'], sizes = sizes, incl_prop = incl_prop))\n",
    "#ss_dict.update(df_var_dict(review_name = 'Int3', df = dfs_int['Int3'], sizes = sizes, incl_prop = incl_prop))\n",
    "ss_dict.update(df_var_dict(review_name = 'Int4', df = dfs_int['Int4'], sizes = sizes, incl_prop = incl_prop))\n",
    "#ss_dict.update(df_var_dict(review_name = 'Int5', df = dfs_int['Int5'], sizes = sizes, incl_prop = incl_prop))\n",
    "ss_dict.update(df_var_dict(review_name = 'Int6', df = dfs_int['Int6'], sizes = sizes, incl_prop = incl_prop))\n",
    "\n",
    "sizes = [500]\n",
    "incl_prop = [0.10]\n",
    "ss_dict.update(df_var_dict(review_name = 'Int1', df = dfs_int['Int1'], sizes = sizes, incl_prop = incl_prop))\n",
    "ss_dict.update(df_var_dict(review_name = 'Int2', df = dfs_int['Int2'], sizes = sizes, incl_prop = incl_prop))\n",
    "#ss_dict.update(df_var_dict(review_name = 'Int3', df = dfs_int['Int3'], sizes = sizes, incl_prop = incl_prop))\n",
    "ss_dict.update(df_var_dict(review_name = 'Int4', df = dfs_int['Int4'], sizes = sizes, incl_prop = incl_prop))\n",
    "#ss_dict.update(df_var_dict(review_name = 'Int5', df = dfs_int['Int5'], sizes = sizes, incl_prop = incl_prop))\n",
    "ss_dict.update(df_var_dict(review_name = 'Int6', df = dfs_int['Int6'], sizes = sizes, incl_prop = incl_prop))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d248bb38",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Added for last analysis\n",
    "sizes = [2000]\n",
    "incl_prop = [0.025]#incl_prop = [0.01, 0.025, 0.05, 0.075]\n",
    "ss_dict.update(df_var_dict(review_name = 'Prog1', df = dfs_prog['Prog1'], sizes = sizes, incl_prop = incl_prop))\n",
    "ss_dict.update(df_var_dict(review_name = 'Prog2', df = dfs_prog['Prog2'], sizes = sizes, incl_prop = incl_prop))\n",
    "ss_dict.update(df_var_dict(review_name = 'Prog3', df = dfs_prog['Prog3'], sizes = sizes, incl_prop = incl_prop))\n",
    "ss_dict.update(df_var_dict(review_name = 'Prog4', df = dfs_prog['Prog4'], sizes = sizes, incl_prop = incl_prop))\n",
    "ss_dict.update(df_var_dict(review_name = 'Prog5', df = dfs_prog['Prog5'], sizes = sizes, incl_prop = incl_prop))\n",
    "ss_dict.update(df_var_dict(review_name = 'Prog6', df = dfs_prog['Prog6'], sizes = sizes, incl_prop = incl_prop))\n",
    "ss_dict.update(df_var_dict(review_name = 'Prog7', df = dfs_prog['Prog7'], sizes = sizes, incl_prop = incl_prop))\n",
    "\n",
    "sizes = [1000]\n",
    "incl_prop = [0.05]\n",
    "ss_dict.update(df_var_dict(review_name = 'Prog1', df = dfs_prog['Prog1'], sizes = sizes, incl_prop = incl_prop))\n",
    "ss_dict.update(df_var_dict(review_name = 'Prog2', df = dfs_prog['Prog2'], sizes = sizes, incl_prop = incl_prop))\n",
    "ss_dict.update(df_var_dict(review_name = 'Prog3', df = dfs_prog['Prog3'], sizes = sizes, incl_prop = incl_prop))\n",
    "ss_dict.update(df_var_dict(review_name = 'Prog4', df = dfs_prog['Prog4'], sizes = sizes, incl_prop = incl_prop))\n",
    "ss_dict.update(df_var_dict(review_name = 'Prog5', df = dfs_prog['Prog5'], sizes = sizes, incl_prop = incl_prop))\n",
    "ss_dict.update(df_var_dict(review_name = 'Prog6', df = dfs_prog['Prog6'], sizes = sizes, incl_prop = incl_prop))\n",
    "ss_dict.update(df_var_dict(review_name = 'Prog7', df = dfs_prog['Prog7'], sizes = sizes, incl_prop = incl_prop))\n",
    "\n",
    "sizes = [500]\n",
    "incl_prop = [0.10]\n",
    "ss_dict.update(df_var_dict(review_name = 'Prog1', df = dfs_prog['Prog1'], sizes = sizes, incl_prop = incl_prop))\n",
    "ss_dict.update(df_var_dict(review_name = 'Prog2', df = dfs_prog['Prog2'], sizes = sizes, incl_prop = incl_prop))\n",
    "ss_dict.update(df_var_dict(review_name = 'Prog3', df = dfs_prog['Prog3'], sizes = sizes, incl_prop = incl_prop))\n",
    "ss_dict.update(df_var_dict(review_name = 'Prog4', df = dfs_prog['Prog4'], sizes = sizes, incl_prop = incl_prop))\n",
    "ss_dict.update(df_var_dict(review_name = 'Prog5', df = dfs_prog['Prog5'], sizes = sizes, incl_prop = incl_prop))\n",
    "ss_dict.update(df_var_dict(review_name = 'Prog6', df = dfs_prog['Prog6'], sizes = sizes, incl_prop = incl_prop))\n",
    "ss_dict.update(df_var_dict(review_name = 'Prog7', df = dfs_prog['Prog7'], sizes = sizes, incl_prop = incl_prop))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c487e11",
   "metadata": {},
   "outputs": [],
   "source": [
    "ss_sims = list(ss_dict.keys())\n",
    "ss_sims"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "472a4c9d",
   "metadata": {},
   "outputs": [],
   "source": [
    "for key,value in ss_dict.items():\n",
    "    print(key + \" \" + str(len(value['label_included'])) + \" \" + str(value['label_included'].sum()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e13047a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run the default simulation for each dataset-inclusion combination\n",
    "\n",
    "# multiple_sims_sizes = []\n",
    "# for key, value in ss_dict.items():\n",
    "#     print(key)\n",
    "#     sim = ASReview_simulation(review_id = key, review_data = value, path_name = path_name, n_simulations = 10)\n",
    "#     multiple_sims_sizes.append(sim)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5245655e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Define the classification model(s) to be tested\n",
    "# train_models = [NaiveBayesClassifier()] \n",
    "\n",
    "# # Define the feature extraction model(s) to be tested\n",
    "# feature_models = [Tfidf()]\n",
    "\n",
    "# # Define the query model(s) to be tested (for now no variation in query models)query_models = [MaxQuery()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dca25e95",
   "metadata": {},
   "outputs": [],
   "source": [
    "# train_models = [NaiveBayesClassifier()] \n",
    "# feature_models = [Tfidf()]\n",
    "# query_models = [MaxQuery()]\n",
    "\n",
    "# sim_list_names_ss = []\n",
    "# for review in ss_dict:\n",
    "#     for train_model in train_models:\n",
    "#         for feature_model in feature_models:\n",
    "#             for query_model in query_models:\n",
    "#                 review_id = str(review + \"_\" + train_model.name + \"_\" + feature_model.name + \"_\" + query_model.name)\n",
    "#                 sim_list_names_ss.append(review_id)\n",
    "# sim_list_names_ss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "02684671",
   "metadata": {},
   "outputs": [],
   "source": [
    "# The resulting output from the HPC can then be retrieved and merged as follows:\n",
    "\n",
    "train_models = [NaiveBayesClassifier()] \n",
    "feature_models = [Tfidf()]\n",
    "query_models = [MaxQuery()]\n",
    "\n",
    "# Create a list of the review-model combination names\n",
    "sim_list_names_ss = []\n",
    "for review in ss_sims:\n",
    "    for train_model in train_models:\n",
    "        for feature_model in feature_models:\n",
    "            for query_model in query_models:\n",
    "                review_id = str(review + \"_\" + train_model.name + \"_\" + feature_model.name + \"_\" + query_model.name)\n",
    "                sim_list_names_ss.append(review_id)\n",
    "\n",
    "# Derive the results from the HPC retrieved pickle files with each having the rankings of a single simulation\n",
    "multiple_sims_sizes = []\n",
    "n_simulations = 200 # number of simulations per review-model combination\n",
    "for i in range(0, len(sim_list_names_ss)):\n",
    "    raw_output_ss = {}\n",
    "    for j in range(1,n_simulations+1):\n",
    "        if Path(path_results_HPC +'sim_{review_id}_{sim}.p'.format(review_id=sim_list_names_ss[i], sim=j)).is_file():\n",
    "            with open(path_results_HPC + 'sim_{review_id}_{sim}.p'.format(review_id=sim_list_names_ss[i], sim=j),'rb') as f:\n",
    "                raw_output_ss.update(pickle.load(f))\n",
    "    if len(raw_output_ss) > 0:\n",
    "        multiple_sims_sizes.append((sim_list_names_ss[i], len(ss_dict['_'.join(sim_list_names_ss[0].split('_')[0:3])]), n_simulations, raw_output_ss))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d64f1daa",
   "metadata": {},
   "outputs": [],
   "source": [
    "multiple_sims_sizes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "54fd2dbc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save (back-up) the file with the simulation results\n",
    "\n",
    "# with open(path_results + 'multiple_sims_sizes_all_int-prog.p','wb') as f:\n",
    "#     pickle.dump(multiple_sims_sizes, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f06d0a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Open the file with the simulation results\n",
    "\n",
    "with open(path_results + 'multiple_sims_sizes_all_int-prog.p','rb') as f:\n",
    "    multiple_sims_sizes = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f0c574f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the proportions and sample sizes for evaluation\n",
    "# proportions = [0, 0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9, 1]\n",
    "# sample_sizes = [50, 250, 500, 1000, 2500, 5000, 7500, 10000]\n",
    "\n",
    "# Define the proportions and sample sizes for evaluation of the ranking and creating the output metrics\n",
    "\n",
    "# The proportion of screened records at which to calculate the performance metrics\n",
    "proportions = [0, 0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9, 1]\n",
    "\n",
    "# The number of screened records at which to calculate the performance metrics\n",
    "sample_sizes = list(map(int,list(np.linspace(0, 99, 100,retstep = True)[0]))) + list(map(int,list(np.linspace(100, 12400, 124,retstep = True)[0])))\n",
    "\n",
    "#output_sizes = ASReview_output(multiple_sims_sizes, proportions, sample_sizes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ddab554e",
   "metadata": {},
   "outputs": [],
   "source": [
    "#output_sizes_nwss = ASReview_output_nwss(multiple_sims_sizes, proportions, sample_sizes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec35a4d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ! DON'T OVERWRITE THIS PART\n",
    "# Save (back-up) a file with the computed output\n",
    "\n",
    "# with open(path_results + 'sims_output_saved_all_perc_int-prog_final_extra.p','wb') as f:\n",
    "#     pickle.dump(output_sizes, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "973321c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ! DON'T OVERWRITE THIS PART\n",
    "# Save (back-up) a file with the computed output\n",
    "\n",
    "# with open(path_results + 'sims_output_saved_all_perc_nwss_int-prog_final.p','wb') as f:\n",
    "#     pickle.dump(output_sizes_nwss, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d213f656",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Open the files with the computed output\n",
    "\n",
    "with open(path_results + 'sims_output_saved_all_perc_int-prog_final_extra.p','rb') as f:\n",
    "    output_sizes = pickle.load(f)\n",
    "    \n",
    "with open(path_results + 'sims_output_saved_all_perc_nwss_int-prog_final.p','rb') as f:\n",
    "    output_sizes_nwss = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "56f9fc71",
   "metadata": {},
   "outputs": [],
   "source": [
    "evaluation_sizes = {}\n",
    "for i in range(0, len(output_sizes)):\n",
    "    evaluation_sizes[output_sizes[i][0]] = []\n",
    "    evaluation_sizes[output_sizes[i][0]].append(output_sizes[i][3:7])\n",
    "    \n",
    "evaluation_sizes_nwss = {}\n",
    "for i in range(0, len(output_sizes_nwss)):\n",
    "    evaluation_sizes_nwss[output_sizes_nwss[i][0]] = []\n",
    "    evaluation_sizes_nwss[output_sizes_nwss[i][0]].append(output_sizes_nwss[i][3:7])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "413bdf7b",
   "metadata": {},
   "outputs": [],
   "source": [
    "evaluation_sizes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db4c960b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Output table for proportions\n",
    "\n",
    "n_simulations = 200\n",
    "props = [0, 0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9, 1] \n",
    "\n",
    "proportions = []\n",
    "for i in range(0, len(props)):\n",
    "    proportions.append(str(int(props[i]*100)) + '%')\n",
    "length_prop = len(proportions) * n_simulations\n",
    "\n",
    "df_prop = pd.DataFrame()\n",
    "for key, value in evaluation_sizes.items():\n",
    "    names = key.split('_')\n",
    "    name = str(names[0]+\"_\"+names[1]+\"_\"+names[2])\n",
    "    review = [name] * length_prop #[names[0]] * length_prop\n",
    "    n_records = [names[1]] * length_prop\n",
    "    n_inclusions = [names[2]] * length_prop\n",
    "    train_model = [names[3]] * length_prop\n",
    "    feature_model = [names[4]] * length_prop\n",
    "    simulations = []\n",
    "    \n",
    "    props = proportions * n_simulations\n",
    "    for i in range(1, n_simulations+1):\n",
    "        simulations.append(list(np.repeat(i, len(proportions))))\n",
    "    recalls = value[0][0]['Recall']\n",
    "    sim_number = value[0][0]['Simulation']\n",
    "\n",
    "    simulations = [item for sublist in simulations for item in sublist]\n",
    "\n",
    "    df_sim = pd.DataFrame(list(zip(review, sim_number, n_records, n_inclusions, train_model, feature_model, simulations, props, recalls)),columns = ['Review', 'Simulation number', 'Records', 'Inclusions', 'Train model', 'Feature model', 'Simulation', 'percentage of records screened','recall'])\n",
    "    df_prop = pd.concat([df_prop, df_sim])\n",
    "    \n",
    "df_prop = df_prop.reset_index(drop = True)\n",
    "\n",
    "df_prop['Review_full'] = \"\"\n",
    "\n",
    "df_prop['Review_full'] = df_prop['Review'].apply(lambda x: x.split('_')[0].replace('Prog', 'Prognosis '))\n",
    "for i in range(0, len(df_prop['Review_full'])):\n",
    "    if 'Int' in df_prop['Review_full'][i]:\n",
    "        df_prop['Review_full'][i] = df_prop['Review_full'][i].replace('Int', 'Intervention ')\n",
    "\n",
    "df_prop['Simulation'] = \"\"\n",
    "for row in range(0, len(df_prop)):\n",
    "      df_prop['Simulation'][row] = str(df_prop['Review_full'][row]) + \" (\" + str(df_prop['Records'][row]) + \" - \" + str(df_prop['Inclusions'][row]) + \")\" \n",
    "\n",
    "df_prop['Models'] = \"\"\n",
    "for row in range(0, len(df_prop)):\n",
    "      df_prop['Models'][row] = str(df_prop['Feature model'][row]) + \" - \" + str(df_prop['Train model'][row]) #+ \" - \" + str(df_prop['Feature model'][row])\n",
    "\n",
    "df_prop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1fa410a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import collections\n",
    "\n",
    "# Max recall:\n",
    "\n",
    "# Proportions\n",
    "proportions = [0, 0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9, 1] \n",
    "\n",
    "review_name = []\n",
    "max_recalls = []\n",
    "review_dic_ord = collections.OrderedDict(sorted(ss_dict.items()))\n",
    "\n",
    "# Maximum recall per review-model and per proportion\n",
    "for key, value in review_dic_ord.items():\n",
    "    \n",
    "    review_name.append([key]*len(proportions))\n",
    "    records = len(review_dic_ord[key])\n",
    "    inclusions = review_dic_ord[key]['label_included'].sum()\n",
    "    \n",
    "    for i in proportions:\n",
    "        \n",
    "        number_screened = round(float(i*records))\n",
    "        \n",
    "        # Calculate the true positives (TP) as the highest number of positive labels possible before the cutoff\n",
    "        if inclusions >= number_screened:\n",
    "            TP = number_screened\n",
    "        else:\n",
    "            TP = inclusions\n",
    "            \n",
    "        # Calculate the false negatives (FN) as the smallest number of positive labels after the cutoff\n",
    "        if inclusions >= number_screened:\n",
    "            FN = inclusions - number_screened\n",
    "        else:\n",
    "            FN = 0\n",
    "            \n",
    "        # Calculate the maximum recall based on the TP and FN\n",
    "        max_recall = round((TP/(TP+FN)), 2)\n",
    "        max_recalls.append(max_recall)\n",
    "        \n",
    "review_name = [item for sublist in review_name for item in sublist]   \n",
    "\n",
    "df_max_recalls_ss = pd.DataFrame(list(zip(review_name, max_recalls)))\n",
    "df_max_recalls_ss = df_max_recalls_ss.rename(columns={0: 'Review', 1: 'Maximum recall'})\n",
    "df_max_recalls_ss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e6955ef8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Lineplots with percentage on the x-axis\n",
    "import math\n",
    "\n",
    "# Optionally choose a subset to plot\n",
    "subset = '2000'\n",
    "df_prop_subset = df_prop[df_prop['Simulation'].str.contains(subset)]\n",
    "df_max_recalls_ed = df_max_recalls_ss\n",
    "\n",
    "# For prognosis run:\n",
    "df_prop_subset = df_prop_subset[~df_prop_subset.Review_full.str.contains('Prognosis 5')]\n",
    "df_prop_subset = df_prop_subset[~df_prop_subset.Review_full.str.contains('Prognosis 2')]\n",
    "df_prop_subset = df_prop_subset[~df_prop_subset.Review_full.str.contains('Prognosis 1')]\n",
    "df_prop_subset['Review_full'] = df_prop_subset['Review_full'].replace({'Prognosis 6': 'Prognosis 5',\n",
    "                                                                       'Prognosis 7': 'Prognosis 6'})\n",
    "df_prop_subset['Review'] = df_prop_subset['Review'].str.replace('Prog6', 'Prog5')\n",
    "df_prop_subset['Review'] = df_prop_subset['Review'].str.replace('Prog7', 'Prog6')\n",
    "df_prop_subset['Simulation'] = df_prop_subset['Simulation'].str.replace('Prognosis 6', 'Prognosis 5')\n",
    "df_prop_subset['Simulation'] = df_prop_subset['Simulation'].str.replace('Prognosis 7', 'Prognosis 6')\n",
    "\n",
    "df_max_recalls_ed = df_max_recalls_ed[~df_max_recalls_ed.Review.str.contains('Prog5')]\n",
    "df_max_recalls_ed['Review'] = df_max_recalls_ed['Review'].str.replace('Prog6', 'Prog5')\n",
    "df_max_recalls_ed['Review'] = df_max_recalls_ed['Review'].str.replace('Prog7', 'Prog6')\n",
    "\n",
    "# Plot\n",
    "p3 = sns.catplot(data = df_prop_subset, kind = 'point', \n",
    "                 x = 'percentage of records screened', y = 'recall', \n",
    "                 col = 'Simulation', hue = 'Models', errorbar = 'ci',\n",
    "                 col_wrap = 2, aspect = 1.4, legend = False\n",
    "                 )\n",
    "\n",
    "axes = p3.fig.axes\n",
    "\n",
    "for i in range(0, len(df_prop_subset['Review'].unique())): \n",
    "    max_recalls_per_prop = df_max_recalls_ed.loc[df_max_recalls_ed['Review'] == df_prop_subset['Review'].unique()[i]]['Maximum recall']    \n",
    "    manual_screening = [0, 0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9, 1]\n",
    "    x_axis = df_prop_subset['percentage of records screened'][0:len(max_recalls_per_prop)]\n",
    "    axes[i].plot(x_axis, max_recalls_per_prop + 0.005, 'k-', linewidth = 1, color = 'grey', linestyle = '--', label = \"maximum recall\") \n",
    "    axes[i].plot(x_axis, manual_screening, 'k-', linewidth = 1, color = 'grey', linestyle = '-', label = \"manual screening\") \n",
    "    axes[i].legend(loc=\"lower right\", fontsize = 12.5)\n",
    "\n",
    "p3.set_titles(col_template = \"{col_name}\", size = 16, weight = 'bold')\n",
    "p3.set_xlabels('Percentage of records screened', size = 16)\n",
    "p3.set_ylabels('Recall', size = 16)\n",
    "p3.set(ylim = (0, 1.01))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "06134138",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Lineplots with percentage on the x-axis\n",
    "import math\n",
    "\n",
    "# Optionally choose a subset to plot\n",
    "subset = '0.01'\n",
    "df_prop_subset = df_prop[df_prop['Simulation'].str.contains(subset)]\n",
    "df_max_recalls_ed = df_max_recalls_ss\n",
    "\n",
    "print(df_max_recalls_ed['Review'].unique())\n",
    "\n",
    "# For prognosis run:\n",
    "df_prop_subset = df_prop_subset[~df_prop_subset.Review_full.str.contains('Prognosis 5')]\n",
    "df_prop_subset['Review_full'] = df_prop_subset['Review_full'].replace({'Prognosis 6': 'Prognosis 5',\n",
    "                                                                       'Prognosis 7': 'Prognosis 6'})\n",
    "df_prop_subset['Review'] = df_prop_subset['Review'].str.replace('Prog6', 'Prog5')\n",
    "df_prop_subset['Review'] = df_prop_subset['Review'].str.replace('Prog7', 'Prog6')\n",
    "df_prop_subset['Simulation'] = df_prop_subset['Simulation'].str.replace('Prognosis 6', 'Prognosis 5')\n",
    "df_prop_subset['Simulation'] = df_prop_subset['Simulation'].str.replace('Prognosis 7', 'Prognosis 6')\n",
    "\n",
    "\n",
    "df_max_recalls_ed = df_max_recalls_ed[~df_max_recalls_ed.Review.str.contains('Prog5')]\n",
    "df_max_recalls_ed['Review'] = df_max_recalls_ed['Review'].str.replace('Prog6', 'Prog5')\n",
    "df_max_recalls_ed['Review'] = df_max_recalls_ed['Review'].str.replace('Prog7', 'Prog6')\n",
    "\n",
    "df_prop_subset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19ab8c22",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Output table for precision\n",
    "\n",
    "df_prec = pd.DataFrame()\n",
    "length = n_simulations\n",
    "for key, value in evaluation_sizes.items():\n",
    "    names = key.split('_')\n",
    "    review = [names[0]] * length\n",
    "    n_records = [names[1]] * length\n",
    "    rel_records = [names[2]] * length\n",
    "    train_model = [names[3] + \" & \" + names[4]] * length\n",
    "    simulations = range(1, n_simulations+1)\n",
    "    precision = value[0][0]['Precision'] ###\n",
    "    df_sim = pd.DataFrame(list(zip(review, n_records, rel_records, train_model, simulations, precision)),\n",
    "                           columns = ['Review', 'Total records', 'Relevant records', 'Train model', 'Simulation', 'Precision@95%'])\n",
    "    df_prec = pd.concat([df_prec, df_sim])\n",
    "\n",
    "    df_prec = df_prec.reset_index(drop = True)\n",
    "\n",
    "df_prec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "68c12916",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_wss = pd.DataFrame()\n",
    "length = n_simulations\n",
    "for key, value in evaluation_sizes.items():\n",
    "    names = key.split('_')\n",
    "    review = [names[0]] * length\n",
    "    n_records= [names[1]] * length\n",
    "    rel_records = [names[2]] * length\n",
    "    train_model = [names[3] + \" & \" + names[4]] * length\n",
    "    simulations = range(1, n_simulations+1)\n",
    "    wss = value[0][2]['WSS-95%']\n",
    "    n_saved = value[0][3]['Workload reduction (records)']\n",
    "    df_sim = pd.DataFrame(list(zip(review, n_records, rel_records, train_model, simulations, wss, n_saved)),\n",
    "                           columns = ['Review', 'Total records', 'Relevant records', 'Train model', 'Simulation', 'WSS@95%', 'Workload reduction (n)'])\n",
    "    df_wss = pd.concat([df_wss, df_sim])\n",
    "\n",
    "    df_wss = df_wss.reset_index(drop = True)\n",
    "    \n",
    "df_wss\n",
    "\n",
    "df_nwss = pd.DataFrame()\n",
    "length = n_simulations\n",
    "for key, value in evaluation_sizes_nwss.items():\n",
    "    names = key.split('_')\n",
    "    review = [names[0]] * length\n",
    "    n_records = [names[1]] * length\n",
    "    rel_records = [names[2]] * length\n",
    "    train_model = [names[3] + \" & \" + names[4]] * length\n",
    "    simulations = range(1, n_simulations+1)\n",
    "    nwss = value[0][0]['nWSS-95%']\n",
    "    df_sim = pd.DataFrame(list(zip(review, n_records, rel_records, train_model, simulations, nwss)),\n",
    "                           columns = ['Review', 'Total records', 'Relevant records', 'Train model', 'Simulation', 'n-WSS@95%'])\n",
    "    df_nwss = pd.concat([df_nwss, df_sim])\n",
    "\n",
    "    df_nwss = df_nwss.reset_index(drop = True)\n",
    "\n",
    "df_wss = pd.merge(df_wss[['Review', 'Total records', 'Relevant records', 'Train model', 'Simulation', 'WSS@95%', 'Workload reduction (n)']], \n",
    "                  df_nwss[['Review', 'Total records', 'Relevant records', 'Train model', 'Simulation', 'n-WSS@95%']],\n",
    "                  how = 'left', on = ['Review', 'Total records', 'Relevant records', 'Train model', 'Simulation'])\n",
    "\n",
    "df_wss = df_wss.reindex(columns = ['Review', 'Train model', 'Total records', 'Relevant records', 'Query model', 'Simulation', 'WSS@95%', 'n-WSS@95%', 'Workload reduction (n)']) ###\n",
    "df_wss['Workload reduction (hours)'] = round((df_wss['Workload reduction (n)'] * 0.5 / 60), 1) ###\n",
    "df_wss[0:20]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e3bbd30",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Output table for combined wss and precision (for results)\n",
    "import math\n",
    "\n",
    "df_wss_prec = pd.merge(df_wss, df_prec, on=['Review', 'Total records', 'Relevant records', 'Train model', 'Simulation'])\n",
    "\n",
    "df1 = df_wss_prec.groupby(['Review', 'Total records', 'Relevant records', 'Train model'])['WSS@95%'].agg(Mean1 = 'mean',\n",
    "                                                                                                         Count1 = 'count',\n",
    "                                                                                                        SD1 = 'std').reset_index()\n",
    "\n",
    "ci95_hi_1 = []\n",
    "ci95_lo_1 = []\n",
    "\n",
    "for i in df1.index:\n",
    "    m, c, s = df1.loc[i][4:7]\n",
    "    ci95_hi_1.append(\"{:.3f}\".format(m + 1.96*s/math.sqrt(c)))\n",
    "    ci95_lo_1.append(\"{:.3f}\".format(m - 1.96*s/math.sqrt(c)))\n",
    "df1['ci95_hi_1'] = ci95_hi_1\n",
    "df1['ci95_lo_1'] = ci95_lo_1\n",
    "df1['Mean1'] = df1['Mean1'].apply(lambda x: '{:,.3f}'.format(x)) #remove for plots\n",
    "\n",
    "df2 = df_wss_prec.groupby(['Review', 'Total records', 'Relevant records', 'Train model'])['n-WSS@95%'].agg(Mean2 = 'mean',\n",
    "                                                                                                           Count2 = 'count',\n",
    "                                                                                                           SD2 = 'std').reset_index()\n",
    "\n",
    "ci95_hi_2 = []\n",
    "ci95_lo_2 = []\n",
    "\n",
    "for i in df2.index:\n",
    "    m, c, s = df2.loc[i][4:7]\n",
    "    ci95_hi_2.append(\"{:.3f}\".format(m + 1.96*s/math.sqrt(c)))\n",
    "    ci95_lo_2.append(\"{:.3f}\".format(m - 1.96*s/math.sqrt(c)))\n",
    "df2['ci95_hi_2'] = ci95_hi_2\n",
    "df2['ci95_lo_2'] = ci95_lo_2\n",
    "df2['Mean2'] = df2['Mean2'].apply(lambda x: '{:,.3f}'.format(x)) #remove for plots\n",
    "\n",
    "df3 = df_wss_prec.groupby(['Review', 'Total records', 'Relevant records', 'Train model'])['Precision@95%'].agg(Mean3 = 'mean',\n",
    "                                                                                                               Count3 = 'count',\n",
    "                                                                                                                SD3 = 'std').reset_index()\n",
    "ci95_hi_3 = []\n",
    "ci95_lo_3 = []\n",
    "\n",
    "for i in df3.index:\n",
    "    m, c, s = df3.loc[i][4:7]\n",
    "    ci95_hi_3.append(\"{:.3f}\".format(m + 1.96*s/math.sqrt(c)))\n",
    "    ci95_lo_3.append(\"{:.3f}\".format(m - 1.96*s/math.sqrt(c)))\n",
    "df3['ci95_hi_3'] = ci95_hi_3\n",
    "df3['ci95_lo_3'] = ci95_lo_3\n",
    "df3['Mean3'] = df3['Mean3'].apply(lambda x: '{:,.3f}'.format(x)) #remove for plots\n",
    "\n",
    "df4 = df_wss_prec.groupby(['Review', 'Total records', 'Relevant records', 'Train model'])['Workload reduction (n)'].agg(Mean4 = 'mean',\n",
    "                                                                                     Count4 = 'count',\n",
    "                                                                                     SD4 = 'std').reset_index()\n",
    "\n",
    "ci95_hi_4 = []\n",
    "ci95_lo_4 = []\n",
    "for i in df4.index:\n",
    "    m, c, s = df4.loc[i][4:7] ###\n",
    "    ci95_hi_4.append(\"{:.0f}\".format(m + 1.96*s/math.sqrt(c)))\n",
    "    ci95_lo_4.append(\"{:.0f}\".format(m - 1.96*s/math.sqrt(c)))\n",
    "df4['ci95_hi_4'] = ci95_hi_4\n",
    "df4['ci95_lo_4'] = ci95_lo_4\n",
    "df4['Mean4'] = df4['Mean4'].apply(lambda x: '{:,.0f}'.format(x))\n",
    "\n",
    "df5 = df_wss_prec.groupby(['Review', 'Total records', 'Relevant records', 'Train model'])['Workload reduction (hours)'].agg(Mean5 = 'mean',\n",
    "                                                                                     Count5 = 'count',\n",
    "                                                                                     SD5 = 'std').reset_index()\n",
    "\n",
    "ci95_hi_5 = []\n",
    "ci95_lo_5 = []\n",
    "\n",
    "for i in df5.index:\n",
    "    m, c, s = df5.loc[i][4:7] ###\n",
    "    ci95_hi_5.append(\"{:.1f}\".format(m + 1.96*s/math.sqrt(c)))\n",
    "    ci95_lo_5.append(\"{:.1f}\".format(m - 1.96*s/math.sqrt(c)))\n",
    "df5['ci95_hi_5'] = ci95_hi_5\n",
    "df5['ci95_lo_5'] = ci95_lo_5\n",
    "df5['Mean5'] = df5['Mean5'].apply(lambda x: '{:,.1f}'.format(x))\n",
    "\n",
    "df_wss_prec = pd.merge(df1[['Review', 'Total records', 'Relevant records', 'Train model', 'Mean1', 'ci95_hi_1', 'ci95_lo_1']], \n",
    "                       df2[['Review', 'Total records', 'Relevant records', 'Train model', 'Mean2', 'ci95_hi_2', 'ci95_lo_2']],\n",
    "                       how = 'left', on = ['Review', 'Total records', 'Relevant records', 'Train model'])\n",
    "df_wss_prec = pd.merge(df_wss_prec, \n",
    "                       df3[['Review', 'Total records', 'Relevant records', 'Train model', 'Mean3', 'ci95_hi_3', 'ci95_lo_3']], \n",
    "                       how = 'left', on = ['Review', 'Total records', 'Relevant records', 'Train model'])\n",
    "df_wss_prec = pd.merge(df_wss_prec, \n",
    "                       df4[['Review', 'Total records', 'Relevant records', 'Train model', 'Mean4', 'ci95_hi_4', 'ci95_lo_4']], \n",
    "                       how = 'left', on = ['Review', 'Total records', 'Relevant records', 'Train model'])\n",
    "df_wss_prec = pd.merge(df_wss_prec, \n",
    "                       df5[['Review', 'Total records', 'Relevant records', 'Train model', 'Mean5', 'ci95_hi_5', 'ci95_lo_5']], \n",
    "                       how = 'left', on = ['Review', 'Total records', 'Relevant records', 'Train model'])\n",
    "\n",
    "df_wss_prec = df_wss_prec.drop(df_wss_prec[df_wss_prec['Review'] == 'Prog5'].index)\n",
    "df_wss_prec['Review'] = df_wss_prec['Review'].replace({'Prog6': 'Prog5',\n",
    "                                                       'Prog7': 'Prog6'})\n",
    "\n",
    "# # df_wss_prec['WSS@95%recall (CI)'] = df_wss_prec[\"Mean1\"] + \" (\" + df_wss_prec[\"ci95_lo_1\"].astype(str) + \"-\" + df_wss_prec[\"ci95_hi_1\"].astype(str) + \")\"\n",
    "# # df_wss_prec['n-WSS@95%recall (CI)'] = df_wss_prec[\"Mean2\"] + \" (\" + df_wss_prec[\"ci95_lo_2\"].astype(str) + \"-\" + df_wss_prec[\"ci95_hi_2\"].astype(str) + \")\"\n",
    "# # df_wss_prec['Precision@95%recall (CI)'] = df_wss_prec[\"Mean3\"] + \" (\" + df_wss_prec[\"ci95_lo_3\"].astype(str) + \"-\" + df_wss_prec[\"ci95_hi_3\"].astype(str) + \")\"\n",
    "\n",
    "# #df_wss_prec['Models'] = df_wss_prec['Feature model'] + \" - \" + df_wss_prec['Train model']\n",
    "# sns.barplot(hue='Review',\n",
    "#             y='Mean3', \n",
    "#             x='Relevant records', \n",
    "#             data=df_wss_prec).set(ylabel='Precision@0.95recall',ylim=(0,0.5))\n",
    "# plt.legend(fontsize='10', title_fontsize='14', bbox_to_anchor=(1.05, 1), loc=2, borderaxespad=0.)\n",
    "\n",
    "# plt.show()\n",
    "\n",
    "# #df_wss_prec['Models'] = df_wss_prec['Feature model'] + \" - \" + df_wss_prec['Train model']\n",
    "# sns.barplot(hue='Review',\n",
    "#             y='Mean1', \n",
    "#             x='Relevant records', \n",
    "#             data=df_wss_prec).set(ylabel='WSS@0.95recall', ylim=(0,1))\n",
    "# plt.legend(fontsize='10', title_fontsize='14', bbox_to_anchor=(1.05, 1), loc=2, borderaxespad=0.)\n",
    "\n",
    "# plt.show()\n",
    "\n",
    "# #df_wss_prec['Models'] = df_wss_prec['Feature model'] + \" - \" + df_wss_prec['Train model']\n",
    "# sns.barplot(hue='Review',\n",
    "#             y='Mean2', \n",
    "#             x='Relevant records', \n",
    "#             data=df_wss_prec).set(ylabel='nWSS@0.95recall', ylim=(0,1))\n",
    "# plt.legend(fontsize='10', title_fontsize='14', bbox_to_anchor=(1.05, 1), loc=2, borderaxespad=0.)\n",
    "\n",
    "# plt.show()\n",
    "\n",
    "# remove for plots:\n",
    "df_wss_prec['WSS@95%recall (CI)'] = df_wss_prec[\"Mean1\"] + \" (\" + df_wss_prec[\"ci95_lo_1\"].astype(str) + \"-\" + df_wss_prec[\"ci95_hi_1\"].astype(str) + \")\"\n",
    "df_wss_prec['n-WSS@95%recall (CI)'] = df_wss_prec[\"Mean2\"] + \" (\" + df_wss_prec[\"ci95_lo_2\"].astype(str) + \"-\" + df_wss_prec[\"ci95_hi_2\"].astype(str) + \")\"\n",
    "df_wss_prec['Precision@95%recall (CI)'] = df_wss_prec[\"Mean3\"] + \" (\" + df_wss_prec[\"ci95_lo_3\"].astype(str) + \"-\" + df_wss_prec[\"ci95_hi_3\"].astype(str) + \")\"\n",
    "df_wss_prec['Workload reduction in record numbers (CI)'] = df_wss_prec[\"Mean4\"] + \" (\" + df_wss_prec[\"ci95_lo_4\"].astype(str) + \"-\" + df_wss_prec[\"ci95_hi_4\"].astype(str) + \")\"\n",
    "df_wss_prec['Workload reduction in hours (CI)'] = df_wss_prec[\"Mean5\"] + \" (\" + df_wss_prec[\"ci95_lo_5\"].astype(str) + \"-\" + df_wss_prec[\"ci95_hi_5\"].astype(str) + \")\"\n",
    "\n",
    "df_wss_prec.drop(['Mean1', 'ci95_hi_1', 'ci95_lo_1', 'Mean2', 'ci95_hi_2', 'ci95_lo_2', 'Mean3', 'ci95_hi_3', 'ci95_lo_3', 'Mean4', 'ci95_hi_4', 'ci95_lo_4',  'Mean5', 'ci95_hi_5', 'ci95_lo_5'], axis=1, inplace=True)\n",
    "\n",
    "#df_wss_prec.drop(['Mean1', 'ci95_hi_1', 'ci95_lo_1', 'Mean2', 'ci95_hi_2', 'ci95_lo_2', 'Mean3', 'ci95_hi_3', 'ci95_lo_3'], axis=1, inplace=True)\n",
    "\n",
    "df_wss_prec['Total records'] = df_wss_prec['Total records'].astype(int)\n",
    "df_wss_prec['Relevant records'] = df_wss_prec['Relevant records'].astype(float)\n",
    "df_wss_prec.loc[df_wss_prec['Relevant records'] >= 0, 'Relevant records'] = str(500)\n",
    "df_wss_prec = df_wss_prec.sort_values(by=['Review', 'Total records'])\n",
    "\n",
    "df_wss_prec.to_excel('Table_WSS_precision_sizes_final_extra.xlsx')\n",
    "df_wss_prec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "58c41242",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Figures for wss and precision (for results)\n",
    "import math\n",
    "\n",
    "df_wss_prec = pd.merge(df_wss, df_prec, on=['Review', 'Total records', 'Relevant records', 'Train model', 'Simulation'])\n",
    "\n",
    "df1 = df_wss_prec.groupby(['Review', 'Total records', 'Relevant records', 'Train model'])['WSS@95%'].agg(Mean1 = 'mean',\n",
    "                                                                                                         Count1 = 'count',\n",
    "                                                                                                        SD1 = 'std').reset_index()\n",
    "\n",
    "ci95_hi_1 = []\n",
    "ci95_lo_1 = []\n",
    "\n",
    "for i in df1.index:\n",
    "    m, c, s = df1.loc[i][4:7]\n",
    "    ci95_hi_1.append(\"{:.3f}\".format(m + 1.96*s/math.sqrt(c)))\n",
    "    ci95_lo_1.append(\"{:.3f}\".format(m - 1.96*s/math.sqrt(c)))\n",
    "df1['ci95_hi_1'] = ci95_hi_1\n",
    "df1['ci95_lo_1'] = ci95_lo_1\n",
    "\n",
    "df2 = df_wss_prec.groupby(['Review', 'Total records', 'Relevant records', 'Train model'])['n-WSS@95%'].agg(Mean2 = 'mean',\n",
    "                                                                                                           Count2 = 'count',\n",
    "                                                                                                           SD2 = 'std').reset_index()\n",
    "\n",
    "ci95_hi_2 = []\n",
    "ci95_lo_2 = []\n",
    "\n",
    "for i in df2.index:\n",
    "    m, c, s = df2.loc[i][4:7]\n",
    "    ci95_hi_2.append(\"{:.3f}\".format(m + 1.96*s/math.sqrt(c)))\n",
    "    ci95_lo_2.append(\"{:.3f}\".format(m - 1.96*s/math.sqrt(c)))\n",
    "df2['ci95_hi_2'] = ci95_hi_2\n",
    "df2['ci95_lo_2'] = ci95_lo_2\n",
    "\n",
    "df3 = df_wss_prec.groupby(['Review', 'Total records', 'Relevant records', 'Train model'])['Precision@95%'].agg(Mean3 = 'mean',\n",
    "                                                                                                               Count3 = 'count',\n",
    "                                                                                                                SD3 = 'std').reset_index()\n",
    "ci95_hi_3 = []\n",
    "ci95_lo_3 = []\n",
    "\n",
    "for i in df3.index:\n",
    "    m, c, s = df3.loc[i][4:7]\n",
    "    ci95_hi_3.append(\"{:.3f}\".format(m + 1.96*s/math.sqrt(c)))\n",
    "    ci95_lo_3.append(\"{:.3f}\".format(m - 1.96*s/math.sqrt(c)))\n",
    "df3['ci95_hi_3'] = ci95_hi_3\n",
    "df3['ci95_lo_3'] = ci95_lo_3\n",
    "\n",
    "df_wss_prec = pd.merge(df1[['Review', 'Total records', 'Relevant records', 'Train model', 'Mean1', 'ci95_hi_1', 'ci95_lo_1']], \n",
    "                       df2[['Review', 'Total records', 'Relevant records', 'Train model', 'Mean2', 'ci95_hi_2', 'ci95_lo_2']],\n",
    "                       how = 'left', on = ['Review', 'Total records', 'Relevant records', 'Train model'])\n",
    "df_wss_prec = pd.merge(df_wss_prec, \n",
    "                       df3[['Review', 'Total records', 'Relevant records', 'Train model', 'Mean3', 'ci95_hi_3', 'ci95_lo_3']], \n",
    "                       how = 'left', on = ['Review', 'Total records', 'Relevant records', 'Train model'])\n",
    "\n",
    "df_wss_prec = df_wss_prec.drop(df_wss_prec[df_wss_prec['Review'] == 'Prog5'].index)\n",
    "df_wss_prec['Review'] = df_wss_prec['Review'].replace({'Prog6': 'Prog5',\n",
    "                                                       'Prog7': 'Prog6'})\n",
    "\n",
    "df_wss_prec = df_wss_prec.drop(df_wss_prec[df_wss_prec['Review'] == 'Prog1'].index)\n",
    "df_wss_prec = df_wss_prec.drop(df_wss_prec[df_wss_prec['Review'] == 'Prog2'].index)\n",
    "\n",
    "subset = 'Prog'\n",
    "df_wss_prec = df_wss_prec[df_wss_prec['Review'].str.contains(subset)]\n",
    "\n",
    "sns.barplot(hue='Review',\n",
    "            y='Mean3', \n",
    "            x='Total records', \n",
    "            order = ['500','1000','2000'],\n",
    "            data=df_wss_prec).set(ylabel='Precision@0.95recall',ylim=(0,0.5))\n",
    "plt.legend(fontsize='10', title_fontsize='14', bbox_to_anchor=(1.05, 1), loc=2, borderaxespad=0.)\n",
    "\n",
    "plt.show()\n",
    "\n",
    "sns.barplot(hue='Review',\n",
    "            y='Mean1', \n",
    "            x='Total records', \n",
    "            order = ['500','1000','2000'],\n",
    "            data=df_wss_prec).set(ylabel='WSS@0.95recall', ylim=(0,1))\n",
    "plt.legend(fontsize='10', title_fontsize='14', bbox_to_anchor=(1.05, 1), loc=2, borderaxespad=0.)\n",
    "\n",
    "plt.show()\n",
    "\n",
    "sns.barplot(hue='Review',\n",
    "            y='Mean2', \n",
    "            x='Total records', \n",
    "            order = ['500','1000','2000'],\n",
    "            data=df_wss_prec).set(ylabel='nWSS@0.95recall', ylim=(0,1))\n",
    "plt.legend(fontsize='10', title_fontsize='14', bbox_to_anchor=(1.05, 1), loc=2, borderaxespad=0.)\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dad0b5e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Output table for combined wss and precision (for results)\n",
    "\n",
    "df_wss_prec = pd.merge(df_wss, df_prec, on=['Review', 'Total records', 'Relevant records', 'Query model', 'Simulation'])\n",
    "df_wss_prec.drop(['Query model'], axis=1, inplace=True)\n",
    "\n",
    "df1 = df_wss_prec.groupby(['Review', 'Total records', 'Relevant records'])['WSS@95%'].agg(Mean1 = 'mean',\n",
    "                                                                                     SD1 = 'std').reset_index()\n",
    "df3 = df_wss_prec.groupby(['Review', 'Total records', 'Relevant records'])['Precision@95%'].agg(Mean3 = 'mean',\n",
    "                                                                                           SD3 = 'std').reset_index()\n",
    "    \n",
    "df_wss_prec = pd.merge(df1[['Review', 'Total records', 'Relevant records', 'Mean1', 'SD1']], \n",
    "                       df3[['Review', 'Total records', 'Relevant records', 'Mean3', 'SD3']],\n",
    "                       how = 'left', on = ['Review', 'Total records', 'Relevant records'])\n",
    "\n",
    "df_wss_prec['WSS@95%recall (mean \\u00B1 sd)'] = round(df_wss_prec[\"Mean1\"], 2).astype(str) + u\" \\u00B1 \" + round(df_wss_prec[\"SD1\"], 2).astype(str)\n",
    "df_wss_prec['Precision@95%recall (mean \\u00B1 sd)'] = round(df_wss_prec[\"Mean3\"], 2).astype(str) + u\" \\u00B1 \" + round(df_wss_prec[\"SD3\"], 2).astype(str)\n",
    "df_wss_prec.drop(['Mean1', 'SD1', 'Mean3', 'SD3'], axis=1, inplace=True)\n",
    "df_wss_prec.to_excel('Table_WSS_Precision_sizes.xlsx')\n",
    "df_wss_prec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a1bedef",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "36b1d225",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f1303aaf",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d5e5d0a8",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Derive a table of the recall at different proportions of screened records\n",
    "# TODO cleanup\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib as mpl\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "evaluation_sizes = {}\n",
    "for i in range(0, len(multiple_sims_sizes)):\n",
    "    evaluation_sizes[multiple_sims_sizes[i][0]] = []\n",
    "    evaluation_sizes[multiple_sims_sizes[i][0]].append(multiple_sims_sizes[i][3:7])\n",
    "    \n",
    "n_simulations = 10\n",
    "prop_start = 0.1\n",
    "prop_end = 0.9\n",
    "prop_steps = 9\n",
    "\n",
    "steps = np.linspace(prop_start*100, prop_end*100, prop_steps, retstep = True)[0]\n",
    "proportions = []\n",
    "for i in range(0, len(steps)):\n",
    "    proportions.append(str(int(steps[i])) + '%')\n",
    "\n",
    "inclusion_proportion = [0.05, 0.10, 0.15, 0.20, 0.25, 0.05, 0.10, 0.15, 0.20, 0.25, 0.05, 0.10, 0.15, 0.20, 0.25, 0.05, 0.10, 0.15, 0.20, 0.25, 0.05, 0.10, 0.15, 0.20, 0.25, 0.05, 0.10, 0.15, 0.20, 0.25, 0.05, 0.10, 0.15, 0.20, 0.25]\n",
    "\n",
    "df = pd.DataFrame()\n",
    "length = len(proportions) * n_simulations\n",
    "for i in range(0, len(output_sizes)):\n",
    "    names = [output_sizes[i][0]] * length\n",
    "    size = [output_sizes[i][1]] * length\n",
    "    #inclusions = inclusion_proportion\n",
    "    inclusions = []\n",
    "    for i in [0.05, 0.10, 0.15, 0.20, 0.25]:\n",
    "        inclusions.append(list(np.repeat(i, n_simulations)))\n",
    "    inclusions = [item for sublist in inclusions for item in sublist] \n",
    "    props = proportions * n_simulations\n",
    "    simulations = []\n",
    "    for i in range(1, n_simulations+1):\n",
    "        simulations.append(list(np.repeat(i, len(proportions))))\n",
    "    simulations = [item for sublist in simulations for item in sublist]\n",
    "    recalls_props = output_sizes[i][3]['Recall'] #[item for sublist in value[0][0] for item in sublist]\n",
    "    df_sim = pd.DataFrame(list(zip(size,inclusions,simulations,props,recalls_props)),\n",
    "                       columns = ['Review size', 'Inclusion proportion', 'Simulation', 'Percentage of records screened','Recall'])\n",
    "    df = pd.concat([df, df_sim])\n",
    "\n",
    "df = df.groupby(['Review size', 'Inclusion proportion', 'Percentage of records screened'])['Recall'].agg(Mean = 'mean',\n",
    "                                                   SD = 'std').sort_values('Review size').reset_index()\n",
    "df['Mean_SD'] = round(df[\"Mean\"], 2).astype(str) + u\" \\u00B1 \" + round(df[\"SD\"], 2).astype(str)\n",
    "df = df.round(2)\n",
    "df = df.pivot(index = ['Review size', 'Inclusion proportion'], columns='Percentage of records screened', values='Mean').reset_index()\n",
    "df = df.rename_axis(None, axis=1)\n",
    "\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1aa8ecc8",
   "metadata": {},
   "outputs": [],
   "source": [
    "####################################################################################################################"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "812a8b9b",
   "metadata": {},
   "outputs": [],
   "source": [
    "multiple_sims = []\n",
    "for review in review_dic:\n",
    "    for train_model in train_models:\n",
    "        for feature_model in feature_models:\n",
    "            for query_model in query_models:\n",
    "                sim = [review, train_model, feature_model, query_model]\n",
    "                multiple_sims.append(sim)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a4975fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "str(multiple_sims[0][1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b28805b",
   "metadata": {},
   "outputs": [],
   "source": [
    "ASReview_simulation(review_id = multiple_sims[1][0], review_data = dfs_prog[multiple_sims[1][0]], \n",
    "                        path_name = path_name,\n",
    "                        train_model = multiple_sims[1][1], query_model = multiple_sims[1][3], \n",
    "                        balance_model = DoubleBalance(), feature_model = multiple_sims[1][2], \n",
    "                        n_simulations = 10, #100\n",
    "                        n_model_update = 10, n_prior_included = 10, n_prior_excluded = 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "97e23e56",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
