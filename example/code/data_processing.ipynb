{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a373dde1-f6d1-46d6-b83c-2ca8bb274170",
   "metadata": {},
   "source": [
    "# **Data processing**\n",
    "\n",
    "#### **Last update: 11th June, 2025**\n",
    "<br>\n",
    "\n",
    "This notebook retrieves the data from a number of systematic reviews on intervention studies by conducting the following steps:\n",
    "\n",
    "**Step 1: Download the files with PubMed IDs below and place them in the \"data/raw/\" folder.** <br>\n",
    "In this notebook, some of the intervention review datasets from the CLEF challenge are processed for usage in semi-automated screening simulations. The datasets (which can also be manually downloaded here: https://github.com/CLEF-TAR/tar/tree/master/2019-TAR/Task2 found under Training -> Intervention -> topics or Testing -> Intervention -> topics) are called:\n",
    "\n",
    "- CD011768 (corresponds to intervention review 1)\n",
    "- CD008170 (corresponds to intervention review 2)\n",
    "- CD010558 (corresponds to intervention review 3)\n",
    "- CD006468 (corresponds to intervention review 4)\n",
    "- CD010038 (corresponds to intervention review 5)\n",
    "- CD005139 (corresponds to intervention review 6)\n",
    "- CD008201 (corresponds to intervention review A1)\n",
    "\n",
    "**Step 2: Download the files with title-abstract inclusion labels below and place them in the \"data/meta_data/clef_qrels/\" folder.** <br>\n",
    "The corresponding labels for title-abstract as well as full-text level inclusions are stored in separate files (which can also be manually downloaded here: https://github.com/CLEF-TAR/tar/tree/master/2019-TAR/Task2 found under Training -> Intervention -> qrels or Testing -> Intervention -> qrels) are called:\n",
    "\n",
    "- full.train.int.abs.2019.qrels\n",
    "- full.test.intervention.abs.2019.qrels\n",
    "\n",
    "**Step 3: Process the files for compatibility with semi-automated screening simulations.** <br>\n",
    "Using the downloaded raw files, for each review: <br>\n",
    "\n",
    "1. the PubMed IDs are retrieved, <br>\n",
    "2. from the PubMed IDs the titles and abstracts are retrieved, and <br>\n",
    "3. the title-abstract labels are imported and added to each dataset which is saved as .csv in the \"data/processed/\" folder.\n",
    "\n",
    "**Note**: The resulting datasets may slightly vary depending on the date the Pubmed IDs are being retrieved with this notebook, as PubMed studies can be deleted (or added) over time. \n",
    "\n",
    "------------------------------------------------------------------------------------------------------------------------------------------\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d109378c-77ef-48a4-a7e3-b7302ec6e4b1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Intel MKL WARNING: Support of Intel(R) Streaming SIMD Extensions 4.2 (Intel(R) SSE4.2) enabled only processors has been deprecated. Intel oneAPI Math Kernel Library 2025.0 will require Intel(R) Advanced Vector Extensions (Intel(R) AVX) instructions.\n",
      "Intel MKL WARNING: Support of Intel(R) Streaming SIMD Extensions 4.2 (Intel(R) SSE4.2) enabled only processors has been deprecated. Intel oneAPI Math Kernel Library 2025.0 will require Intel(R) Advanced Vector Extensions (Intel(R) AVX) instructions.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import csv\n",
    "import time\n",
    "import requests\n",
    "import pandas as pd\n",
    "import xml.etree.ElementTree as ET\n",
    "from tqdm import tqdm\n",
    "import glob\n",
    "import warnings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "cccbc3c3-c76e-453a-8833-c4c1423ed93c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/Users/ispiero2/TAR-abstracts_testing\n"
     ]
    }
   ],
   "source": [
    "# Check if the current pathway is in the main folder\n",
    "os.chdir(\"..\") \n",
    "current_path = os.getcwd()\n",
    "print(current_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1ba398b1-73d0-4dc4-8646-244f5f8e7528",
   "metadata": {},
   "source": [
    "#### **Step 1: Download the files below and place them in the \"data/raw/\" folder.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "db479fcf-187d-4252-a0fe-01b84167b187",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Indicate the GitHub urls where the systematic review data can be downloaded\n",
    "urls_reviews_dic = {\n",
    "    'CD011768' : \"https://raw.githubusercontent.com/CLEF-TAR/tar/master/2019-TAR/Task2/Testing/Intervention/topics/CD011768\",\n",
    "    'CD008170' : \"https://raw.githubusercontent.com/CLEF-TAR/tar/master/2019-TAR/Task2/Training/Intervention/topics/CD008170\",\n",
    "    'CD010558' : \"https://raw.githubusercontent.com/CLEF-TAR/tar/master/2019-TAR/Task2/Testing/Intervention/topics/CD010558\",\n",
    "    'CD006468' : \"https://raw.githubusercontent.com/CLEF-TAR/tar/master/2019-TAR/Task2/Testing/Intervention/topics/CD006468\",\n",
    "    'CD010038' : \"https://raw.githubusercontent.com/CLEF-TAR/tar/master/2019-TAR/Task2/Testing/Intervention/topics/CD010038\",\n",
    "    'CD005139' : \"https://raw.githubusercontent.com/CLEF-TAR/tar/master/2019-TAR/Task2/Training/Intervention/topics/CD005139\",\n",
    "    'CD008201' : \"https://raw.githubusercontent.com/CLEF-TAR/tar/master/2019-TAR/Task2/Training/Intervention/topics/CD008201\"\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "15cd4e22-b6e2-4982-a199-e4c0384ca309",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloaded successfully to data/raw/CD011768.txt\n",
      "Downloaded successfully to data/raw/CD008170.txt\n",
      "Downloaded successfully to data/raw/CD010558.txt\n",
      "Downloaded successfully to data/raw/CD006468.txt\n",
      "Downloaded successfully to data/raw/CD010038.txt\n",
      "Downloaded successfully to data/raw/CD005139.txt\n",
      "Downloaded successfully to data/raw/CD008201.txt\n"
     ]
    }
   ],
   "source": [
    "# Path to store the raw review data\n",
    "path_data_raw = \"data/raw/\"  \n",
    "\n",
    "# Download the data for each review using the respective url\n",
    "for key, value in urls_reviews_dic.items():\n",
    "    \n",
    "    response = requests.get(value)\n",
    "\n",
    "    path = path_data_raw + key + \".txt\"\n",
    "    if response.status_code == 200:\n",
    "        with open(path, \"wb\") as f:\n",
    "            f.write(response.content)\n",
    "        print(f\"Downloaded successfully to {path}\")\n",
    "    else:\n",
    "        print(f\"Failed to download. Status code: {response.status_code}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7ba143b5-40b3-489f-a7ae-d52506b1748f",
   "metadata": {},
   "source": [
    "#### **Step 2: Download the files below and place them in the \"data/meta_data/clef_qrels/\" folder.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "702faa24-dcc7-4052-aa26-d1de3a71b09a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Indicate the GitHub urls where the qrels (title-abstract labels) can be downloaded\n",
    "urls_qrels_dic = {\n",
    "    'train' : \"https://raw.githubusercontent.com/CLEF-TAR/tar/master/2019-TAR/Task2/Training/Intervention/qrels/full.train.int.abs.2019.qrels\",\n",
    "    'test' : \"https://raw.githubusercontent.com/CLEF-TAR/tar/master/2019-TAR/Task2/Testing/Intervention/qrels/full.test.intervention.abs.2019.qrels\"\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "12cf3744-27d8-4a54-b4c3-8980d45cd59d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloaded successfully to data/meta_data/clef_qrels/train.qrels\n",
      "Downloaded successfully to data/meta_data/clef_qrels/test.qrels\n"
     ]
    }
   ],
   "source": [
    "# Path to store the raw qrels data\n",
    "path_meta_data = \"data/meta_data/clef_qrels/\"\n",
    "\n",
    "# Download the data for each qrels file using the respective url\n",
    "for key, value in urls_qrels_dic.items():\n",
    "    \n",
    "    response = requests.get(value)\n",
    "\n",
    "    path = path_meta_data + key + \".qrels\"\n",
    "    if response.status_code == 200:\n",
    "        with open(path, \"wb\") as f:\n",
    "            f.write(response.content)\n",
    "        print(f\"Downloaded successfully to {path}\")\n",
    "    else:\n",
    "        print(f\"Failed to download. Status code: {response.status_code}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "803cf0e7-b0f9-42be-8323-8bc802ede8b6",
   "metadata": {},
   "source": [
    "#### **Step 3: Process the files**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0409a5c1-70e6-4ce1-900c-e07697649355",
   "metadata": {},
   "source": [
    "##### **1. From .txt extract PMIDs and save as .csv file**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "4e730ec2-ecef-488a-bdb9-51d6e5bed6c8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Processing: data/raw/CD005139.txt\n",
      "Saved: data/converted/CD005139.csv\n",
      "\n",
      "Processing: data/raw/CD006468.txt\n",
      "Saved: data/converted/CD006468.csv\n",
      "\n",
      "Processing: data/raw/CD010038.txt\n",
      "Saved: data/converted/CD010038.csv\n",
      "\n",
      "Processing: data/raw/CD010558.txt\n",
      "Saved: data/converted/CD010558.csv\n",
      "\n",
      "Processing: data/raw/CD008170.txt\n",
      "Saved: data/converted/CD008170.csv\n",
      "\n",
      "Processing: data/raw/CD011768.txt\n",
      "Saved: data/converted/CD011768.csv\n",
      "\n",
      "Processing: data/raw/CD008201.txt\n",
      "Saved: data/converted/CD008201.csv\n"
     ]
    }
   ],
   "source": [
    "# Path where the raw review data are stored\n",
    "path_data_raw = \"data/raw/\" \n",
    "# Path to store the converted review data\n",
    "path_data_converted = \"data/converted\"\n",
    "\n",
    "format = \"TREC\"  # Change to \"TOP\" if needed\n",
    "\n",
    "# Retrieve the lines in the .txt that are PMIDs and store as .csv\n",
    "def process_file(filename, format, path_data_converted):\n",
    "    \"\"\"\n",
    "    Processes a single .txt file, extracts PMIDs, and saves results as a CSV file.\n",
    "\n",
    "    :param filename: Path to the .txt file\n",
    "    :param format: Output format ('TREC' or 'TOP')\n",
    "    :param output_folder: Folder where processed CSV files will be saved\n",
    "    \"\"\"\n",
    "    record = False\n",
    "    topicid = \"\"\n",
    "    output_data = []\n",
    "\n",
    "    with open(filename, \"r\", encoding=\"utf-8\") as f:\n",
    "        while f:\n",
    "            line = f.readline()\n",
    "            if not line:\n",
    "                break\n",
    "\n",
    "            if record:\n",
    "                pmid = line.strip()\n",
    "                output_data.append([topicid, pmid]) \n",
    "\n",
    "            if line.startswith(\"Topic:\"):\n",
    "                topicid = line.split()[1].strip()\n",
    "\n",
    "            if line.startswith(\"Pids:\"):\n",
    "                record = True\n",
    "\n",
    "    # Save results to a CSV file\n",
    "    if output_data:\n",
    "        os.makedirs(path_data_converted, exist_ok=True)  \n",
    "        output_filename = os.path.join(path_data_converted, os.path.basename(filename).replace(\".txt\", \".csv\"))\n",
    "\n",
    "        with open(output_filename, \"w\", newline=\"\", encoding=\"utf-8\") as csvfile:\n",
    "            writer = csv.writer(csvfile)\n",
    "            writer.writerow([\"TopicID\", \"PMID\"])  \n",
    "            writer.writerows(output_data)  \n",
    "        \n",
    "        print(f\"Saved: {output_filename}\")\n",
    "\n",
    "def process_folder(path_data_raw, path_data_converted, format):\n",
    "    \"\"\"\n",
    "    Processes all .txt files in the specified folder and saves results as CSV.\n",
    "\n",
    "    :param input_folder: Folder containing .txt files\n",
    "    :param output_folder: Folder to save processed CSV files\n",
    "    :param format: Output format ('TREC' or 'TOP')\n",
    "    \"\"\"\n",
    "    if not os.path.exists(path_data_raw):\n",
    "        print(f\"Error: Folder '{path_data_raw}' does not exist.\")\n",
    "        return\n",
    "\n",
    "    files = [f for f in os.listdir(path_data_raw) if f.endswith(\".txt\")]\n",
    "\n",
    "    if not files:\n",
    "        print(\"No .txt files found in the folder.\")\n",
    "        return\n",
    "\n",
    "    for file in files:\n",
    "        file_path = os.path.join(path_data_raw, file)\n",
    "        print(f\"\\nProcessing: {file_path}\")\n",
    "        process_file(file_path, format, path_data_converted)\n",
    "\n",
    "process_folder(path_data_raw, path_data_converted, format)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0b16a93f-77ae-428f-bcc3-eed714aeefbb",
   "metadata": {},
   "source": [
    "##### **2. From the PMIDs in the saved .csv files extract the corresponding titles and abstracts from PubMed**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "c5746a8e-f1e6-418c-8bff-f645cf6f0daf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Processing: data/converted/CD010038.csv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing CD010038.csv: 100%|██████████| 89/89 [01:48<00:00,  1.21s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved: data/extracted/CD010038.csv\n",
      "\n",
      "Processing: data/converted/CD010558.csv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing CD010558.csv: 100%|██████████| 29/29 [00:39<00:00,  1.37s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved: data/extracted/CD010558.csv\n",
      "\n",
      "Processing: data/converted/CD011768.csv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing CD011768.csv: 100%|██████████| 92/92 [01:47<00:00,  1.17s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved: data/extracted/CD011768.csv\n",
      "\n",
      "Processing: data/converted/CD008170.csv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing CD008170.csv: 100%|██████████| 124/124 [02:14<00:00,  1.09s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved: data/extracted/CD008170.csv\n",
      "\n",
      "Processing: data/converted/CD008201.csv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing CD008201.csv: 100%|██████████| 36/36 [00:44<00:00,  1.24s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved: data/extracted/CD008201.csv\n",
      "\n",
      "Processing: data/converted/CD005139.csv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing CD005139.csv:  67%|██████▋   | 36/54 [00:42<00:23,  1.33s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error fetching data for PMIDs ['22232487', '27759581', '23480269', '24610632', '22801846', '22173075', '26411831', '22044337', '20937739', '21268815', '22304024', '22594924', '23430681', '21988318', '21187730', '22595908', '25547525', '20845250', '19410953', '18780651', '17317399', '25905984', '28712657', '22269605', '27465105', '23582991', '19900204', '23242589', '25193672', '19289984', '28366074', '27482641', '26957834', '25006692', '17567661', '20508510', '27766582', '23362796', '17524771', '17097190', '17060799', '25756339', '27409464', '20464787', '24696051', '16603955', '26501397', '25651450', '26148802', '27417506', '19321472', '27000269', '23642783', '18050131', '19584653', '15161830', '21668787', '23746130', '27027523', '21470386', '19556214', '25011025', '20379211', '19668384', '22491395', '20574019', '18617755', '18667951', '23464520', '26090898', '21720153', '27613201', '22495357', '19898177', '19023226', '26529038', '21170652', '23455233', '22408217', '27860530', '20558421', '27380430', '20104938', '25397779', '23813109', '18843500', '26558192', '25234795', '24618706', '23640509', '11672877', '24979237', '24211865', '18577190', '16524319', '22042133', '24961698', '25749297', '24950911', '21545354']: HTTPSConnectionPool(host='eutils.ncbi.nlm.nih.gov', port=443): Max retries exceeded with url: /entrez/eutils/efetch.fcgi?db=pubmed&id=22232487%2C27759581%2C23480269%2C24610632%2C22801846%2C22173075%2C26411831%2C22044337%2C20937739%2C21268815%2C22304024%2C22594924%2C23430681%2C21988318%2C21187730%2C22595908%2C25547525%2C20845250%2C19410953%2C18780651%2C17317399%2C25905984%2C28712657%2C22269605%2C27465105%2C23582991%2C19900204%2C23242589%2C25193672%2C19289984%2C28366074%2C27482641%2C26957834%2C25006692%2C17567661%2C20508510%2C27766582%2C23362796%2C17524771%2C17097190%2C17060799%2C25756339%2C27409464%2C20464787%2C24696051%2C16603955%2C26501397%2C25651450%2C26148802%2C27417506%2C19321472%2C27000269%2C23642783%2C18050131%2C19584653%2C15161830%2C21668787%2C23746130%2C27027523%2C21470386%2C19556214%2C25011025%2C20379211%2C19668384%2C22491395%2C20574019%2C18617755%2C18667951%2C23464520%2C26090898%2C21720153%2C27613201%2C22495357%2C19898177%2C19023226%2C26529038%2C21170652%2C23455233%2C22408217%2C27860530%2C20558421%2C27380430%2C20104938%2C25397779%2C23813109%2C18843500%2C26558192%2C25234795%2C24618706%2C23640509%2C11672877%2C24979237%2C24211865%2C18577190%2C16524319%2C22042133%2C24961698%2C25749297%2C24950911%2C21545354&rettype=xml&retmode=xml (Caused by SSLError(SSLError(1, '[SSL: SSLV3_ALERT_UNEXPECTED_MESSAGE] ssl/tls alert unexpected message (_ssl.c:2548)')))\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing CD005139.csv: 100%|██████████| 54/54 [01:03<00:00,  1.18s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved: data/extracted/CD005139.csv\n",
      "\n",
      "Processing: data/converted/CD006468.csv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing CD006468.csv: 100%|██████████| 39/39 [00:43<00:00,  1.12s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved: data/extracted/CD006468.csv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# Path where the PMIDs of the review data are stored\n",
    "path_data_converted = \"data/converted/\"  \n",
    "# Path to store the extracted titles and abstracts\n",
    "path_data_extracted = \"data/extracted/\"  \n",
    "combined_output_file = os.path.join(path_data_extracted, \"clef_all.csv\")  \n",
    "\n",
    "files_to_skip = []\n",
    "\n",
    "def fetch_pubmed_data(list_of_pids):\n",
    "    \"\"\"\n",
    "    Fetches PubMed article details for a given list of PMIDs.\n",
    "    Skips unavailable PMIDs and returns data for available ones.\n",
    "    \n",
    "    :param list_of_pids: List of PubMed IDs (PMIDs)\n",
    "    :return: DataFrame containing PMID, Title, and Abstract for available PMIDs\n",
    "    \"\"\"\n",
    "    if not list_of_pids:\n",
    "        return pd.DataFrame(columns=[\"PMID\", \"Title\", \"Abstract\"])\n",
    "\n",
    "    # Convert list of PMIDs to comma-separated string\n",
    "    list_of_pids_str = \",\".join(list_of_pids)\n",
    "\n",
    "    # Request parameters\n",
    "    payload = {\n",
    "        'db': 'pubmed',\n",
    "        'id': list_of_pids_str,\n",
    "        'rettype': 'xml',\n",
    "        'retmode': 'xml'\n",
    "    }\n",
    "\n",
    "    try:\n",
    "        # Fetch data from PubMed\n",
    "        response = requests.get(\"https://eutils.ncbi.nlm.nih.gov/entrez/eutils/efetch.fcgi?\", params=payload)\n",
    "\n",
    "        # Check if the response contains valid content\n",
    "        if not response.content.strip():\n",
    "            print(f\"No data returned for PMIDs: {list_of_pids}\")\n",
    "            return pd.DataFrame(columns=[\"PMID\", \"Title\", \"Abstract\"])\n",
    "\n",
    "        # Parse XML response\n",
    "        root = ET.fromstring(response.content)\n",
    "\n",
    "        # Extract article information\n",
    "        articles = []\n",
    "        for article in root.findall(\".//PubmedArticle\"):\n",
    "            pmid = article.find(\".//PMID\").text\n",
    "            title_element = article.find(\".//ArticleTitle\")\n",
    "            title = title_element.text if title_element is not None else \"No title available\"\n",
    "            \n",
    "            abstract_element = article.find(\".//AbstractText\")\n",
    "            abstract = abstract_element.text if abstract_element is not None else \"No abstract available\"\n",
    "            \n",
    "            articles.append({\"PMID\": pmid, \"Title\": title, \"Abstract\": abstract})\n",
    "\n",
    "        # Convert to DataFrame\n",
    "        return pd.DataFrame(articles)\n",
    "\n",
    "    except requests.exceptions.RequestException as e:\n",
    "        print(f\"Error fetching data for PMIDs {list_of_pids}: {e}\")\n",
    "        return pd.DataFrame(columns=[\"PMID\", \"Title\", \"Abstract\"])\n",
    "\n",
    "def process_csv_files(path_data_converted, path_data_extracted, combined_output_file):\n",
    "    \"\"\"\n",
    "    Reads all CSV files from the input folder, extracts PMIDs, fetches PubMed data, \n",
    "    saves new CSV files, and creates a combined output file.\n",
    "\n",
    "    :param input_folder: Path to the folder containing processed CSV files (PMID lists).\n",
    "    :param output_folder: Path to the folder where new CSV files will be saved.\n",
    "    :param combined_output_file: Path to the final combined output CSV file.\n",
    "    \"\"\"\n",
    "    if not os.path.exists(path_data_converted):\n",
    "        print(f\"Error: Folder '{path_data_converted}' does not exist.\")\n",
    "        return\n",
    "\n",
    "    files = [f for f in os.listdir(path_data_converted) if f.endswith(\".csv\")]\n",
    "    \n",
    "    if not files:\n",
    "        print(\"No .csv files found in the folder.\")\n",
    "        return\n",
    "\n",
    "    os.makedirs(path_data_extracted, exist_ok=True)  \n",
    "    all_data = []  \n",
    "\n",
    "    for file in files:\n",
    "        # Skip the file if it's in the 'files_to_skip' list\n",
    "        if file in files_to_skip:\n",
    "            print(f\"Skipping file: {file}\")\n",
    "            continue\n",
    "\n",
    "        file_path = os.path.join(path_data_converted, file)\n",
    "        print(f\"\\nProcessing: {file_path}\")\n",
    "\n",
    "        # Read CSV and extract PMIDs\n",
    "        df = pd.read_csv(file_path)\n",
    "        if \"PMID\" not in df.columns:\n",
    "            print(f\"Skipping {file}: No 'PMID' column found.\")\n",
    "            continue\n",
    "\n",
    "        pmid_list = df[\"PMID\"].dropna().astype(str).tolist() \n",
    "\n",
    "        # Fetch PubMed data for batches of PMIDs\n",
    "        batch_size = 100  \n",
    "        batched_data = []  \n",
    "        for i in tqdm(range(0, len(pmid_list), batch_size), desc=f\"Processing {file}\"):\n",
    "            batch = pmid_list[i:i+batch_size]\n",
    "            pubmed_data = fetch_pubmed_data(batch)\n",
    "            batched_data.append(pubmed_data)\n",
    "\n",
    "        # Concatenate the batched data for this file\n",
    "        pubmed_data_combined = pd.concat(batched_data, ignore_index=True)\n",
    "\n",
    "        # Save the new CSV file in the output folder with the same filename\n",
    "        output_filename = os.path.join(path_data_extracted, file)\n",
    "        pubmed_data_combined.to_csv(output_filename, index=False)\n",
    "        print(f\"Saved: {output_filename}\")\n",
    "\n",
    "        # Append to the combined data list\n",
    "        pubmed_data_combined[\"Source_File\"] = file  \n",
    "        all_data.append(pubmed_data_combined)\n",
    "\n",
    "process_csv_files(path_data_converted, path_data_extracted, combined_output_file)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ec12e75c-ddc2-47d5-9207-917a07bf609e",
   "metadata": {},
   "source": [
    "##### **3. Retrieve the labels for title-abstract screening from the qurels and add to each dataset**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "c3d1cad6-c5bb-489d-87f4-0e8142274e7c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Converted: data/meta_data/clef_qrels/test.qrels -> data/meta_data/clef_qrels/test.txt\n",
      "Converted: data/meta_data/clef_qrels/train.qrels -> data/meta_data/clef_qrels/train.txt\n"
     ]
    }
   ],
   "source": [
    "# Path where the qrels are stored\n",
    "path_meta_data = \"data/meta_data/clef_qrels/\"\n",
    "\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Convert the .qrels files to .txt files\n",
    "def convert_qrels_to_txt(path_meta_data):\n",
    "    # Find all .qrels files in the folder\n",
    "    qrels_files = glob.glob(os.path.join(path_meta_data, \"*.qrels\"))\n",
    "\n",
    "    for qrels_file in qrels_files:\n",
    "        # Extract base filename without extension\n",
    "        base_name = os.path.splitext(os.path.basename(qrels_file))[0]\n",
    "        \n",
    "        # Load the .qrels file (assuming whitespace-separated format)\n",
    "        qrels_df = pd.read_csv(qrels_file, sep=r\"\\s+\", header=None, names=[\"query_id\", \"unused\", \"doc_id\", \"relevance\"])\n",
    "        \n",
    "        # Define output .txt path\n",
    "        txt_file = os.path.join(path_meta_data, f\"{base_name}.txt\")\n",
    "        \n",
    "        # Save as .txt\n",
    "        qrels_df.to_csv(txt_file, sep=\" \", index=False, header=False)\n",
    "        print(f\"Converted: {qrels_file} -> {txt_file}\")\n",
    "\n",
    "convert_qrels_to_txt(path_meta_data)\n",
    "\n",
    "# Specify the path to your .txt files (adjust the path accordingly)\n",
    "txt_files = glob.glob(path_meta_data + \"*.txt\")\n",
    "\n",
    "# Initialize an empty list to hold dataframes\n",
    "dfs = []\n",
    "\n",
    "# Loop through each file\n",
    "for file in txt_files:\n",
    "    # Read the content of the file into a dataframe, assuming whitespace as the delimiter\n",
    "    df = pd.read_csv(file, delim_whitespace=True)  \n",
    "    \n",
    "    # Rename the columns\n",
    "    df.columns = ['topic', 'unknown', 'PMID', 'label']\n",
    "    \n",
    "    # Append the dataframe to the list\n",
    "    dfs.append(df)\n",
    "\n",
    "# Concatenate all the dataframes into one\n",
    "concatenated_df = pd.concat(dfs, ignore_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "3311fdbd-50fa-45ba-8199-367c18b2c8ed",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>topic</th>\n",
       "      <th>unknown</th>\n",
       "      <th>PMID</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>CD005139</td>\n",
       "      <td>0</td>\n",
       "      <td>22972355</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>CD005139</td>\n",
       "      <td>0</td>\n",
       "      <td>17644433</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>CD005139</td>\n",
       "      <td>0</td>\n",
       "      <td>26866528</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>CD005139</td>\n",
       "      <td>0</td>\n",
       "      <td>17380066</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>CD005139</td>\n",
       "      <td>0</td>\n",
       "      <td>26107864</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>73634</th>\n",
       "      <td>CD012551</td>\n",
       "      <td>0</td>\n",
       "      <td>11272678</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>73635</th>\n",
       "      <td>CD012551</td>\n",
       "      <td>0</td>\n",
       "      <td>12504236</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>73636</th>\n",
       "      <td>CD012551</td>\n",
       "      <td>0</td>\n",
       "      <td>17546832</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>73637</th>\n",
       "      <td>CD012551</td>\n",
       "      <td>0</td>\n",
       "      <td>1872650</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>73638</th>\n",
       "      <td>CD012551</td>\n",
       "      <td>0</td>\n",
       "      <td>18038549</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>73438 rows × 4 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "          topic  unknown      PMID  label\n",
       "0      CD005139        0  22972355      0\n",
       "1      CD005139        0  17644433      0\n",
       "2      CD005139        0  26866528      0\n",
       "3      CD005139        0  17380066      0\n",
       "4      CD005139        0  26107864      0\n",
       "...         ...      ...       ...    ...\n",
       "73634  CD012551        0  11272678      0\n",
       "73635  CD012551        0  12504236      0\n",
       "73636  CD012551        0  17546832      0\n",
       "73637  CD012551        0   1872650      0\n",
       "73638  CD012551        0  18038549      0\n",
       "\n",
       "[73438 rows x 4 columns]"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "qrels_df = concatenated_df.drop_duplicates()\n",
    "qrels_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "d9de27b5-9e0e-4169-b72b-5a170e15ea5b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing files: 100%|██████████| 7/7 [00:00<00:00,  7.69file/s]\n"
     ]
    }
   ],
   "source": [
    "# Path where the extracted data is stored\n",
    "path_data_extracted = 'data/extracted/'\n",
    "# Path to store the cleaned data\n",
    "path_data_processed = 'data/processed/'\n",
    "\n",
    "# Get the list of .csv files in the input folder\n",
    "csv_files = glob.glob(os.path.join(path_data_extracted, '*.csv'))\n",
    "\n",
    "# Create the output folder if it doesn't exist\n",
    "os.makedirs(path_data_processed, exist_ok=True)\n",
    "\n",
    "# Process files \n",
    "for file in tqdm(csv_files, desc=\"Processing files\", unit=\"file\"):\n",
    "    # Extract the topic from the filename (remove the extension .csv)\n",
    "    topic = os.path.basename(file).replace('.csv', '')\n",
    "    \n",
    "    # Filter qrels_df based on the topic (from the filename)\n",
    "    filtered_qrels_df = qrels_df[qrels_df['topic'] == topic]\n",
    "    \n",
    "    # Read the current .csv file into a DataFrame\n",
    "    df_csv = pd.read_csv(file)\n",
    "    \n",
    "    # Merge the label from qrels_df to the .csv file based on 'PMID'\n",
    "    df_merged = pd.merge(df_csv, filtered_qrels_df[['PMID', 'label']], on='PMID', how='left')\n",
    "\n",
    "    # Rename the needed columns for consistency\n",
    "    df_merged = df_merged.rename(columns={'PMID': 'pubmed_id', \n",
    "                            'Title': 'title',\n",
    "                            'Abstract': 'abstract',\n",
    "                            'label': 'label_included'})\n",
    "\n",
    "    # Add the needed columns\n",
    "    df_merged['id'] = range(1, len(df_merged) + 1)\n",
    "    df_merged['openalex_id'] = None\n",
    "    df_merged['doi'] = None\n",
    "    df_merged['keywords'] = None\n",
    "    df_merged['year'] = None\n",
    "\n",
    "    # Select the needed columns\n",
    "    df_merged = df_merged[['id', 'title', 'abstract', 'pubmed_id', 'openalex_id', 'doi',\n",
    "             'keywords', 'year', 'label_included']]\n",
    "\n",
    "    # Create a new output file path\n",
    "    output_file = os.path.join(path_data_processed, os.path.basename(file))\n",
    "\n",
    "    # Ensure all abstracts have a label that is not NA and as integer\n",
    "    df_merged = df_merged.dropna(subset=['label_included'])\n",
    "    df_merged['label_included'] = df_merged['label_included'].astype(int)\n",
    "    \n",
    "    # Save the updated .csv file to the output folder\n",
    "    df_merged.to_csv(output_file, index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0ab47316-beae-4391-8254-a6804e71f006",
   "metadata": {},
   "source": [
    "##### **End of notebook**"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
